{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypolars as pl\n",
    "from pypolars.lazy import *\n",
    "import numpy as np\n",
    "from string import ascii_letters\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazyness\n",
    "\n",
    "Py-polars has a lazy API that supports a subset of the eager API. Lazyness means that operations aren't executed until you ask for them. Let's start with a short example..\n",
    "\n",
    "Below we'll create a DataFrame in an eager fashion (meaning that the creation of the DataFrame is executed at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+-------+-----+\n",
       "| a   | b     | c   |\n",
       "| --- | ---   | --- |\n",
       "| i64 | f64   | str |\n",
       "+=====+=======+=====+\n",
       "| 0   | 0.538 | \"a\" |\n",
       "+-----+-------+-----+\n",
       "| 1   | 0.111 | \"b\" |\n",
       "+-----+-------+-----+\n",
       "| 2   | 0.539 | \"c\" |\n",
       "+-----+-------+-----+\n",
       "| 3   | 0.75  | \"d\" |\n",
       "+-----+-------+-----+\n",
       "| 4   | 0.479 | \"e\" |\n",
       "+-----+-------+-----+\n",
       "| 5   | 0.246 | \"f\" |\n",
       "+-----+-------+-----+\n",
       "| 6   | 0.754 | \"g\" |\n",
       "+-----+-------+-----+\n",
       "| 7   | 0.457 | \"h\" |\n",
       "+-----+-------+-----+\n",
       "| 8   | 0.331 | \"i\" |\n",
       "+-----+-------+-----+\n",
       "| 9   | 0.805 | \"j\" |\n",
       "+-----+-------+-----+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame({\"a\": np.arange(0, 10),\n",
    "              \"b\": np.random.rand(10),\n",
    "               \"c\": list(ascii_letters[:10])})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy DataFrame\n",
    "To make this a lazy dataframe we call the `.lazy` method. As we can see, not much happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pypolars.lazy.LazyFrame at 0x7f212d85a9e8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf = df.lazy()\n",
    "ldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter this `DataFrame` on all the rows, but we'll see that again nothing happens. \n",
    "\n",
    "*Note the `col` and `lit` (meaning **column** and **literal value**) are part of the lazy **dsl** (domain specific language) and are needed to build a query plan.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pypolars.lazy.LazyFrame at 0x7f212d85aef0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf = ldf.filter(col(\"a\") == (lit(2)))\n",
    "ldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is only executed when we ask for it. This can be done with `.collect` method. \n",
    "Below we execute the query and obtain our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----+-------+-----+\n",
       "| a   | b     | c   |\n",
       "| --- | ---   | --- |\n",
       "| i64 | f64   | str |\n",
       "+=====+=======+=====+\n",
       "| 2   | 0.539 | \"c\" |\n",
       "+-----+-------+-----+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why lazy?\n",
    "This lazyness opens up quite some cool possibitlies from an optimization perspective. \n",
    "It allows polars to modify the query right before executing it and make suboptimal queries more performant. Let's show this using various operations, comparing lazy execution with eager execution in both Polars and Pandas.\n",
    "\n",
    "Let's create 2 DataFrames with quite some columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_string(n: int, set_size: int, lower=True) -> str:\n",
    "    s = \"\".join(np.random.choice(list(ascii_letters[:set_size]), n))\n",
    "    if lower:\n",
    "        return s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing a subset of df_a:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "+---------+----------+----------+----------+----------+----------+-----------------------+----------+----------+----------+\n",
       "| key     | column_0 | column_1 | column_2 | column_3 | column_4 | column_5              | column_6 | column_7 | column_8 |\n",
       "| ---     | ---      | ---      | ---      | ---      | ---      | ---                   | ---      | ---      | ---      |\n",
       "| str     | f32      | i64      | f32      | f32      | f32      | str                   | i64      | f64      | f64      |\n",
       "+=========+==========+==========+==========+==========+==========+=======================+==========+==========+==========+\n",
       "| \"ejbid\" | 4.17     | 9        | 6.627    | 4.492    | 2.78     | \"2.3672007927500314\"  | 3        | 4.789    | 9.194    |\n",
       "+---------+----------+----------+----------+----------+----------+-----------------------+----------+----------+----------+\n",
       "| \"bdgbc\" | 7.203    | 3        | 1.216    | 8.639    | 5.813    | \"0.17036859867274767\" | 9        | 8.049    | 0.638    |\n",
       "+---------+----------+----------+----------+----------+----------+-----------------------+----------+----------+----------+\n",
       "| \"ijffe\" | 0.001    | 5        | 4.181    | 1.95     | 1.51     | \"3.907531612288251\"   | 6        | 0.404    | 6.904    |\n",
       "+---------+----------+----------+----------+----------+----------+-----------------------+----------+----------+----------+"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 30_000\n",
    "columns = 30\n",
    "key_size = 5\n",
    "key_set_size = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "dtypes = [np.float32, np.float64, np.int, str]\n",
    "\n",
    "df_a = pl.DataFrame({f\"column_{i}\": np.array(np.random.rand(rows) * 10, dtype=np.random.choice(dtypes)) for i in range(columns)})\n",
    "s = pl.Series(\"key\",  np.array([rand_string(key_size, key_set_size) for _ in range(rows)]))\n",
    "df_a.insert_at_idx(0, s)\n",
    "\n",
    "rows = 20_000\n",
    "columns = 8\n",
    "df_b = pl.DataFrame({f\"column_{i}\": np.array(np.random.rand(rows) * 10, dtype=np.random.choice(dtypes)) for i in range(columns)})\n",
    "s = pl.Series(\"key\",  np.array([rand_string(key_size, key_set_size) for _ in range(rows)]))\n",
    "df_b.insert_at_idx(0, s)\n",
    "\n",
    "\n",
    "print(\"Showing a subset of df_a:\")\n",
    "# only show a sub_slice\n",
    "df_a[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_pd = df_a.to_pandas()\n",
    "df_b_pd = df_b.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where Polars loses (or wins?)\n",
    "Let's start with an operation where polars is slower than pandas; filtering. A filter predicate creates a boolean array. Polars/Arrow stores these boolean values not as boolean values of 1 byte,\n",
    "but as bits, meaning 1 bytes stores 8 booleans. This reduces memory 8-fold, but has some overhead on array creation. As we can see pandas is more than 5x faster, though there is a huge spread.\n",
    "\n",
    "Pandas has something called a blockmanager which hugely increases filtering performance (I believe due to cache optimallity). However this blockmanager gives performance hits when modifying blocks and block consolidation is triggered. This block consolidation triggers:\n",
    "\n",
    "* when the blockmanager has > 100 blocks\n",
    "* groupby operation is executed\n",
    "* Operations: diff, take, xs, reindex, _is_mixed_type, _is_numeric_mixed_type, values, fillna, replace, resample, concat\n",
    "\n",
    "Read more about the [blockmanager](https://uwekorn.com/2020/05/24/the-one-pandas-internal.html). Below we'll see that the performance hit of the block manager can be very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 453 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a[\"column_2\"] < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.99 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 5: 91.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd[\"column_2\"] < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this mask to select rows from the DataFrame we see that polars gets slower linearly with the number of columns. If we apply this filter on a DataFrame with a single column pandas is **1.2~1.4x** faster, again with a huge_spread in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 962 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a[:, :1][df_a[\"column_2\"] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 647 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd.iloc[:, :1][df_a_pd[\"column_2\"] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss inreases as we have more columns in the DataFrame. Here we observe that with 30 columns, polars is more than **10x** slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 14.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a[df_a[\"column_2\"] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 1.33 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd[df_a_pd[\"column_2\"] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance hit of block manager.\n",
    "Below we'll see how the block managar can also hurt performance in pandas badly. Below we'll see that accessing by index is **> 100x** slower in pandas than in polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  404, 15927, 14838, ...,  2915,  3514, 17696])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(rows)\n",
    "np.random.shuffle(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 5: 52.1 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a[idx, [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 5.53 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd.iloc[idx, [1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where polars definitly wins\n",
    "However, polars wins in all the expensive operations. Joins an groupby operations take most of the running time of query. Below we see that a join is more than **2.5** faster and that pandas join operation takes 500 / 15 = **33** times the runtime of polars' DataFrame filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 211 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a.join(df_b, left_on=\"key\", right_on=\"key\", how=\"inner\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 513 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd.merge(df_b_pd, left_on=\"key\", right_on=\"key\", how=\"inner\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the groupby operation with an aggregation on all the columns we see that polars is more than **100x** faster. Again the pandas groupby run time takes **>33** times the runtime of the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 4.28 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a.groupby(\"key\").max();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 573 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_a_pd.groupby(\"key\").max();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query optimization\n",
    "As we've seen, filtering is slower part of polars, let's see if lazyness can help optimize that. We'll start with a sub-optimal query which you see often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17413, 31)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eager(df: Union[pl.DataFrame, pd.DataFrame]):\n",
    "    df = df[df['column_2'] < 9]\n",
    "    df = df[df['column_3'] > 1]\n",
    "    df = df[df['column_6'] > 1]\n",
    "    df = df[df['column_4'] > 1]\n",
    "    return df\n",
    "    \n",
    "assert eager(df_a_pd).shape == eager(df_a).shape\n",
    "eager(df_a_pd).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 5: 62.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 14.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_query(df_a: pl.DataFrame):\n",
    "    return (df_a.lazy().filter(col(\"column_2\") < lit(9))\n",
    "            .filter(col(\"column_3\") > lit(1))\n",
    "            .filter(col(\"column_6\") > lit(1))\n",
    "            .filter(col(\"column_4\") > lit(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 5: 18.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lazy_query(df_a).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Combine predicates\n",
    "Above the query optimizer aggregated all the filter and executed them at once. This reduces a lot of extra allocations at every filter operations. As polars doesn't have a block managar, this optmization is still not enough to beat pandas, which is **~1.3x** faster. However we did increase the eager performance by **63/18 = ~3.3x** by rewriting the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization (Projection pushdown) Selecting important columns.\n",
    "Let's look at another optimization. Let's say we are only interested in the columns `\"key\"` and `\"column_1\"`. A suboptimal eager could be writter like below. This query could be more performant if the projection (selecting columns) was done before the selection (filtering rows). Below we see that the lazy query is optimized and selects the needed columns before doing the filter operation. This reduces the speeds up the query to **~1.3x** that of pandas and **~5x** that of polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager(df: Union[pl.DataFrame, pd.DataFrame]):\n",
    "    df = df[df['column_2'] < 9]\n",
    "    df = df[df['column_3'] > 1]\n",
    "    df = df[df['column_6'] > 1]\n",
    "    df = df[df['column_4'] > 1]\n",
    "    return df[[\"key\", \"column_1\"]]\n",
    "\n",
    "def lazy_query(df_a: pl.DataFrame):\n",
    "    return (df_a.lazy().filter(col(\"column_2\") < lit(9))\n",
    "            .filter(col(\"column_3\") > lit(1))\n",
    "            .filter(col(\"column_6\") > lit(1))\n",
    "            .filter(col(\"column_4\") > lit(1))\n",
    "            .select([col(\"key\"), col(\"column_1\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 5: 63.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 15.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 12.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lazy_query(df_a).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization: Combine join with predicates\n",
    "The previous queries where suboptmial and could written more performantly. Now let's look at an optimization that we only get by lazyness.\n",
    "Let's say we join `df_a` with `df_b`. Because we already have both DataFrames in memory it is hard to tell if we need to do the filter before or after the join for an optimal query. Let's try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager(df_a: Union[pl.DataFrame, pd.DataFrame], df_b: Union[pl.DataFrame, pd.DataFrame]):\n",
    "    df_a = df_a[df_a[\"column_1\"] < 1]\n",
    "    df_b = df_b[df_b[\"column_1\"] < 1]\n",
    "    # pandas\n",
    "    if hasattr(df_a, \"values\"):\n",
    "        return df_a.merge(df_b, left_on=\"key\", right_on=\"key\")\n",
    "    return df_a.join(df_b, left_on=\"key\", right_on=\"key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager polars; filter before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 18 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a, df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager pandas; filter before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 16.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a_pd, df_b_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager(df_a: Union[pl.DataFrame, pd.DataFrame], df_b: Union[pl.DataFrame, pd.DataFrame]):\n",
    "    # pandas\n",
    "    if hasattr(df_a, \"values\"):\n",
    "        df = df_a.merge(df_b, left_on=\"key\", right_on=\"key\")\n",
    "        df = df[df[\"column_1_x\"] < 1]\n",
    "        return df\n",
    "    df = df_a.join(df_b, left_on=\"key\", right_on=\"key\")\n",
    "    df = df[df[\"column_1\"] < 1]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager polars; filter after join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 8.69 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a, df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager pandas; filter after join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 692 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "eager(df_a_pd, df_b_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_query(df_a: pl.DataFrame, df_b: pl.DataFrame):\n",
    "    return (df_a.lazy()\n",
    "         .join(df_b.lazy(), left_on=col(\"key\"), right_on=col(\"key\"), how=\"inner\")\n",
    "         .filter(col(\"column_1\") < lit(1))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy polars; filter after join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 4.77 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lazy_query(df_a, df_b).collect().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in pandas choosing the wrong order of filters has a huge effect, slowing down the query more **40x**. In polars the filtering is relatively expensive and choosing the wrong order slows down execution **2x**, but because filtering remains a relative fast operation, on the whole query it has less effect because the join is much faster. \n",
    "\n",
    "In the lazy variant, the optimizer combines the join and the filter in a single operation, thereby saving a redundant allocation. This is not always optimal. If the join algorithm increases the number of rows by a factor of 10 or 100, this can be more expensive than a filter up front. However, it remains a reasonable default that reduces most queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_query(df_a: pl.DataFrame, df_b: pl.DataFrame):\n",
    "    return (df_a.lazy()\n",
    "         .join(df_b.lazy(), left_on=col(\"key\"), right_on=col(\"key\"), how=\"inner\")\n",
    "         .filter(col(\"column_1\") < lit(1))\n",
    "         .groupby(\"key\")\n",
    "         .agg([col(\"column_0\").agg_sum()])\n",
    "         .select([col(\"key\"), col(\"column_0_sum\")])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 4.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lazy_query(df_a, df_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udf (User defined fucntions <3 Lazyness)\n",
    "The lazy api also has access to all the `eager` operations on `Series` because there are udf's with almost no overhead (no serializing or pickling). Below we'll add a column `\"udf\"` with a `lambda` and help of the eager api. It still needs some polishing, as we need to make sure that we don't modify the dtypes. I hope you can imagine that this can be very powerful! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 ms, sys: 0 ns, total: 16.8 ms\n",
      "Wall time: 9.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def lazy_query(df_a: pl.DataFrame, df_b: pl.DataFrame):\n",
    "    return (df_a.lazy()\n",
    "         .join(df_b.lazy(), left_on=col(\"key\"), right_on=col(\"key\"), how=\"inner\")\n",
    "         .filter(col(\"column_1\") < lit(1))\n",
    "         .groupby(\"key\")\n",
    "         .agg([col(\"column_0\").agg_sum(), col(\"column_2\").agg_max().alias(\"foo\")])\n",
    "         .with_column(col(\"foo\").apply(\n",
    "             lambda series: pl.Series(\"\", np.ones(series.len(), dtype=np.float32) * series.sum() )\n",
    "                                               ).alias('udf'))\n",
    "         .select([col(\"key\"), col(\"column_0_sum\"), col(\"udf\"), col(\"foo\")])\n",
    "    )\n",
    "\n",
    "lazy_query(df_a, df_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More coming up later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
