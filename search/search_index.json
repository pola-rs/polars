{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"Blazingly Fast DataFrame Library  <p>Polars is a blazingly fast DataFrame library for manipulating structured data. The core is written in Rust, and available for Python, R and NodeJS.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Fast: Written from scratch in Rust, designed close to the machine and without external   dependencies.</li> <li>I/O: First class support for all common data storage layers: local, cloud storage &amp; databases.</li> <li>Intuitive API: Write your queries the way they were intended. Polars, internally, will   determine the most efficient way to execute using its query optimizer.</li> <li>Out of Core: The streaming API allows you to process your results without requiring all your   data to be in memory at the same time.</li> <li>Parallel: Utilises the power of your machine by dividing the workload among the available CPU   cores without any additional configuration.</li> <li>Vectorized Query Engine</li> <li>GPU Support: Optionally run queries on NVIDIA GPUs for maximum performance for in-memory   workloads.</li> <li>Apache Arrow support: Polars can consume and produce Arrow data   often with zero-copy operations. Note that Polars is not built on a Pyarrow/Arrow implementation.   Instead, Polars has its own compute and buffer implementations.</li> </ul> <p>Users new to DataFrames</p> <p>A DataFrame is a 2-dimensional data structure that is useful for data manipulation and analysis. With labeled axes for rows and columns, each column can contain different data types, making complex data operations such as merging and aggregation much easier. Due to their flexibility and intuitive way of storing and working with data, DataFrames have become increasingly popular in modern data analytics and engineering.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>The goal of Polars is to provide a lightning fast DataFrame library that:</p> <ul> <li>Utilizes all available cores on your machine.</li> <li>Optimizes queries to reduce unneeded work/memory allocations.</li> <li>Handles datasets much larger than your available RAM.</li> <li>A consistent and predictable API.</li> <li>Adheres to a strict schema (data-types should be known before running the query).</li> </ul> <p>Polars is written in Rust which gives it C/C++ performance and allows it to fully control performance-critical parts in a query engine.</p>"},{"location":"#example","title":"Example","text":"Python Rust <p> <code>scan_csv</code> \u00b7 <code>filter</code> \u00b7 <code>group_by</code> \u00b7 <code>collect</code> <pre><code>import polars as pl\n\nq = (\n    pl.scan_csv(\"docs/assets/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7 <code>filter</code> \u00b7 <code>group_by</code> \u00b7 <code>collect</code> \u00b7  Available on feature streaming \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\n\nlet q = LazyCsvReader::new(PlRefPath::new(\"docs/assets/data/iris.csv\"))\n    .with_has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"*\").sum()]);\n\nlet df = q.collect()?;\n</code></pre></p> <p>A more extensive introduction can be found in the next chapter.</p>"},{"location":"#community","title":"Community","text":"<p>Polars has a very active community with frequent releases (approximately weekly). Below are some of the top contributors to the project:</p> <p> </p>"},{"location":"#contributing","title":"Contributing","text":"<p>We appreciate all contributions, from reporting bugs to implementing new features. Read our contributing guide to learn more.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"_build/snippets/under_construction/","title":"Under construction","text":"<p> Under Construction </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our contributing guide on how to proceed.</p>"},{"location":"api/reference/","title":"Reference guide","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"api/reference/#python","title":"Python","text":"<p>The Python API reference is built using Sphinx. It's available in our docs.</p>"},{"location":"api/reference/#rust","title":"Rust","text":"<p>The Rust API reference is built using Cargo. It's available on docs.rs.</p>"},{"location":"development/versioning/","title":"Versioning","text":""},{"location":"development/versioning/#version-changes","title":"Version changes","text":"<p>Polars adheres to the semantic versioning specification:</p> <ul> <li>Breaking changes lead to a major version increase (<code>1.0.0</code>, <code>2.0.0</code>, ...)</li> <li>New features and performance improvements lead to a minor version increase (<code>1.1.0</code>, <code>1.2.0</code>,   ...)</li> <li>Other changes lead to a patch version increase (<code>1.0.1</code>, <code>1.0.2</code>, ...)</li> </ul>"},{"location":"development/versioning/#policy-for-breaking-changes","title":"Policy for breaking changes","text":"<p>Polars takes backwards compatibility seriously, but we are not afraid to change things if it leads to a better product.</p>"},{"location":"development/versioning/#philosophy","title":"Philosophy","text":"<p>We don't always get it right on the first try. We learn as we go along and get feedback from our users. Sometimes, we're a little too eager to get out a new feature and didn't ponder all the possible implications.</p> <p>If this happens, we correct our mistakes and introduce a breaking change. Most of the time, this is no big deal. Users get a deprecation warning, they do a quick search-and-replace in their code base, and that's that.</p> <p>At times, we run into an issue requires more effort on our user's part to fix. A change in the query engine can seriously impact the assumptions in a data pipeline. We do not make such changes lightly, but we will make them if we believe it makes Polars better.</p> <p>Freeing ourselves of past indiscretions is important to keep Polars moving forward. We know it takes time and energy for our users to keep up with new releases but, in the end, it benefits everyone for Polars to be the best product possible.</p>"},{"location":"development/versioning/#what-qualifies-as-a-breaking-change","title":"What qualifies as a breaking change","text":"<p>A breaking change occurs when an existing component of the public API is changed or removed.</p> <p>A feature is part of the public API if it is documented in the API reference.</p> <p>Examples of breaking changes:</p> <ul> <li>A deprecated function or method is removed.</li> <li>The default value of a parameter is changed.</li> <li>The outcome of a query has changed due to changes to the query engine.</li> </ul> <p>Examples of changes that are not considered breaking:</p> <ul> <li>An undocumented function is removed.</li> <li>The module path of a public class is changed.</li> <li>An optional parameter is added to an existing method.</li> </ul> <p>Bug fixes are not considered a breaking change, even though it may impact some users' workflows.</p>"},{"location":"development/versioning/#unstable-functionality","title":"Unstable functionality","text":"<p>Some parts of the public API are marked as unstable. You can recognize this functionality from the warning in the API reference, or from the warning issued when the configuration option <code>warn_unstable</code> is active. There are a number of reasons functionality may be marked as unstable:</p> <ul> <li>We are unsure about the exact API. The name, function signature, or implementation are likely to   change in the future.</li> <li>The functionality is not tested extensively yet. Bugs may pop up when used in real-world   scenarios.</li> <li>The functionality does not yet integrate well with the full Polars API. You may find it works in   one context but not in another.</li> </ul> <p>Releasing functionality as unstable allows us to gather important feedback from users that use Polars in real-world scenarios. This helps us fine-tune things before giving it our final stamp of approval. Users that are only interested in solid, well-tested functionality can avoid this part of the API.</p> <p>Functionality marked as unstable may change at any point without it being considered a breaking change.</p>"},{"location":"development/versioning/#deprecation-warnings","title":"Deprecation warnings","text":"<p>If we decide to introduce a breaking change, the existing behavior is deprecated if possible. For example, if we choose to rename a function, the new function is added alongside the old function, and using the old function will result in a deprecation warning.</p> <p>Not all changes can be deprecated nicely. A change to the query engine may have effects across a large part of the API. Such changes will not be warned for, but will be included in the changelog and the migration guide.</p> <p>Warning</p> <p>Breaking changes to the Rust API are not deprecated first, but will be listed in the changelog. Supporting deprecated functionality would slow down development too much at this point in time.</p>"},{"location":"development/versioning/#deprecation-period","title":"Deprecation period","text":"<p>As a rule, deprecated functionality is removed two breaking releases after the deprecation happens. For example, a function deprecated in version <code>1.2.3</code> will be retained in version <code>2.0.0</code> and removed in version <code>3.0.0</code>.</p> <p>An exception to this rule are deprecations introduced with a breaking release. These will be enforced on the next breaking release. For example, a function deprecated in version <code>2.0.0</code> will be removed in version <code>3.0.0</code>.</p> <p>This means that if your program does not raise any deprecation warnings, it should be mostly safe to upgrade to the next major version. As breaking releases happen about once every six months, this allows six to twelve months to adjust to any pending breaking changes.</p> <p>In some cases, we may decide to adjust the deprecation period. If retaining the deprecated functionality blocks other improvements to Polars, we may shorten the deprecation period to a single breaking release. This will be mentioned in the warning message. If the deprecation affects many users, we may extend the deprecation period.</p>"},{"location":"development/versioning/#release-frequency","title":"Release frequency","text":"<p>Polars does not have a set release schedule. We issue a new release whenever we feel like we have something new and valuable to offer to our users. In practice, a new minor version is released about once every two weeks.</p>"},{"location":"development/versioning/#pre-releases","title":"Pre-releases","text":"<p>To minimize the risk of regressions, we begin by publishing a pre-release version. This is followed by a cooldown period lasting a few days, during which we block pull requests that might introduce instability. Any regression bugs reported during this time are more likely to be addressed before the final release. We encourage users to actively test the pre-release versions to help ensure greater stability.</p>"},{"location":"development/versioning/#breaking-releases","title":"Breaking releases","text":"<p>Over time, issues pop up that require a breaking change to address. When enough issues have accumulated, we issue a breaking release.</p> <p>So far, breaking releases have happened about once every three to six months. The rate and severity of breaking changes will continue to diminish as Polars grows more solid. From this point on, we expect new major versions to be released about once every six months.</p>"},{"location":"development/contributing/","title":"Overview","text":"<p>Thanks for taking the time to contribute! We appreciate all contributions, from reporting bugs to implementing new features. If you're unclear on how to proceed after reading this guide, please contact us on Discord.</p>"},{"location":"development/contributing/#reporting-bugs","title":"Reporting bugs","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can report a bug by opening a new issue. Use the appropriate issue type for the language you are using (Rust / Python).</p> <p>Before creating a bug report, please check that your bug has not already been reported, and that your bug exists on the latest version of Polars. If you find a closed issue that seems to report the same bug you're experiencing, open a new issue and include a link to the original issue in your issue description.</p> <p>Please include as many details as possible in your bug report. The information helps the maintainers resolve the issue faster.</p>"},{"location":"development/contributing/#suggesting-enhancements","title":"Suggesting enhancements","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can suggest an enhancement by opening a new feature request. Before creating an enhancement suggestion, please check that a similar issue does not already exist.</p> <p>Please describe the behavior you want and why, and provide examples of how Polars would be used if your feature were added.</p>"},{"location":"development/contributing/#contributing-to-the-codebase","title":"Contributing to the codebase","text":""},{"location":"development/contributing/#picking-an-issue","title":"Picking an issue","text":"<p>Pick an issue by going through the issue tracker and finding an issue you would like to work on. Feel free to pick any issue with an accepted label that is not already assigned. We use the help wanted label to indicate issues that are high on our wishlist.</p> <p>If you are a first time contributor, you might want to look for issues labeled good first issue. The Polars code base is quite complex, so starting with a small issue will help you find your way around!</p> <p>If you would like to take on an issue, please comment on the issue to let others know. You may use the issue to discuss possible solutions.</p>"},{"location":"development/contributing/#setting-up-your-local-environment","title":"Setting up your local environment","text":"<p>The Polars development flow relies on both Rust and Python, which means setting up your local development environment is not trivial. If you run into problems, please contact us on Discord.</p> <p>Note</p> <p>If you are a Windows user, the steps below might not work as expected. Try developing using WSL. Under native Windows, you may have to manually copy the contents of <code>toolchain.toml</code> to <code>py-polars/toolchain.toml</code>, as Git for Windows may not correctly handle symbolic links.</p>"},{"location":"development/contributing/#configuring-git","title":"Configuring Git","text":"<p>For contributing to Polars you need a free GitHub account and have git installed on your machine. Start by forking the Polars repository, then clone your forked repository using <code>git</code>:</p> <pre><code>git clone https://github.com/&lt;username&gt;/polars.git\ncd polars\n</code></pre> <p>Optionally set the <code>upstream</code> remote to be able to sync your fork with the Polars repository in the future:</p> <pre><code>git remote add upstream https://github.com/pola-rs/polars.git\ngit fetch upstream\n</code></pre>"},{"location":"development/contributing/#installing-dependencies","title":"Installing dependencies","text":"<p>In order to work on Polars effectively, you will need Rust, Python, and dprint.</p> <p>First, install Rust using rustup. After the initial installation, you will also need to install the nightly toolchain:</p> <pre><code>rustup toolchain install nightly --component miri\n</code></pre> <p>Next, install Python, for example using pyenv. We recommend using the latest Python version (<code>3.13</code>). Make sure you deactivate any active virtual environments (command: <code>deactivate</code>) or conda environments (command: <code>conda deactivate</code>), as the steps below will create a new virtual environment for Polars. You will need Python even if you intend to work on the Rust code only, as we rely on the Python tests to verify all functionality.</p> <p>Finally, install dprint. This is not strictly required, but it is recommended as we use it to autoformat certain file types.</p> <p>You can now check that everything works correctly by going into the <code>py-polars</code> directory and running the test suite (warning: this may be slow the first time you run it):</p> <pre><code>cd py-polars\nmake test\n</code></pre> <p>Note</p> <p>You need to have CMake installed for <code>make test</code> to work.</p> <p>This will do a number of things:</p> <ul> <li>Use Python to create a virtual environment in the <code>.venv</code> folder.</li> <li>Use pip and uv to install all Python   dependencies for development, linting, and building documentation.</li> <li>Use Rust to compile and install Polars in your virtual environment. At least 8GB of RAM is   recommended for this step to run smoothly.</li> <li>Use pytest to run the Python unittests in your virtual environment</li> </ul> <p>Note</p> <p>There are a small number of specialized dependencies that are not installed by default. If you are running specific tests and encounter an error message about a missing dependency, try running <code>make requirements-all</code> to install all known dependencies).</p> <p>Check if linting also works correctly by running:</p> <pre><code>make pre-commit\n</code></pre> <p>Note that we do not actually use the pre-commit tool. We use the Makefile to conveniently run the following formatting and linting tools:</p> <ul> <li>ruff</li> <li>mypy</li> <li>rustfmt</li> <li>clippy</li> <li>dprint</li> </ul> <p>If this all runs correctly, you're ready to start contributing to the Polars codebase!</p>"},{"location":"development/contributing/#updating-the-development-environment","title":"Updating the development environment","text":"<p>Dependencies are updated regularly - at least once per month. If you do not keep your environment up-to-date, you may notice tests or CI checks failing, or you may not be able to build Polars at all.</p> <p>To update your environment, first make sure your fork is in sync with the Polars repository:</p> <pre><code>git checkout main\ngit fetch upstream\ngit rebase upstream/main\ngit push origin main\n</code></pre> <p>Update all Python dependencies to their latest versions by running:</p> <pre><code>make requirements\n</code></pre> <p>If the Rust toolchain version has been updated, you should update your Rust toolchain. Follow it up by running <code>cargo clean</code> to make sure your Cargo folder does not grow too large:</p> <pre><code>rustup update\ncargo clean\n</code></pre>"},{"location":"development/contributing/#working-on-your-issue","title":"Working on your issue","text":"<p>Create a new git branch from the <code>main</code> branch in your local repository, and start coding!</p> <p>The Rust code is located in the <code>crates</code> directory, while the Python codebase is located in the <code>py-polars</code> directory. Both directories contain a <code>Makefile</code> with helpful commands. Most notably:</p> <ul> <li><code>make test</code> to run the test suite (see the test suite docs for more info)</li> <li><code>make pre-commit</code> to run autoformatting and linting</li> </ul> <p>Note that your work cannot be merged if these checks fail! Run <code>make help</code> to get a list of other helpful commands.</p> <p>Two other things to keep in mind:</p> <ul> <li>If you add code that should be tested, add tests.</li> <li>If you change the public API, update the documentation.</li> </ul>"},{"location":"development/contributing/#pull-requests","title":"Pull requests","text":"<p>When you have resolved your issue, open a pull request in the Polars repository. Please adhere to the following guidelines:</p> <ul> <li>Title:<ul> <li>Start your pull request title with a conventional commit tag.   This helps us add your contribution to the right section of the changelog.   We use the Angular convention.   Scope can be <code>rust</code> and/or <code>python</code>, depending on your contribution: this tag determines which changelog(s) will include your change.   Omit the scope if your change affects both Rust and Python.</li> <li>Use a descriptive title starting with an uppercase letter.   This text will end up in the changelog, so make sure the text is meaningful to the user.   Use single backticks to annotate code snippets.   Use active language and do not end your title with punctuation.</li> <li>Example: <code>fix(python): Fix `DataFrame.top_k` not handling nulls correctly</code></li> </ul> </li> <li>Description:<ul> <li>In the pull request description, link to the issue you were working on.</li> <li>Add any relevant information to the description that you think may help the maintainers review your code.</li> </ul> </li> <li>Make sure your branch is rebased against the latest version of the <code>main</code> branch.</li> <li>Make sure all GitHub Actions checks pass.</li> <li>If your contribution contains code generated by AI you must:<ul> <li>Clearly state in your pull request's description which parts of the code were AI-generated.</li> <li>Explicitly state that you yourself have reviewed all changes in your pull request, and believe   that they are relevant and correct.</li> <li>Adhere to the rest of our AI policy.   If you fail either requirement the maintainer may simply close your pull request.</li> </ul> </li> </ul> <p>After you have opened your pull request, a maintainer will review it and possibly leave some comments. Once all issues are resolved, the maintainer will merge your pull request, and your work will be part of the next Polars release!</p> <p>Keep in mind that your work does not have to be perfect right away! If you are stuck or unsure about your solution, feel free to open a draft pull request and ask for help.</p>"},{"location":"development/contributing/#contributing-to-documentation","title":"Contributing to documentation","text":"<p>The most important components of Polars documentation are the user guide, the API references, and the database of questions on Stack Overflow for Python Polars and Rust Polars.</p>"},{"location":"development/contributing/#user-guide","title":"User guide","text":"<p>The user guide is maintained in the <code>docs/source/user-guide</code> folder. Before creating a PR first raise an issue to discuss what you feel is missing or could be improved.</p>"},{"location":"development/contributing/#building-and-serving-the-user-guide","title":"Building and serving the user guide","text":"<p>The user guide is built using MkDocs. You install the dependencies for building the user guide by running <code>make build</code> in the root of the repo. Additionally, you need to make sure the graphviz <code>dot</code> binary is on your path.</p> <p>Activate the virtual environment and run <code>mkdocs serve</code> to build and serve the user guide, so you can view it locally and see updates as you make changes.</p>"},{"location":"development/contributing/#creating-a-new-user-guide-page","title":"Creating a new user guide page","text":"<p>Each user guide page is based on a <code>.md</code> markdown file. This file must be listed in <code>mkdocs.yml</code>.</p>"},{"location":"development/contributing/#adding-a-shell-code-block","title":"Adding a shell code block","text":"<p>To add a code block with code to be run in a shell with tabs for Python and Rust, use the following format:</p> <pre><code>=== \":fontawesome-brands-python: Python\"\n\n    ```shell\n    $ pip install fsspec\n    ```\n\n=== \":fontawesome-brands-rust: Rust\"\n\n    ```shell\n    $ cargo add aws_sdk_s3\n    ```\n</code></pre>"},{"location":"development/contributing/#adding-a-code-block","title":"Adding a code block","text":"<p>The snippets for Python and Rust code blocks are in the <code>docs/source/src/python/</code> and <code>docs/source/src/rust/</code> directories, respectively. To add a code snippet with Python or Rust code to a <code>.md</code> page, use the following format:</p> <pre><code>{{code_block('user-guide/io/cloud-storage','read_parquet',['read_parquet','read_csv'])}}\n</code></pre> <ul> <li>The first argument is a path to either or both files called   <code>docs/source/src/python/user-guide/io/cloud-storage.py</code> and   <code>docs/source/src/rust/user-guide/io/cloud-storage.rs</code>.</li> <li>The second argument is the name given at the start and end of each snippet in the <code>.py</code> or <code>.rs</code>   file</li> <li>The third argument is a list of links to functions in the API docs. For each element of the list   there must be a corresponding entry in <code>docs/source/_build/API_REFERENCE_LINKS.yml</code></li> </ul> <p>If the corresponding <code>.py</code> and <code>.rs</code> snippet files both exist then each snippet named in the second argument to <code>code_block</code> above must exist or the build will fail. An empty snippet should be added to the <code>.py</code> or <code>.rs</code> file if the snippet is not needed.</p> <p>Each snippet is formatted as follows:</p> <pre><code>import polars as pl\n\ndf = pl.read_parquet(\"file.parquet\")\n</code></pre> <p>The snippet is delimited by <code>--8&lt;-- [start:&lt;snippet_name&gt;]</code> and <code>--8&lt;-- [end:&lt;snippet_name&gt;]</code>. The snippet name must match the name given in the second argument to <code>code_block</code> above.</p> <p>In some cases, you may need to add links to different functions for the Python and Rust APIs. When that is the case, you can use the two extra optional arguments that <code>code_block</code> accepts, that can be used to pass Python-only and Rust-only links:</p> <pre><code>{{code_block('path', 'snippet_name', ['common_api_links'], ['python_only_links'], ['rust_only_links'])}}\n</code></pre>"},{"location":"development/contributing/#linting","title":"Linting","text":"<p>Before committing, install <code>dprint</code> (see above) and run <code>dprint fmt</code> from the <code>docs</code> directory to lint the markdown files.</p>"},{"location":"development/contributing/#api-reference","title":"API reference","text":"<p>Polars has separate API references for Rust and Python. These are generated directly from the codebase, so in order to contribute, you will have to follow the steps outlined in this section above.</p>"},{"location":"development/contributing/#rust","title":"Rust","text":"<p>Rust Polars uses <code>cargo doc</code> to build its documentation. Contributions to improve or clarify the API reference are welcome.</p>"},{"location":"development/contributing/#python","title":"Python","text":"<p>For the Python API reference, we always welcome good docstring examples. There are still parts of the API that do not have any code examples. This is a great way to start contributing to Polars!</p> <p>Note that we follow the numpydoc convention. Docstring examples should also follow the Black codestyle. From the <code>py-polars</code> directory, run <code>make fmt</code> to make sure your additions pass the linter, and run <code>make doctest</code> to make sure your docstring examples are valid.</p> <p>Polars uses Sphinx to build the API reference. This means docstrings in general should follow the reST format. If you want to build the API reference locally, go to the <code>py-polars/docs</code> directory and run <code>make html</code>. The resulting HTML files will be in <code>py-polars/docs/build/html</code>.</p> <p>New additions to the API should be added manually to the API reference by adding an entry to the correct <code>.rst</code> file in the <code>py-polars/docs/source/reference</code> directory.</p>"},{"location":"development/contributing/#stackoverflow","title":"StackOverflow","text":"<p>We use StackOverflow to create a database of high quality questions and answers that is searchable and remains up-to-date. There is a separate tag for each language:</p> <ul> <li>Python Polars</li> <li>Rust Polars</li> </ul> <p>Contributions in the form of well-formulated questions or answers are always welcome! If you add a new question, please notify us by adding a matching issue to our GitHub issue tracker.</p>"},{"location":"development/contributing/#release-flow","title":"Release flow","text":"<p>This section is intended for Polars maintainers.</p> <p>Polars releases Rust crates to crates.io and Python packages to PyPI.</p> <p>New releases are marked by an official GitHub release and an associated git tag. We utilize Release Drafter to automatically draft GitHub releases with release notes.</p>"},{"location":"development/contributing/#steps","title":"Steps","text":"<p>The steps for releasing a new Rust or Python version are similar. The release process is mostly automated through GitHub Actions, but some manual steps are required. Follow the steps below to release a new version.</p> <p>Start by bumping the version number in the source code:</p> <ol> <li>Check the releases page on GitHub and find the    appropriate draft release. Note the version number associated with this release.</li> <li>Make sure your fork is up-to-date with the latest version of the main Polars repository, and    create a new branch.</li> <li> <p>Bump the version number.</p> </li> <li> <p>Rust: Update the version number in all <code>Cargo.toml</code> files in the <code>polars</code> directory and   subdirectories. You'll probably want to use some search/replace strategy, as there are quite a few   crates that need to be updated.</p> </li> <li> <p>Python: Update the version number in   <code>py-polars/Cargo.toml</code>   to match the version of the draft release.</p> </li> <li> <p>From the <code>py-polars</code> directory, run <code>make build</code> to generate a new <code>Cargo.lock</code> file.</p> </li> <li>Create a new commit with all files added. The name of the commit should follow the format    <code>release(&lt;language&gt;): &lt;Language&gt; Polars &lt;version-number&gt;</code>. For example:    <code>release(python): Python Polars 0.16.1</code></li> <li>Push your branch and open a new pull request to the <code>main</code> branch of the main Polars repository.</li> <li>Wait for the GitHub Actions checks to pass, then squash and merge your pull request.</li> </ol> <p>Directly after merging your pull request, release the new version:</p> <ol> <li>Go to the release workflow    (Python/Rust),    click Run workflow in the top right, and click the green button. This will trigger the    workflow, which will build all release artifacts and publish them.</li> <li>Wait for the workflow to finish, then check    crates.io/PyPI/GitHub    to verify that the new Polars release is now available.</li> </ol>"},{"location":"development/contributing/#troubleshooting","title":"Troubleshooting","text":"<p>It may happen that one or multiple release jobs fail. If so, you should first try to simply re-run the failed jobs from the GitHub Actions UI.</p> <p>If that doesn't help, you will have to figure out what's wrong and commit a fix. Once your fix has made it to the <code>main</code> branch, simply re-trigger the release workflow.</p>"},{"location":"development/contributing/#license","title":"License","text":"<p>Any contributions you make to this project will fall under the MIT License that covers the Polars project.</p>"},{"location":"development/contributing/ci/","title":"Continuous integration","text":"<p>Polars uses GitHub Actions as its continuous integration (CI) tool. The setup is reasonably complex, as far as CI setups go. This page explains some of the design choices.</p>"},{"location":"development/contributing/ci/#goal","title":"Goal","text":"<p>Overall, the CI suite aims to achieve the following:</p> <ul> <li>Enforce code correctness by running automated tests.</li> <li>Enforce code quality by running automated linting checks.</li> <li>Enforce code performance by running benchmark tests.</li> <li>Enforce that code is properly documented.</li> <li>Allow maintainers to easily publish new releases.</li> </ul> <p>We rely on a wide range of tools to achieve this for both the Rust and the Python code base, and thus a lot of checks are triggered on each pull request.</p> <p>It's entirely possible that you submit a relatively trivial fix that subsequently fails a bunch of checks. Do not despair - check the logs to see what went wrong and try to fix it. You can run the failing command locally to verify that everything works correctly. If you can't figure it out, ask a maintainer for help!</p>"},{"location":"development/contributing/ci/#design","title":"Design","text":"<p>The CI setup is designed with the following requirements in mind:</p> <ul> <li>Get feedback on each step individually. We want to avoid our test job being cancelled because a   linting check failed, only to find out later that we also have a failing test.</li> <li>Get feedback on each check as quickly as possible. We want to be able to iterate quickly if it   turns out our code does not pass some of the checks.</li> <li>Only run checks when they need to be run. A change to the Rust code does not warrant a linting   check of the Python code, for example.</li> </ul> <p>This results in a modular setup with many separate workflows and jobs that rely heavily on caching.</p>"},{"location":"development/contributing/ci/#modular-setup","title":"Modular setup","text":"<p>The repository consists of two main parts: the Rust code base and the Python code base. Both code bases are interdependent: Rust code is tested through Python tests, and the Python code relies on the Rust implementation for most functionality.</p> <p>To make sure CI jobs are only run when they need to be run, each workflow is triggered only when relevant files are modified.</p>"},{"location":"development/contributing/ci/#caching","title":"Caching","text":"<p>The main challenge is that the Rust code base for Polars is quite large, and consequently, compiling the project from scratch is slow. This is addressed by caching the Rust build artifacts.</p> <p>However, since GitHub Actions does not allow sharing caches between feature branches, we need to run the workflows on the main branch as well - at least the part that builds the Rust cache. This leads to many workflows that trigger both on pull request AND on push to the main branch, with individual steps of jobs enabled or disabled based on the branch it runs on.</p> <p>Care must also be taken not to exceed the maximum cache space of 10Gb allotted to open source GitHub repositories. Hence we do not do any caching on feature branches - we always use the cache available from the main branch. This also avoids any extra time that would be required to store the cache.</p>"},{"location":"development/contributing/ci/#releases","title":"Releases","text":"<p>The release jobs for Rust and Python are triggered manually. Refer to the contributing guide for the full release process.</p>"},{"location":"development/contributing/code-style/","title":"Code style","text":"<p>This page contains some guidance on code style.</p> <p>Info</p> <p>Additional information will be added to this page later.</p>"},{"location":"development/contributing/code-style/#rust","title":"Rust","text":""},{"location":"development/contributing/code-style/#naming-conventions","title":"Naming conventions","text":"<p>Naming conventions for variables:</p> <pre><code>let s: Series = ...\nlet ca: ChunkedArray = ...\nlet arr: ArrayRef = ...\nlet arr: PrimitiveArray = ...\nlet dtype: DataType = ...\nlet dtype: ArrowDataType = ...\n</code></pre>"},{"location":"development/contributing/code-style/#code-example","title":"Code example","text":"<pre><code>use std::ops::Add;\n\nuse polars::export::arrow::array::*;\nuse polars::export::arrow::compute::arity::binary;\nuse polars::export::arrow::types::NativeType;\nuse polars::prelude::*;\nuse polars_core::utils::{align_chunks_binary, combine_validities_and};\nuse polars_core::with_match_physical_numeric_polars_type;\n\n// Prefer to do the compute closest to the arrow arrays.\n// this will tend to be faster as iterators can work directly on slices and don't have\n// to go through boxed traits\nfn compute_kernel&lt;T&gt;(arr_1: &amp;PrimitiveArray&lt;T&gt;, arr_2: &amp;PrimitiveArray&lt;T&gt;) -&gt; PrimitiveArray&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt; + NativeType,\n{\n    // process the null data separately\n    // this saves an expensive branch and bitoperation when iterating\n    let validity_1 = arr_1.validity();\n    let validity_2 = arr_2.validity();\n\n    let validity = combine_validities_and(validity_1, validity_2);\n\n    // process the numerical data as if there were no validities\n    let values_1: &amp;[T] = arr_1.values().as_slice();\n    let values_2: &amp;[T] = arr_2.values().as_slice();\n\n    let values = values_1\n        .iter()\n        .zip(values_2)\n        .map(|(a, b)| *a + *b)\n        .collect::&lt;Vec&lt;_&gt;&gt;();\n\n    PrimitiveArray::from_data_default(values.into(), validity)\n}\n\n// Same kernel as above, but uses the `binary` abstraction. Prefer this,\n#[allow(dead_code)]\nfn compute_kernel2&lt;T&gt;(arr_1: &amp;PrimitiveArray&lt;T&gt;, arr_2: &amp;PrimitiveArray&lt;T&gt;) -&gt; PrimitiveArray&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt; + NativeType,\n{\n    binary(arr_1, arr_2, arr_1.dtype().clone(), |a, b| a + b)\n}\n\nfn compute_chunked_array_2_args&lt;T: PolarsNumericType&gt;(\n    ca_1: &amp;ChunkedArray&lt;T&gt;,\n    ca_2: &amp;ChunkedArray&lt;T&gt;,\n) -&gt; ChunkedArray&lt;T&gt; {\n    // This ensures both ChunkedArrays have the same number of chunks with the\n    // same offset and the same length.\n    let (ca_1, ca_2) = align_chunks_binary(ca_1, ca_2);\n    let chunks = ca_1\n        .downcast_iter()\n        .zip(ca_2.downcast_iter())\n        .map(|(arr_1, arr_2)| compute_kernel(arr_1, arr_2));\n    ChunkedArray::from_chunk_iter(ca_1.name(), chunks)\n}\n\npub fn compute_expr_2_args(arg_1: &amp;Series, arg_2: &amp;Series) -&gt; Series {\n    // Dispatch the numerical series to `compute_chunked_array_2_args`.\n    with_match_physical_numeric_polars_type!(arg_1.dtype(), |$T| {\n        let ca_1: &amp;ChunkedArray&lt;$T&gt; = arg_1.as_ref().as_ref().as_ref();\n        let ca_2: &amp;ChunkedArray&lt;$T&gt; = arg_2.as_ref().as_ref().as_ref();\n        compute_chunked_array_2_args(ca_1, ca_2).into_series()\n    })\n}\n</code></pre>"},{"location":"development/contributing/ide/","title":"IDE configuration","text":"<p>Using an integrated development environments (IDE) and configuring it properly will help you work on Polars more effectively. This page contains some recommendations for configuring popular IDEs.</p>"},{"location":"development/contributing/ide/#visual-studio-code","title":"Visual Studio Code","text":"<p>Make sure to configure VSCode to use the virtual environment created by the Makefile.</p>"},{"location":"development/contributing/ide/#extensions","title":"Extensions","text":"<p>The extensions below are recommended.</p>"},{"location":"development/contributing/ide/#rust-analyzer","title":"rust-analyzer","text":"<p>If you work on the Rust code at all, you will need the rust-analyzer extension. This extension provides code completion for the Rust code.</p> <p>For it to work well for the Polars code base, add the following settings to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"rust-analyzer.cargo.features\": \"all\",\n  \"rust-analyzer.cargo.targetDir\": true\n}\n</code></pre>"},{"location":"development/contributing/ide/#ruff","title":"Ruff","text":"<p>The Ruff extension will help you conform to the formatting requirements of the Python code. We use both the Ruff linter and formatter. It is recommended to configure the extension to use the Ruff installed in your environment. This will make it use the correct Ruff version and configuration.</p> <pre><code>{\n  \"ruff.importStrategy\": \"fromEnvironment\"\n}\n</code></pre>"},{"location":"development/contributing/ide/#codelldb","title":"CodeLLDB","text":"<p>The CodeLLDB extension is useful for debugging Rust code. You can also debug Rust code called from Python (see section below).</p>"},{"location":"development/contributing/ide/#debugging","title":"Debugging","text":"<p>Due to the way that Python and Rust interoperate, debugging the Rust side of development from Python calls can be difficult. This guide shows how to set up a debugging environment that makes debugging Rust code called from a Python script painless.</p>"},{"location":"development/contributing/ide/#preparation","title":"Preparation","text":"<p>Start by installing the CodeLLDB extension (see above). Then add the following two configurations to your <code>launch.json</code> file. This file is usually found in the <code>.vscode</code> folder of your project root. See the official VSCode documentation for more information about the <code>launch.json</code> file.</p> <code>launch.json</code> <pre><code>{\n  \"configurations\": [\n    {\n      \"name\": \"Debug Rust/Python\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/py-polars/debug/launch.py\",\n      \"args\": [\n        \"${file}\"\n      ],\n      \"console\": \"internalConsole\",\n      \"justMyCode\": true,\n      \"serverReadyAction\": {\n        \"pattern\": \"pID = ([0-9]+)\",\n        \"action\": \"startDebugging\",\n        \"name\": \"Rust LLDB\"\n      }\n    },\n    {\n      \"name\": \"Rust LLDB\",\n      \"pid\": \"0\",\n      \"type\": \"lldb\",\n      \"request\": \"attach\",\n      \"program\": \"${workspaceFolder}/py-polars/.venv/bin/python\",\n      \"stopOnEntry\": false,\n      \"sourceLanguages\": [\n        \"rust\"\n      ],\n      \"presentation\": {\n        \"hidden\": true\n      }\n    }\n  ]\n}\n</code></pre> <p>Info</p> <p>On some systems, the LLDB debugger will not attach unless ptrace protection is disabled. To disable, run the following command:</p> <pre><code>echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n</code></pre>"},{"location":"development/contributing/ide/#running-the-debugger","title":"Running the debugger","text":"<ol> <li> <p>Create a Python script containing Polars code. Ensure that your virtual environment is activated.</p> </li> <li> <p>Set breakpoints in any <code>.rs</code> or <code>.py</code> file.</p> </li> <li> <p>In the <code>Run and Debug</code> panel on the left, select <code>Debug Rust/Python</code> from the drop-down menu on    top and click the <code>Start Debugging</code> button.</p> </li> </ol> <p>At this point, your debugger should stop on breakpoints in any <code>.rs</code> file located within the codebase.</p>"},{"location":"development/contributing/ide/#details","title":"Details","text":"<p>The debugging feature runs via the specially-designed VSCode launch configuration shown above. The initial Python debugger is launched using a special launch script located at <code>py-polars/debug/launch.py</code> and passes the name of the script to be debugged (the target script) as an input argument. The launch script determines the process ID, writes this value into the <code>launch.json</code> configuration file, compiles the target script and runs it in the current environment. At this point, a second (Rust) debugger is attached to the Python debugger. The result is two simultaneous debuggers operating on the same running instance. Breakpoints in the Python code will stop on the Python debugger and breakpoints in the Rust code will stop on the Rust debugger.</p>"},{"location":"development/contributing/ide/#jetbrains-pycharm-rustrover-clion","title":"JetBrains (PyCharm, RustRover, CLion)","text":"<p>Info</p> <p>More information needed.</p>"},{"location":"development/contributing/test/","title":"Test suite","text":"<p>Info</p> <p>Additional information on the Rust test suite will be added to this page later.</p> <p>The <code>py-polars/tests</code> folder contains the main Polars test suite. This page contains some information on the various components of the test suite, as well as guidelines for writing new tests.</p> <p>The test suite contains four main components, each confined to their own folder: unit tests, parametric tests, benchmark tests, and doctests.</p> <p>Note that this test suite is indirectly responsible for testing Rust Polars as well. The Rust test suite is kept small to reduce compilation times. A lot of the Rust functionality is tested here instead.</p>"},{"location":"development/contributing/test/#unit-tests","title":"Unit tests","text":"<p>The <code>unit</code> folder contains all regular unit tests. These tests are intended to make sure all Polars functionality works as intended.</p>"},{"location":"development/contributing/test/#running-unit-tests","title":"Running unit tests","text":"<p>Run unit tests by running <code>make test</code> from the <code>py-polars</code> folder. This will compile the Rust bindings and then run the unit tests.</p> <p>If you're working in the Python code only, you can avoid recompiling every time by simply running <code>pytest</code> instead from your virtual environment.</p> <p>By default, \"slow\" tests and \"ci-only\" tests are skipped for local test runs. Such tests are marked using a custom pytest marker. To run these tests specifically, you can run <code>pytest -m slow</code>, <code>pytest -m ci_only</code>, <code>pytest -m slow ci_only</code> or run <code>pytest -m \"\"</code> to run all tests, regardless of marker.</p> <p>Note that the \"ci-only\" tests may require you to run <code>make requirements-all</code> to get additional dependencies (such as <code>torch</code>) that are otherwise not installed as part of the default Polars development environment.</p> <p>Tests can be run in parallel by running <code>pytest -n auto</code>. The parallelization is handled by <code>pytest-xdist</code>.</p>"},{"location":"development/contributing/test/#writing-unit-tests","title":"Writing unit tests","text":"<p>Whenever you add new functionality, you should also add matching unit tests. Add your tests to appropriate test module in the <code>unit</code> folder. Some guidelines to keep in mind:</p> <ul> <li>Try to fully cover all possible inputs and edge cases you can think of.</li> <li>Utilize pytest tools like <code>fixture</code>   and <code>parametrize</code> where appropriate.</li> <li>Since many tests will require some data to be defined first, it can be efficient to run multiple   checks in a single test. This can also be addressed using pytest fixtures.</li> <li>Unit tests should not depend on external factors, otherwise test parallelization will break.</li> </ul>"},{"location":"development/contributing/test/#parametric-tests","title":"Parametric tests","text":"<p>The <code>parametric</code> folder contains parametric tests written using the Hypothesis framework. These tests are intended to find and test edge cases by generating many random datapoints.</p>"},{"location":"development/contributing/test/#running-parametric-tests","title":"Running parametric tests","text":"<p>Run parametric tests by running <code>pytest -m hypothesis</code>.</p> <p>Note that parametric tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m hypothesis</code> to run them.</p> <p>These tests will be included when calculating test coverage, and will also be run as part of the <code>make test-all</code> make command.</p>"},{"location":"development/contributing/test/#doctests","title":"Doctests","text":"<p>The <code>docs</code> folder contains a script for running <code>doctest</code>. This folder does not contain any actual tests - rather, the script checks all docstrings in the Polars package for <code>Examples</code> sections, runs the code examples, and verifies the output.</p> <p>The aim of running <code>doctest</code> is to make sure the <code>Examples</code> sections in our docstrings are valid and remain up-to-date with code changes.</p>"},{"location":"development/contributing/test/#running-doctest","title":"Running <code>doctest</code>","text":"<p>To run the <code>doctest</code> module, run <code>make doctest</code> from the <code>py-polars</code> folder. You can also run the script directly from your virtual environment.</p> <p>Note that doctests are not run using pytest. While pytest does have the capability to run doc examples, configuration options are too limited for our purposes.</p> <p>Doctests will not count towards test coverage. They are not a substitute for unit tests, but rather intended to convey the intended use of the Polars API to the user.</p>"},{"location":"development/contributing/test/#writing-doc-examples","title":"Writing doc examples","text":"<p>Almost all classes/methods/functions that are part of Polars' public API should include code examples in their docstring. These examples help users understand basic usage and allow us to illustrate more advanced concepts as well. Some guidelines for writing a good docstring <code>Examples</code> section:</p> <ul> <li>Start with a minimal example that showcases the default functionality.</li> <li>Showcase the effect of its parameters.</li> <li>Showcase any special interactions when combined with other code.</li> <li>Keep it succinct and avoid multiple examples showcasing the same thing.</li> </ul> <p>There are many great docstring examples already, just check other code if you need inspiration!</p> <p>In addition to the regular options available when writing doctests, the script configuration allows for a new <code>IGNORE_RESULT</code> directive. Use this directive if you want to ensure the code runs, but the output may be random by design or not interesting to check.</p> <pre><code>&gt;&gt;&gt; df.sample(n=2)  # doctest: +IGNORE_RESULT\n</code></pre>"},{"location":"development/contributing/test/#benchmark-tests","title":"Benchmark tests","text":"<p>The <code>benchmark</code> folder contains code for running various benchmark tests. The aim of this part of the test suite is to spot performance regressions in the code, and to verify that Polars functionality works as expected when run on a release build or at a larger scale.</p> <p>Polars uses CodSpeed for tracking the performance of the benchmark tests.</p>"},{"location":"development/contributing/test/#generating-data","title":"Generating data","text":"<p>For most tests, a relatively large dataset must be generated first. This is done as part of the <code>pytest</code> setup process.</p> <p>The data generation logic was taken from the H2O.ai database benchmark, which is the foundation for many of the benchmark tests.</p>"},{"location":"development/contributing/test/#running-the-benchmark-tests","title":"Running the benchmark tests","text":"<p>The benchmark tests can be run using pytest. Run <code>pytest -m benchmark --durations 0 -v</code> to run these tests and report run duration.</p> <p>Note that benchmark tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m benchmark</code> to run them. They will also be excluded when calculating test coverage.</p> <p>These tests will be run as part of the <code>make test-all</code> make command.</p>"},{"location":"polars-cloud/","title":"Index","text":""},{"location":"polars-cloud/#introducing-polars-cloud","title":"Introducing Polars Cloud","text":"<p>DataFrame implementations always differed from SQL and databases. SQL could run anywhere from embedded databases to massive data warehouses. Yet, DataFrame users have been forced to choose between a solution for local work or solutions geared towards distributed computing, each with their own APIs and limitations.</p> <p>Polars is bridging this gap with Polars Cloud. Build on top of the popular open source project, Polars Cloud enables you to write DataFrame code once and run it anywhere. The distributed engine available with Polars Cloud allows to scale your Polars queries beyond a single machine.</p>"},{"location":"polars-cloud/#key-features-of-polars-cloud","title":"Key Features of Polars Cloud","text":"<ul> <li>Unified DataFrame Experience: Run a Polars query seamlessly on your local machine and at scale   with our new distributed engine. All from the same API.</li> <li>Serverless Compute: Effortlessly start compute resources without managing infrastructure with   options to execute queries on both CPU and GPU (coming soon).</li> <li>Any Environment: Start a remote query from a notebook on your machine, Airflow DAG, AWS   Lambda, or your server. Get the flexibility to embed Polars Cloud in any environment.</li> </ul>"},{"location":"polars-cloud/#install-polars-cloud","title":"Install Polars Cloud","text":"<p>Simply extend the capabilities of Polars with:</p> <pre><code>pip install polars polars_cloud\n</code></pre>"},{"location":"polars-cloud/#example-query","title":"Example query","text":"<p>To run your query in the cloud, simply write Polars queries like you are used to, but call <code>LazyFrame.remote()</code> to indicate that the query should be run remotely.</p>  Python <p> <code>ComputeContext</code> \u00b7 <code>LazyFrameRemote</code> <pre><code>import polars as pl\nimport polars_cloud as pc\n\nctx = pc.ComputeContext(workspace=\"your-workspace\", cpus=16, memory=64)\n\nquery = (\n    pl.scan_parquet(\"s3://my-dataset/\")\n    .group_by(\"l_returnflag\", \"l_linestatus\")\n    .agg(\n        avg_price=pl.mean(\"l_extendedprice\"),\n        avg_disc=pl.mean(\"l_discount\"),\n        count_order=pl.len(),\n    )\n)\n\n(\n    query.remote(context=ctx)\n    .sink_parquet(\"s3://my-dst/\")\n)\n</code></pre></p>"},{"location":"polars-cloud/#sign-up-today-and-start-your-30-day-trial","title":"Sign up today and start your 30 day trial","text":"<p>Polars Cloud is available to try with a 30 day free trial. You can sign up on cloud.pola.rs to get started.</p>"},{"location":"polars-cloud/#cloud-availability","title":"Cloud availability","text":"<p>Polars Cloud is available on AWS. Other cloud providers and on-premises solutions are on the roadmap and will become available in the upcoming months.</p>"},{"location":"polars-cloud/cli/","title":"CLI","text":"<p>Polars cloud comes with a command line interface (CLI) out of the box. This allows you to interact with polars cloud resources from the terminal.</p> <pre><code>pc --help\n\nusage: pc [-h] [-v] [-V] {authenticate,setup,login,organization,workspace,compute} ...\n\npositional arguments:\n  {authenticate,setup,login,organization,workspace,compute}\n    authenticate        Authenticate with Polars Cloud by loading stored credentials or otherwise logging in through the browser.\n    setup               Set up an organization and workspace to quickly run queries. Ideal to get started with Polars Cloud.\n    login               Authenticate with Polars Cloud by logging in through the browser.\n    organization        Manage Polars Cloud organizations.\n    workspace           Manage Polars Cloud workspaces.\n    compute             Manage Polars Cloud compute clusters.\n\noptional arguments:\n  -h, --help            show this help message and exit.\n  -v, --verbose         Output debug logging messages.\n  -V, --version         Display the version of the Polars Cloud client.\n</code></pre> <p>If you're just starting out with Polars Cloud then <code>pc setup</code> will guide you through setting up your environment to be able to quickly run queries.</p>"},{"location":"polars-cloud/connect-cloud/","title":"Connect to your cloud","text":"<p>Polars Cloud requires connection to your AWS environment to execute queries. After account registration, you'll set up an organization that manages connected workspaces and team access.</p> <p></p>"},{"location":"polars-cloud/connect-cloud/#workspace-setup","title":"Workspace setup","text":"<p>When you first access the Polars Cloud dashboard with a new account, you'll see a notification indicating your environment requires AWS connection. While you can explore the interface in this state, executing queries requires completing the setup process.</p> <p></p> <p>The setup process begins with naming your workspace. The default \"Personal Workspace\" works for individual use, but consider using team or project names for collaborative environments. This workspace name becomes part of your compute context configuration for remote query execution.</p> <p>Naming your workspace</p> <p>If you\u2019re unsure, you can use a temporary name. You can change the workspace name later under the workspace settings.</p> <p></p>"},{"location":"polars-cloud/connect-cloud/#aws-infrastructure-deployment","title":"AWS infrastructure deployment","text":"<p>The deployment process uses a CloudFormation template to provision the necessary IAM roles and permissions in your AWS environment. This creates the setup that allows Polars Cloud to execute queries.</p> <p></p> <p>For detailed information about the AWS resources and architecture, see the AWS Infrastructure page.</p> <p>CloudFormation permissions</p> <p>Users without CloudFormation deployment permissions can easily share the deployment URL with their operations team or AWS administrators.</p> <p>CloudFormation stack deployment typically completes within 5 minutes. You can monitor progress through either the AWS console or the Polars Cloud setup interface for real-time status updates.</p> <p></p> <p>Upon successful deployment, the setup confirms completion and provides access to the full Polars Cloud dashboard for immediate query execution.</p> <p></p> <p>You can now run your Polars queries remotely in the cloud. Go to the getting started section to your first query in minutes, learn more how to run queries remotely or manage your workspace to invite your team.</p> <p>One-time workspace setup</p> <p>Cloud environment connection is required once per workspace. Team members invited to connected workspaces can immediately execute remote queries without additional setup.</p>"},{"location":"polars-cloud/connect-cloud/#start-your-next-project","title":"Start your next project","text":"<p>With your workspace connected to AWS, you can execute Polars queries remotely. Consider these next actions:</p> <ul> <li>Run your first remote query with the getting started guide</li> <li>Learn about compute context configuration for performance   optimization</li> <li>Invite team members to your connected workspace to collaborate on your next   project.</li> </ul>"},{"location":"polars-cloud/faq/","title":"FAQ","text":"<p>On this page you can find answers to some frequently asked questions around Polars Cloud.</p>"},{"location":"polars-cloud/faq/#who-is-behind-polars-cloud","title":"Who is behind Polars Cloud?","text":"<p>Polars Cloud is built by the organization behind the open source Polars project. We are committed to improve Polars open source for all single machine workloads. Polars Cloud will extend Polars functionalities for remote and distributed compute.</p>"},{"location":"polars-cloud/faq/#where-does-the-compute-run","title":"Where does the compute run?","text":"<p>All compute runs in your own cloud environment. The main reason is that this ensures that your data never leaves your environment and that the compute is always close to your data.</p> <p>You can learn more about how this setup in the infrastructure section of the documentation.</p>"},{"location":"polars-cloud/faq/#can-you-run-polars-cloud-on-premises","title":"Can you run Polars Cloud on-premises?","text":"<p>Currently, Polars Cloud is only available to organizations that are on AWS. Support for on-premises infrastructure is on our roadmap and will become available soon.</p>"},{"location":"polars-cloud/faq/#what-does-polars-cloud-offer-me-beyond-polars","title":"What does Polars Cloud offer me beyond Polars?","text":"<p>Polars Cloud offers a managed service that enables scalable data processing with the flexibility and expressiveness of the Polars API. It extends the open source Polars project with the following capabilities:</p> <ul> <li>Distributed engine to scale workloads horizontally.</li> <li>Cost-optimized serverless architecture that automatically scales compute resources</li> <li>Built-in fault tolerance mechanisms ensuring query completion even during hardware failures or   system interruptions</li> <li>Comprehensive monitoring and analytics tools providing detailed insights into query performance   and resource utilization.</li> </ul>"},{"location":"polars-cloud/faq/#what-are-the-main-use-cases-for-polars-cloud","title":"What are the main use cases for Polars Cloud?","text":"<p>Polars Cloud enables data teams to scale their existing Polars queries beyond local machine limitations. Teams should use it when datasets exceed local memory, when computationally intensive operations require more processing power, or when data resides in cloud storage and shouldn't or can't be downloaded locally.</p> <p>The platform allows the same query to run on appropriately sized compute instances while being executable from any environment (e.g. notebooks, serverless functions, or orchestration platforms). This eliminates the need for teams to refactor their code into different tools or coordinate handovers between analytics and infrastructure teams when moving from development to production.</p>"},{"location":"polars-cloud/faq/#how-can-polars-cloud-integrate-with-my-workflow","title":"How can Polars Cloud integrate with my workflow?","text":"<p>One of our key priorities is ensuring that running remote queries feels as native and seamless as running them locally. Every user should be able to scale their queries effortlessly.</p> <p>Polars Cloud is completely environment agnostic. This allows you to run your queries from anywhere such as your own machine, Jupyter/Marimo notebooks, Airflow DAGs, AWS Lambda functions, or your servers. By not tying you to a specific platform, Polars Cloud gives you the flexibility to execute your queries wherever it best fits your workflow.</p>"},{"location":"polars-cloud/faq/#what-is-the-pricing-model-of-polars-cloud","title":"What is the pricing model of Polars Cloud?","text":"<p>Polars Cloud has a 'Pay-as-you-go' model. You pay a fixed price per vCPU per hour and only for the resources you use for executing your queries. This is on top of AWS related costs for used compute instances. Polars Cloud scales down to zero if no queries run. Find more information on our Payment and Billing page.</p>"},{"location":"polars-cloud/faq/#can-i-try-polars-cloud-for-my-use-case","title":"Can I try Polars Cloud for my use case?","text":"<p>Yes, Polars Cloud offers a 30 day free trial. The 30 day free trial will start, when you connect your first workspace to your cloud environment. During the trial period you can make full use of Polars Cloud features. See the Trial page for more details.</p>"},{"location":"polars-cloud/faq/#will-the-distributed-engine-be-available-in-open-source","title":"Will the distributed engine be available in open source?","text":"<p>The distributed engine is only available in Polars Cloud. There are no plans to make it available in the open source project. Polars is focused on single node compute, as it makes efficient use of the available resources. Users already report utilizing Polars to process hundreds of gigabytes of data on single (large) compute instance. The distributed engine is geared towards teams and organizations that are I/O bound or want to scale their Polars queries beyond single machines.</p>"},{"location":"polars-cloud/public-datasets/","title":"Public datasets","text":""},{"location":"polars-cloud/public-datasets/#public-datasets_1","title":"Public datasets","text":"<p>Start experimenting with Polars Cloud immediately using our curated public datasets. These datasets span different scale factors, letting you test performance across various data sizes\u2014from small exploratory queries to large-scale processing workloads.</p>"},{"location":"polars-cloud/public-datasets/#available-datasets","title":"Available datasets","text":"<p>PDSH - derived from TPC-H benchmark Standard analytical queries for testing joins, aggregations, and filtering operations. Queries available in the Polars benchmark repository.</p> <p>PDSDS - derived from TPC-DS benchmark Decision support dataset designed for complex analytical workloads.</p> <p>NYC Taxi - source: NYC.gov Real-world transportation data with temporal patterns and geospatial dimensions.</p>"},{"location":"polars-cloud/public-datasets/#usage","title":"Usage","text":"<p>Access any dataset directly from your Polars code and execute in Polars Cloud:</p> <pre><code>data = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/{dataset}/{scale_factor/year}/\",\n    storage_options={\"request_payer\": \"true\"}\n)\nquery = data.select().remote(ctx).execute()\n</code></pre> <p>Note: These buckets use AWS Requester Pays, meaning you pay only for pays the cost of the request and the data download from the bucket. The storage costs are covered.</p>"},{"location":"polars-cloud/public-datasets/#dataset-urls","title":"Dataset URLs","text":"<p>All datasets are hosted in AWS region <code>us-east-2</code> and use Requester Pays buckets.</p>"},{"location":"polars-cloud/public-datasets/#pdsh-tpc-h-derived","title":"PDSH (TPC-H derived)","text":"Scale Factor Size URL Pattern Format SF10 ~10GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/{filename}.parquet</code> Single files SF100 ~100GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/{table}/_.parquet</code> Partitioned SF1000 ~1TB <code>s3://polars-cloud-samples-us-east-2-prd/pdsh/sf1000/{table}/_.parquet</code> Partitioned"},{"location":"polars-cloud/public-datasets/#example","title":"Example","text":"<pre><code>data = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/lineitem.parquet\",\n    storage_options={\"request_payer\": \"true\"}\n)\n\npartitioned_data = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/lineitem/*.parquet\",\n    storage_options={\"request_payer\": \"true\"}\n)\n</code></pre>"},{"location":"polars-cloud/public-datasets/#pdsds-tpc-ds-derived","title":"PDSDS (TPC-DS derived)","text":"Scale Factor Size URL Pattern SF1 ~1GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsds/sf1/{filename}.parquet</code> SF10 ~10GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsds/sf10/{filename}.parquet</code> SF100 ~100GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsds/sf100/{filename}.parquet</code> SF300 ~300GB <code>s3://polars-cloud-samples-us-east-2-prd/pdsds/sf300/{filename}.parquet</code>"},{"location":"polars-cloud/public-datasets/#example_1","title":"Example","text":"<pre><code>data = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/store_sales.parquet\",\n    storage_options={\"request_payer\": \"true\"}\n)\n</code></pre>"},{"location":"polars-cloud/public-datasets/#nyc-taxi","title":"NYC Taxi","text":"Year URL Pattern 2023 <code>s3://polars-cloud-samples-us-east-2-prd/taxi/2023/{filename}.parquet</code> 2024 <code>s3://polars-cloud-samples-us-east-2-prd/taxi/2024/{filename}.parquet</code>"},{"location":"polars-cloud/public-datasets/#example_2","title":"Example","text":"<pre><code>data = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/taxi/2024/yellow_tripdata_2024-01.parquet\",\n    storage_options={\"request_payer\": \"true\"}\n)\n</code></pre>"},{"location":"polars-cloud/quickstart/","title":"Getting started","text":"<p>Polars Cloud is a managed compute platform for your Polars queries. It allows you to effortlessly run your local queries in your cloud environment without infrastructure management. By working in a 'Bring your own Cloud' model the data never leaves your environment.</p>"},{"location":"polars-cloud/quickstart/#installation","title":"Installation","text":"<p>Install the Polars Cloud python library in your environment</p> <pre><code>$ pip install polars polars-cloud\n</code></pre> <p>Create an account and login by running the command below.</p> <pre><code>$ pc authenticate\n</code></pre>"},{"location":"polars-cloud/quickstart/#connect-your-cloud","title":"Connect your cloud","text":"<p>Polars Cloud currently exclusively supports AWS as a cloud provider.</p> <p>Polars Cloud needs permission to manage hardware in your environment. This is done by deploying our cloudformation template. See our infrastructure section for more details.</p> <p>To set up your Polars Cloud environment and connect your cloud run you can either</p> <ul> <li>Run <code>pc setup</code> to guide you through creation and connecting via CLI.</li> <li>Or create an organization and workspace   via the browser.</li> </ul>"},{"location":"polars-cloud/quickstart/#run-your-queries","title":"Run your queries","text":"<p>Now that we are done with the setup, we can start running queries. You can write Polars like you're used and to only need to call <code>.remote()</code> on your <code>LazyFrame</code>. In the following example we create a compute cluster and run a simple Polars query.</p>  Python <p> <code>ComputeContext</code> \u00b7 <code>LazyFrameRemote</code> <pre><code>import polars_cloud as pc\nimport polars as pl\n\n# First, we need to define the hardware the cluster will run on.\n# This can be done by specifying the minimum CPU and memory or\n# by specifying the exact instance type in AWS.\n\nctx = pc.ComputeContext(memory=8, cpus=2, cluster_size=1)\n\n# Then we write a regular lazy Polars query. In this example\n# we compute the maximum of column.\n\nlf = pl.LazyFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [4, 4, 5],\n    }\n).with_columns(\n    pl.col(\"a\").max().over(\"b\").alias(\"c\"),\n)\n\n# At this point, the query has not been executed yet.\n# We need to call `.remote()` to signal that we want to run\n# on Polars Cloud and then `.execute()` send the query and execute it.\n\n(\n    lf.remote(context=ctx).execute().await_result()\n)\n\n# We can then wait for the result with `await_result()`.\n# The query and compute used will also show up in the\n# portal at https://cloud.pola.rs/portal/\n</code></pre></p>"},{"location":"polars-cloud/context/compute-context/","title":"Compute context introduction","text":"<p>The compute context defines the hardware configuration used to execute your queries. This can be either a single node or, for distributed execution, multiple nodes. This section explains how to set up and manage your compute context.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(\n    workspace=\"your-workspace\",\n    instance_type=\"t2.micro\",\n    cluster_size=2,\n    labels=[\"docs\"],\n)\n</code></pre></p>"},{"location":"polars-cloud/context/compute-context/#setting-the-context","title":"Setting the context","text":"<p>You can define your compute context in three ways:</p> <ol> <li>Use your workspace default</li> <li>Specify CPUs and RAM requirements</li> <li>Select a specific instance type</li> </ol>"},{"location":"polars-cloud/context/compute-context/#workspace-default","title":"Workspace default","text":"<p>In the Polars Cloud dashboard, you can set default requirements from your cloud service provider to be used for all queries. You can also manually define storage and the default cluster size.</p> <p>Polars Cloud will use these defaults if no other parameters are passed to the <code>ComputeContext</code>.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(workspace=\"your-workspace\")\n</code></pre></p> <p>Find out more about how to set workspace defaults in the workspace settings section.</p>"},{"location":"polars-cloud/context/compute-context/#define-hardware-specifications","title":"Define hardware specifications","text":"<p>You can directly specify the <code>cpus</code> and <code>memory</code> requirements in your <code>ComputeContext</code>. When set, Polars Cloud will select the most suitable instance type from your cloud service provider that meets the specifications. The requirements are lower bounds, meaning the machine will have at least that number of CPUs and memory.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(\n    workspace=\"your-workspace\",\n    memory=8,\n    cpus=2,\n)\n</code></pre></p>"},{"location":"polars-cloud/context/compute-context/#set-instance-type","title":"Set instance type","text":"<p>For more control, you can specify the exact instance type for Polars to use. This is useful when you have specific hardware requirements in a production environment.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(\n    workspace=\"your-workspace\",\n    instance_type=\"t2.micro\",\n    cluster_size=2\n)\n</code></pre></p>"},{"location":"polars-cloud/context/compute-context/#saving-the-compute-context","title":"Saving the compute context","text":"<p>To simplify configuration and enable cluster sharing, you can save your settings under a unique identifier called a manifest in Polars Cloud. This eliminates the need to specify all settings in every script and makes it easy to reconnect to or collaborate on the same compute cluster. You can create a manifest either programmatically by calling <code>register</code> on a <code>ComputeContext</code> or through the cloud portal interface.</p>  Python <p> <code>register</code> \u00b7 <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(\n    workspace=\"your-workspace\",\n    instance_type=\"t2.micro\",\n    cluster_size=2\n)\nctx.register(\"ComputeName\")\n</code></pre></p>"},{"location":"polars-cloud/context/compute-context/#applying-the-compute-context","title":"Applying the compute context","text":"<p>Once defined, you can apply your compute context to queries in three ways:</p> <ol> <li>By directly passing the context to the remote query:</li> </ol> <pre><code>query.remote(context=ctx).sink_parquet(...)\n</code></pre> <ol> <li>By globally setting the compute context. This way you set it once and don't need to provide it to    every <code>remote</code> call:</li> </ol> <pre><code>pc.set_compute_context(ctx)\n\nquery.remote().sink_parquet(...)\n</code></pre> <ol> <li>When a default compute context is set via the Polars Cloud dashboard. It is no longer required to    define a compute context.</li> </ol> <pre><code>query.remote().sink_parquet(...)\n</code></pre>"},{"location":"polars-cloud/context/plugins/","title":"Plugins and custom libraries","text":"<p>Polars Cloud supports extending functionality through plugins and user-defined functions (UDFs). This guide covers how to use these extensions in your workflows.</p>"},{"location":"polars-cloud/context/plugins/#expression-plugins","title":"Expression Plugins","text":"<p>Expression plugins are the preferred way to extend Polars functionality in cloud environments. They work seamlessly with Polars Cloud's remote execution.</p> <p>Plugins are compiled Rust functions that integrate directly into the Polars expression engine, allowing you to add custom functionality that behaves like native Polars operations. They're distributed as Python packages that register new expression methods.</p>"},{"location":"polars-cloud/context/plugins/#setting-up-your-environment","title":"Setting Up Your Environment","text":"<p>Start by creating a <code>requirements.txt</code> file with your dependencies.</p> <pre><code># requirements.txt\n\nnumpy==2.2.4\npolars-xdt==0.16.8\n</code></pre> <p>After that, include the file in the compute context. In the compute context, you can specify a <code>requirements.txt</code> file with additional packages to install on your compute instance.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(workspace=\"your-workspace\",\n    cpus=12,\n    memory=12,\n    requirements=\"requirements.txt\"\n)\n</code></pre></p>"},{"location":"polars-cloud/context/plugins/#using-plugins-in-remote-workflows","title":"Using Plugins in Remote Workflows","text":"<p>In this example query we use the polars-xdt plugin. This plugin offers extra datetime-related functionality which isn't in-scope for the main Polars library.</p> <p>Once installed, plugins extend the Polars expression API with new namespaces and methods. When importing the plugin, we can use its functionality just as on a local machine:</p>  Python <pre><code>import polars_xdt as xdt\n\nlf = pl.LazyFrame(\n    {\n        \"local_dt\": [\n            datetime(2020, 10, 10, 1),\n            datetime(2020, 10, 10, 2),\n            datetime(2020, 10, 9, 20),\n        ],\n        \"timezone\": [\n            \"Europe/London\",\n            \"Africa/Kigali\",\n            \"America/New_York\",\n        ],\n    }\n)\n\nquery = lf.with_columns(\n    xdt.from_local_datetime(\n        \"local_dt\", pl.col(\"timezone\"), \"UTC\"\n    ).alias(\"date\")\n)\n\nquery.remote(ctx).show()\n</code></pre> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 local_dt            \u2506 timezone         \u2506 date                    \u2502\n\u2502 ---                 \u2506 ---              \u2506 ---                     \u2502\n\u2502 datetime[\u03bcs]        \u2506 str              \u2506 datetime[\u03bcs, UTC]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-10-10 01:00:00 \u2506 Europe/London    \u2506 2020-10-10 00:00:00 UTC \u2502\n\u2502 2020-10-10 02:00:00 \u2506 Africa/Kigali    \u2506 2020-10-10 00:00:00 UTC \u2502\n\u2502 2020-10-09 20:00:00 \u2506 America/New_York \u2506 2020-10-10 00:00:00 UTC \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Plugins generally offer better performance than UDFs since they run compiled code. However, UDFs provide more flexibility for custom logic.</p>"},{"location":"polars-cloud/context/plugins/#using-other-libraries-in-remote-execution","title":"Using Other Libraries in Remote Execution","text":"<p>The process of using a user-defined function in your workflow is the same as with plugins. Ensure your dependencies are included in the requirements file you pass in the compute context. Your custom Python functions will execute on the remote compute instances:</p>  Python <pre><code>import numpy as numpy\n\nlf = pl.LazyFrame(\n    {\n        \"keys\": [\"a\", \"a\", \"b\", \"b\"],\n        \"values\": [10, 7, 1, 23],\n    }\n)\n\nq = lf.select(\n    pl.col(\"values\").map_batches(np.log, return_dtype=pl.Float64)\n)\n\nq.remote(ctx).show()\n</code></pre> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 values   \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.302585 \u2502\n\u2502 1.94591  \u2502\n\u2502 0.0      \u2502\n\u2502 3.135494 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"polars-cloud/context/proxy-mode/","title":"Proxy mode","text":"<p>Proxy mode routes queries through the Polars Cloud control plane rather than direct cluster connections. This approach provides enhanced security by eliminating the need to expose network ports on your compute cluster.</p> <p></p> <p>In proxy mode the control plane polls the compute plane to retrieve the latest information about running compute and queries.</p>"},{"location":"polars-cloud/context/proxy-mode/#reconnect-to-active-clusters","title":"Reconnect to active clusters","text":"<p>Clusters that are started in <code>proxy</code> mode allow you and your team to reconnect to the instance while it is active. There is more information available on the page about reconnecting to clusters.</p>"},{"location":"polars-cloud/context/reconnect/","title":"Reconnect to compute cluster","text":"<p>Polars Cloud allows you to reconnect to active compute clusters. This lets you reconnect to run multiple queries in a short time span, without having to wait for machines to spin up.</p>"},{"location":"polars-cloud/context/reconnect/#reconnect-with-a-manifest-recommended","title":"Reconnect with a Manifest (Recommended)","text":"<p>The preferred approach for creating a ComputeContext is through a manifest. A manifest defines all compute cluster properties and stores them under a unique name. This unique name lets you easily start or reconnect to a compute cluster. Each manifest can only have one active compute cluster at a time. Attempting to start an already active manifest will reconnect you to the existing cluster rather than spinning up a new one. Manifests can be created either through the cloud portal's Compute tab or by calling <code>.register(name=\"WorkspaceName\")</code> on a <code>ComputeContext</code>.</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(workspace=\"your-workspace\", cpus=4, memory=16)\nctx.register(\"ManifestName\")\n\n# On another process / machine\nctx = pc.ComputeContext(workspace=\"your-workspace\", name=\"ManifestName\")\nctx.start()\n</code></pre></p>"},{"location":"polars-cloud/context/reconnect/#manual","title":"Manual","text":""},{"location":"polars-cloud/context/reconnect/#starting-a-cluster","title":"Starting a cluster","text":"<p>We will start a simple cluster to show how you can reconnect. We will save the cluster ID so we can connect directly to the cluster in the following examples:</p>  Python <p> <code>ComputeContext</code> <pre><code>ctx = pc.ComputeContext(workspace=\"your-workspace\", cpus=4, memory=16)\n\nctx.start()\n</code></pre></p> <p>You can easily find the ID of your cluster by printing the ComputeContext to your console:</p>  Python <pre><code>print(ctx)\n</code></pre> <pre><code>ComputeContext(id=0198e107-xxxx-xxxx-xxxx-xxxxxxxxxxxx, cpus=4, memory=16, instance_type=None, storage=16, ...)\n</code></pre>"},{"location":"polars-cloud/context/reconnect/#reconnect-to-an-existing-cluster","title":"Reconnect to an existing cluster","text":"<p>If you lose connection or want to connect to a running cluster in your workspace, use <code>.connect</code> on <code>pc.ComputeContext</code>. This connects directly using the <code>compute_id</code> of the running cluster:</p>  Python <pre><code>ctx = pc.ComputeContext.connect('0198e107-xxxx-xxxx-xxxx-xxxxxxxxxxxx')\n</code></pre> <p>If you don't know your <code>compute_id</code>, use <code>.select()</code> to access an interactive interface where you can browse available clusters:</p>  Python <pre><code># Interactive interface to select the compute cluster you want to (re)connect to\nctx = pc.ComputeContext.select()\n</code></pre> <pre><code>Found 1 available clusters:\n-----------------------------------------------------------------------------------------------------------------------------\n#   Workspace       Type         vCPUs    Memory     Storage    Size       Runtime    ID\n-----------------------------------------------------------------------------------------------------------------------------\n1   your-workspace     Unknown    4        16 GiB     16 GiB     1          14m        0198e107-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n</code></pre>"},{"location":"polars-cloud/context/reconnect/#find-clusters-by-workspace","title":"Find clusters by workspace","text":"<p>You can find your <code>compute_id</code> by listing workspaces and then finding your cluster within a specific workspace. First, get your workspace ID using <code>pc.Workspace.list()</code>, then list all ComputeContexts for that workspace:</p>  Python <pre><code># List all clusters in the specified workspace\npc.ComputeContext.list('your-workspace-name')\n</code></pre> <pre><code>[(ComputeContext(id=0198e107-xxxx-xxxx-xxxx-xxxxxxxxxxxx, cpus=4, memory=16, instance_type=None, storage=16, ...),]\n</code></pre> <p>With the cluster <code>id</code> from the output above, you can then establish a connection using the same <code>.connect()</code> method shown in the previous section.</p>"},{"location":"polars-cloud/explain/authentication/","title":"Logging in","text":"<p>Polars Cloud allows authentication through short-lived authentication tokens. Authentication also supports programmatic workflows.</p> <ol> <li><code>authenticate</code> loads cached tokens when available, falling back to browser-based login when    needed. This is the recommended method for most development workflows since it reuses existing    sessions.</li> <li><code>login</code> always initiates browser-based authentication. Use this when you need to switch between    accounts.</li> </ol> <p>You can find more information about the usage of authentication options in the API Reference documentation.</p>"},{"location":"polars-cloud/explain/authentication/#credential-resolution","title":"Credential resolution","text":"<p>Both methods will attempt to authenticate with the following priority:</p> <ol> <li><code>POLARS_CLOUD_ACCESS_TOKEN</code> environment variable</li> <li><code>POLARS_CLOUD_CLIENT_ID</code> / <code>POLARS_CLOUD_CLIENT_SECRET</code> service account environment variables</li> <li>Any cached access or refresh tokens</li> </ol> <p>When all methods fail, the user will be redirected to the browser to log in and get a new access token. If you are in a non-interactive workflow and want to fail if there is no valid token you can run <code>pc.authenticate(interactive=False)</code> instead.</p> <p>After successful authentication, Polars Cloud stores the token in your OS config directory:</p> OS Value Example Linux <code>$XDG_CONFIG_HOME</code> or <code>$HOME/.config/</code> home/alice/.config macOS <code>$HOME/Library/Application Support</code> /Users/Alice/Library/Application Support Windows <code>{FOLDERID_RoamingAppData}</code> C:\\Users\\Alice\\AppData\\Roaming <p>You can override this path by setting the environment variable <code>POLARS_CLOUD_ACCESS_TOKEN_PATH</code>.</p>"},{"location":"polars-cloud/explain/authentication/#service-accounts","title":"Service accounts","text":"<p>Service accounts provide programmatic authentication for automated workflows, orchestration tools, and CI/CD pipelines. These accounts facilitate automated processes (typically in production environments) without requiring interactive login sessions. See the page on service accounts for more information.</p>"},{"location":"polars-cloud/explain/service-accounts/","title":"Using service accounts","text":"<p>Service accounts function as programmatic identities that enable secure machine-to-machine communication for remote data processing workflows. These accounts facilitate automated processes (typically in production environments) without requiring interactive login sessions.</p>"},{"location":"polars-cloud/explain/service-accounts/#create-a-service-account","title":"Create a service account","text":"<p>In the workspace settings page, navigate to the Service Accounts section. Here, you can create and manage service accounts associated with your workspace.</p> <p>To create a new Service Account:</p> <ol> <li>Click the Create New Service Account button</li> <li>Provide a name and description for the account</li> <li>Copy and securely store the Client ID and Client Secret</li> </ol> <p>Client ID and Secret visible once</p> <p>If you lose the Client ID or Client Secret, you will need to generate a new service account.</p>"},{"location":"polars-cloud/explain/service-accounts/#set-up-your-environment-to-use-a-service-account","title":"Set up your environment to use a Service Account","text":"<p>To authenticate using a Service Account, you must set environment variables. The following variables should be defined using the credentials provided when the service account was created:</p> <pre><code>export POLARS_CLOUD_CLIENT_ID=\"CLIENT_ID_HERE\"\nexport POLARS_CLOUD_CLIENT_SECRET=\"CLIENT_SECRET_HERE\"\n</code></pre>"},{"location":"polars-cloud/explain/service-accounts/#execute-query-with-service-account","title":"Execute query with Service Account","text":"<p>Once the environment variables are set, you do not need to log in to Polars Cloud manually. Your process will automatically connect to the workspace.</p>"},{"location":"polars-cloud/explain/service-accounts/#revoking-access-or-deleting-a-service-account","title":"Revoking access or deleting a Service Account","text":"<p>When a service account is no longer needed, it is recommended to revoke its access. Keep in mind that deleting a service account is irreversible, and any processes or applications relying on this account will lose access immediately.</p> <ol> <li>Navigate to the Workspace Settings page.</li> <li>Go to the Service Accounts section.</li> <li>Locate the service account to delete.</li> <li>Click the three-dot menu next to the service account and select Delete.</li> <li>Confirm the deletion of the Service Account.</li> </ol>"},{"location":"polars-cloud/integrations/","title":"Orchestrate your queries","text":"<p>Polars Cloud can be integrated and used in popular third-party orchestration tools. This approach lets you focus on data logic while Polars Cloud handles query execution, eliminating infrastructure management.</p> <p>Consider a typical workflow: generating a daily report by ingesting, transforming, and joining two datasets. We omit standard Polars code to focus on orchestration-specific steps: defining data flow and managing service accounts credentials for Polars Cloud authentication.</p> <p>The most commonly used data orchestrators are presented in alphabetical order:</p> <ul> <li>Airflow</li> <li>Dagster</li> <li>Prefect</li> </ul> <p>In a similar manner, we suggest a way to orchestrate Polars Cloud queries using AWS-native infrastructure:</p> <ul> <li>AWS lambda</li> </ul>"},{"location":"polars-cloud/integrations/airflow/","title":"Airflow","text":"<p>Execute Polars Cloud queries remotely using Airflow workflows through secure credential management. This section explains how to configure Airflow to submit and monitor Polars Cloud workloads using Airflow's built-in security mechanisms, keeping service account credentials isolated from DAG code while maintaining full workflow control.</p> <ol> <li>Secret manager    (docs):    is the  Airflow-recommended way to handle secrets. It involves setting up a    <code>Secret Backend</code> (many providers maintained by the community) in the <code>airflow.cfg</code> and let    Airflow workers pull the given secrets via the <code>airflow.models.Variable</code> API as    <code>Variable.get(\"&lt;SECRET NAME&gt;\")</code>. Note Airflow will pull the secret in its own metastore; if this    situation is not desirable, interacting with the cloud provider's Secret Manager (or any other    vault accessible via API) can simply be performed as a task of your DAG; see relevant official    docs (here is    AWS'    as an example).</li> <li>Environment variables    (docs):    load your environment variables into your containers after prefixing them by <code>AIRFLOW_VAR_</code>, for    instance <code>AIRFLOW_VAR_POLARS_CLOUD_CLIENT_ID</code> and <code>AIRFLOW_VAR_POLARS_CLOUD_CLIENT_SECRET</code>. They    should then be available through the <code>airflow.models.Variable</code> API as    <code>Variable.get(\"POLARS_CLOUD_CLIENT_ID\")</code>.</li> <li>Airflow <code>Variables</code>    (docs):    in the Airflow UI &gt; Admin &gt; Variables tab one can add/edit key: value pairs provided to Airflow,    which will make them accessible through the <code>airflow.models.Variable</code> API. Note these objects can    also be defined using the Airflow CLI (if accessible):    <code>airflow variables set POLARS_CLOUD_CLIENT_ID \"&lt;SECRET&gt;\"</code>.</li> </ol> <p>Some code snippets for solutions #1 and #2 described above:</p> <pre><code># pull secrets from the aws secret manager\n@resource\ndef service_account_from_aws(_):\n    client = boto3.client(\"secretsmanager\")\n    Variable.set(\n        \"client_id\",\n        client.get_secret_value(SecretId=\"&lt;SECRET NAME&gt;\").get(\"SecretString\"),\n    )\n    Variable.set(\n        \"client_secret\",\n        client.get_secret_value(SecretId=\"&lt;SECRET NAME&gt;\").get(\"SecretString\"),\n    )\n</code></pre> <pre><code># fetch [securely injected!] secrets from environment\n@resource\ndef service_account_from_env(_):\n    Variable.set(\"client_id\", os.getenv(\"POLARS_CLOUD_CLIENT_ID\"))\n    Variable.set(\"client_secret\", os.getenv(\"POLARS_CLOUD_CLIENT_SECRET\"))\n</code></pre> <p>Below a few lines of pseudo-code using Airflow' <code>TaskFlow</code> API:</p> <pre><code>import polars as pl\n\nfrom airflow.models import Variable\nfrom airflow.sdk import dag, task\nfrom polars_cloud import ComputeContext, authenticate, set_compute_context\n\n# define two compute contexts (two instance sizes)\nvm_small = ComputeContext(cpus=2, memory=4)\nvm_large = ComputeContext(cpus=4, memory=16)\n\n# queries will execute on the small vm by default\nset_compute_context(vm_small)\n\n@dag(...)\ndef taskflow():\n\n    @task()\n    def prepare_dataset_1():\n        pl.scan_csv(...).remote().sink_parquet(...)\n\n    @task()\n    def prepare_dataset_2():\n        pl.scan_ndjson(...).remote().sink_parquet(...)\n\n    # use a bigger machine for this operation\n    @task()\n    @set_compute_context(vm_large)\n    def join_datasets():\n        pl.scan_parquet(...).remote().sink_parquet(...)\n\n    # authenticate to polars cloud with the secrets created above\n    authenticate(\n        client_id=Variable.get(\"secret_id\"),\n        client_secret=Variable.get(\"secret_secret\"),\n    )\n\n    prepare_dataset_1()\n    prepare_dataset_2()\n    join_datasets()\n\ntaskflow()\n\n# stop the instances\nvm_small.stop()\nvm_large.stop()\n</code></pre>"},{"location":"polars-cloud/integrations/dagster/","title":"Dagster","text":"<p>Configure Polars Cloud authentication securely within Dagster pipelines using resource based secret management. This section details how to integrate Polars Cloud service account credentials with Dagster's resource pattern.</p> <p>Dagster implements secret management through Resources), which provide dependency injection for external services. To configure Polars Cloud authentication, define credentials through one of these standard approaches:</p> <ol> <li>Secret manager (recommended): pull the secret from a metastore (see official docs    of your secret manager; here is    AWS'    as an example).</li> <li>Environment variables: define the values as environment variables in your Dagster environment    (containers or else), and pick them up from your code or Dagster configuration (via the    <code>dagster.yaml</code> or <code>workspace.yaml</code>).</li> </ol> <p>Some code snippets for the solutions described above:</p> <pre><code># pull secrets from the aws secret manager\n@resource\ndef service_account_from_aws(_):\n    client = boto3.client(\"secretsmanager\")\n    return {\n        \"client_id\": client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n        \"client_secret\": client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n    }\n</code></pre> <pre><code># fetch [securely injected!] secrets from environment\n@resource\ndef service_account_from_env(_):\n    return {\n        \"client_id\": os.getenv(\"POLARS_CLOUD_CLIENT_ID\"),\n        \"client_secret\": os.getenv(\"POLARS_CLOUD_CLIENT_SECRET\"),\n    }\n</code></pre> <p>Below a few lines of pseudo-code to define a Dagster flow:</p> <pre><code>import os\nimport polars as pl\n\nfrom dagster import job, op, resource\nfrom polars_cloud import ComputeContext, authenticate, set_compute_context\n\n# define two compute contexts (two instance sizes)\nvm_small = ComputeContext(cpus=2, memory=4)\nvm_large = ComputeContext(cpus=4, memory=16)\n\n# queries will execute on the small vm by default\nset_compute_context(vm_small)\n\n@op(required_resource_keys={\"sa\"})\ndef prepare_dataset_1():\n    pl.scan_csv(...).remote().sink_parquet(...)\n\n@op(required_resource_keys={\"sa\"})\ndef prepare_dataset_2():\n    pl.scan_ndjson(...).remote().sink_parquet(...)\n\n# use a bigger machine for this operation\n@op(required_resource_keys={\"sa\"})\n@set_compute_context(vm_large)\ndef join_datasets():\n    pl.scan_parquet(...).remote().sink_parquet(...)\n\n@job(resource_defs={\"sa\": service_account_from_aws})\ndef report():\n    # authenticate to polars cloud with the secrets created above\n    authenticate(**sa)\n\n    prepare_dataset_1()\n    prepare_dataset_2()\n    join_datasets()\n\n# stop the instances\nvm_small.stop()\nvm_large.stop()\n</code></pre>"},{"location":"polars-cloud/integrations/lambda/","title":"AWS Lambda","text":"<p>Orchestrate Polars Cloud queries using AWS-native serverless infrastructure through EventBridge and Lambda. This section details how to implement scheduled query execution without infrastructure management by submitting workloads to Polars Cloud via Lambda functions while leveraging AWS Secrets Manager for secure credential handling.</p> <p>Tip</p> <p>Submitting a query does not require for the process submitting it to remain alive if the Polars Cloud compute context is *not* built as a regular <code>Python</code> context manager.</p>"},{"location":"polars-cloud/integrations/lambda/#lambda-function","title":"Lambda function","text":"<p>The first hurdle is providing an environment including <code>Polars</code> dependencies to the Lambda function; this can be done in various ways, all documented by AWS here. The most commonly used approach is via creating a <code>zip</code> package including dependencies. The code for the lambda function can be boiled down to the following (pseudo-code):</p> <pre><code>import boto3\nimport polars as pl\nimport polars_cloud as pc\n\nclient = boto3.client(\"secretsmanager\")\n\n# authenticate to polars cloud with the secrets created above\npc.authenticate(\n    client_id=client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n    client_secret=client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n)\n\n# define the compute context\ncc = pc.ComputeContext(cpus=2, memory=4)\n\n# submit the query\npl.scan_csv(...).remote(cc).sink_parquet(...)\n</code></pre> <p>Once the query is submitted the Lambda will gracefully exit, leaving the rest of the handling to Polars Cloud.</p>"},{"location":"polars-cloud/integrations/lambda/#triggering-rule","title":"Triggering rule","text":"<p>Since we are here not using any dedicated orchestrator infrastructure (like Airflow for instance) we can instead generate triggering rules in AWS EventBridge. Rules can be defined via the AWS Console (point-and-click) or via the AWS CLI, as documented here. A simple CRON rule should be enough to trigger your query to run at given interval.</p>"},{"location":"polars-cloud/integrations/prefect/","title":"Prefect","text":"<p>Configure Polars Cloud authentication securely within Prefect workflows using native secret management patterns. This section details how to integrate Polars Cloud service account credentials with Prefect's configuration system.</p> <p>Prefect implements secure credential handling through three standard approaches:</p> <ol> <li>Secret manager (recommended): pull the secret secret manager of your choice and    use it in your workflow (see official docs; here is    AWS'    as an example). One can also use the AWS-specific <code>Secret</code> <code>Block</code> (see below;    docs) to interact with    the AWS Secret Manager.</li> <li>Environment variables: load your environment variables into your running instance (container    or else).</li> <li><code>Block</code> system (docs): Prefect defined a    <code>Block</code> framework that can be used via the CLI    (<code>prefect block register -m prefect.blocks.system</code>) or directly in the code    (<code>from prefect.blocks.system import Secret</code>). A secret can be created via CLI (for instance):    <code>prefect block create secret polars-cloud-client-id</code> and retrieved from the code as    <code>Secret.load(\"polars-cloud-client-id\").get()</code>.</li> </ol> <p>Some code snippets for solutions #1 and #2 described above:</p> <pre><code># pull secrets from the aws secret manager\ndef service_account_from_aws(_):\n    client = boto3.client(\"secretsmanager\")\n    return {\n        \"client_id\": client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n        \"client_secret\": client.get_secret_value(SecretId=\"&lt;SECRET&gt;\").get(\"SecretString\"),\n    }\n</code></pre> <pre><code># fetch [securely injected!] secrets from environment\n@resource\ndef service_account_from_env(_):\n    return {\n        \"client_id\": os.getenv(\"POLARS_CLOUD_CLIENT_ID\"),\n        \"client_secret\": os.getenv(\"POLARS_CLOUD_CLIENT_SECRET\"),\n    }\n</code></pre> <p>Below a few lines of pseudo-code to define a Prefect flow:</p> <pre><code>import os\nimport polars as pl\n\nfrom polars_cloud import ComputeContext, authenticate, set_compute_context\nfrom prefect import flow, task\n\n# define two compute contexts (two instance sizes)\nvm_small = ComputeContext(cpus=2, memory=4)\nvm_large = ComputeContext(cpus=4, memory=16)\n\n# queries will execute on the small vm by default\nset_compute_context(vm_small)\n\n@task\ndef prepare_dataset_1():\n    pl.scan_csv(...).remote().sink_parquet(...)\n\n@task\ndef prepare_dataset_2():\n    pl.scan_ndjson(...).remote().sink_parquet(...)\n\n# use a bigger machine for this operation\n@task\n@set_compute_context(vm_large)\ndef join_datasets():\n    pl.scan_parquet(...).remote().sink_parquet(...)\n\n@flow(name=\"Daily report\")\ndef report():\n    # authenticate to polars cloud with the secrets created above\n    authenticate(**service_account_from_env())\n\n    prepare_dataset_1()\n    prepare_dataset_2()\n    join_datasets()\n\n# run the flow\nreport()\n\n# stop the instances\nvm_small.stop()\nvm_large.stop()\n</code></pre>"},{"location":"polars-cloud/organization/billing/","title":"Payment and billing","text":"<p>All payments and billing for Polars Cloud are handled via the AWS Marketplace. This integration provides a secure, streamlined way to manage your subscription, payments, and billing using AWS's trusted marketplace infrastructure.</p> <p>30-day Free Trial</p> <p>New organizations get full access to explore Polars Cloud for 30 days. The trial activates automatically when connecting your first workspace to AWS. After the trial you can easily subscribe via the AWS Marketplace.</p>"},{"location":"polars-cloud/organization/billing/#pricing","title":"Pricing","text":"<p>Polars Cloud uses usage-based pricing charged per vCPU hour of compute usage. Billing begins when instances are booted and ready to execute queries, and ends when users stop the instances or when we detect instances have been terminated externally. Instance startup time and shutdown time after stopping are not charged.</p>"},{"location":"polars-cloud/organization/billing/#subscribe-to-polars-cloud","title":"Subscribe to Polars Cloud","text":"<p>Subscribing to Polars Cloud requires an AWS account with billing permissions. You can access the marketplace listing either through the Billing page in your Polars Cloud organization or by searching \"Polars Cloud\" directly in AWS Marketplace.</p> <p>The subscription process connects your Polars Cloud organization to AWS billing, after which your organization status updates to reflect the marketplace connection. This typically completes within minutes, though allow up to an hour for status propagation.</p>"},{"location":"polars-cloud/organization/billing/#verify-your-organization-connection","title":"Verify Your Organization Connection","text":"<p>After subscribing via Marketplace, you'll be redirected to the Polars Cloud organization dashboard. Under <code>Billing</code>, the status will update to show your organization is connected to AWS Marketplace. If the status doesn't update within 15 minutes, contact our support team at support@polars.tech.</p>"},{"location":"polars-cloud/organization/billing/#request-and-accept-private-offers","title":"Request and Accept Private Offers","text":"<p>Customers with large-scale analytics workloads or enterprise customers requiring custom pricing, annual commitments, or specific terms can contact our team for a private offer. Reach out to the team at support@polars.tech.</p>"},{"location":"polars-cloud/organization/billing/#cost-and-usage-monitoring","title":"Cost and Usage Monitoring","text":"<p>All up to date data about Polars Cloud usage and costs can be found in the AWS Cost Explorer. You can find more information about usage per workspace in the Polars Cloud dashboard.</p>"},{"location":"polars-cloud/organization/billing/#manage-your-subscription","title":"Manage Your Subscription","text":""},{"location":"polars-cloud/organization/billing/#cancelling-your-subscription","title":"Cancelling Your Subscription","text":"<p>Cancel through the AWS Marketplace subscription management interface.</p> <p>Important: Unsubscribing will instantly stop all active queries in your organization. Ensure no critical workflows are running before cancelling.</p>"},{"location":"polars-cloud/organization/billing/#reactivate-subscription","title":"Reactivate Subscription","text":"<p>After cancelling your subscription, you can still access Polars Cloud but no workflows can be started. You can easily reactivate the subscription following the steps on the <code>Billing</code> page in Polars Cloud or via the AWS Marketplace.</p>"},{"location":"polars-cloud/organization/members/","title":"Manage members","text":"<p>The Members page allows you to manage who has access to the organization and see open invitations. You can invite new users by email/url and select any workspaces they will automatically join once they've accepted.</p>"},{"location":"polars-cloud/organization/members/#roles","title":"Roles","text":"<p>There are two types of roles:</p> <ol> <li><code>Admin</code> - Able to do anything a member can and additionally invite new members, change roles or    organization settings</li> <li><code>Member</code> - Able to see the organization and create new workspaces in it</li> </ol> <p>There is also the notion of implicit and explicit roles. An organization admin is implicitly also a workspace admin even though they are not an explicit member of the workspace.</p> <p>Organization admins are able to promote others to admin or demote other admins to member. The system enforces that there is always at least one organization admin. In case you are unable to contact the only organization admin, please contact Polars Cloud support at support@polars.tech.</p>"},{"location":"polars-cloud/organization/organizations/","title":"Set up organization","text":"<p>Organizations in Polars Cloud are the top-level entity and typically represent a company. They can have members, contain multiple workspaces and are used to manage billing.</p> <p>To set up an organization you can either use the dashboard or the following CLI commands:</p> <ul> <li>If you're just starting with Polars Cloud then you need to set up both an organization and   workspace:</li> </ul> <pre><code>pc setup\n</code></pre> <ul> <li>Or if you only want to set up an organization:</li> </ul> <pre><code>pc organization setup\n</code></pre> <p>For other subcommands you can execute the help command:</p> <pre><code>pc organization --help \n\nusage: pc organization [-h] [-v] [-t TOKEN] [-p TOKEN_PATH] {list,setup,delete,details} ...\n\npositional arguments:\n  {list,setup,delete,details}\n    list                List all active organizations\n    setup               Set up an organization\n    delete              Delete an organization\n    details             Print the details of an organization\n</code></pre>"},{"location":"polars-cloud/organization/start-trial/","title":"Start trial period","text":"<p>Your Polars Cloud free trial begins automatically the moment you connect your first workspace to your AWS environment. No credit card required.</p>"},{"location":"polars-cloud/organization/start-trial/#whats-included-during-the-trial","title":"What's Included During the Trial","text":"<p>During the 30-day free trial, you have full access to Polars Cloud to explore the platform in detail. This includes:</p> <ul> <li>Complete feature access: Use all Polars Cloud capabilities without restrictions</li> <li>Team collaboration: Invite team members to collaborate in your workspace</li> <li>Serverless compute: Run queries remotely without infrastructure management</li> <li>Distributed engine: Scale your Polars queries beyond a single machine</li> </ul>"},{"location":"polars-cloud/organization/start-trial/#cost-considerations-during-trial","title":"Cost Considerations During Trial","text":"<p>Polars Cloud is free during the trial. However, AWS may charge for the compute resources your queries consume, depending on your AWS agreement. Monitor your AWS billing dashboard to track resource usage during the trial.</p>"},{"location":"polars-cloud/organization/start-trial/#checking-your-trial-status","title":"Checking Your Trial Status","text":"<p>You can find more information about your trial on your Organization dashboard under <code>Billing</code>. Here you can find the current status and end date. You will also be prompted to set up billing here.</p>"},{"location":"polars-cloud/organization/start-trial/#subscribe-after-trial","title":"Subscribe After Trial","text":"<p>When your 30-day trial concludes, you'll be prompted to subscribe via the AWS Marketplace. Your data and workspace configurations remain intact, but new query execution pauses until subscription activation. Once subscribed, all functionality immediately returns.</p> <p>For more information about subscription options, see the Payment and Billing page.</p>"},{"location":"polars-cloud/providers/aws/infra/","title":"Infrastructure","text":"<p>Polars Cloud manages the hardware for you by spinning up and down raw EC2 instances. In order to do this it needs permissions in your own cloud environment. None of the resources below have costs associated with them. While no compute clusters are running Polars Cloud will not create any AWS costs. The recommended way of doing this is running <code>pc setup</code>.</p>"},{"location":"polars-cloud/providers/aws/infra/#recommended-setup","title":"Recommended setup","text":"<p>When you deploy Polars Cloud the following infrastructure is setup.</p> <ol> <li>A <code>VPC</code> and two <code>subnets</code> in which Polars EC2 workers can run.</li> <li>Two <code>security groups</code>. One for the default mode that allows direct communication between your    local environment and the cluster, and one for proxy mode, which does not have any public ports.</li> <li><code>PolarsWorker</code> IAM role. Polars EC2 workers run under this IAM role.</li> <li><code>UserInitiated</code> &amp; <code>Unattended</code> IAM role. The <code>UserInitiated</code> role has the permissions to start    Polars EC2 workers in your environment. The <code>Unattended</code> role can terminate unused compute    clusters that you might have forgot about.</li> </ol>"},{"location":"polars-cloud/providers/aws/infra/#security","title":"Security","text":"<p>By design Polars Cloud never has access to the data inside your cloud environment. The data never leaves your environment.</p>"},{"location":"polars-cloud/providers/aws/infra/#iam-permissions","title":"IAM permissions","text":"<p>The list below show an overview of the required permissions for each of the roles.</p> User <ul> <li>ec2:CreateTags</li> <li>ec2:RunInstances</li> <li>ec2:DescribeInstances</li> <li>ec2:DescribeInstanceTypeOfferings</li> <li>ec2:DescribeInstanceTypes</li> <li>ec2:TerminateInstances</li> <li>ec2:CreateFleet</li> <li>ec2:CreateLaunchTemplate</li> <li>ec2:CreateLaunchTemplateVersion</li> <li>ec2:DescribeLaunchTemplates</li> </ul> Unattended <ul> <li>ec2:DescribeInstances</li> <li>ec2:TerminateInstances</li> <li>ec2:DescribeFleets</li> <li>ec2:DeleteLaunchTemplate</li> <li>ec2:DeleteLaunchTemplateVersions</li> <li>ec2:DeleteFleets</li> <li>sts:GetCallerIdentity</li> <li>sts:TagSession</li> </ul> Worker <ul> <li>s3:PutObject</li> <li>s3:GetObject</li> <li>s3:ListBucket</li> </ul> <p>The permissions of the worker role and S3 access can be scoped to certain buckets, by adjusting the role within AWS.</p>"},{"location":"polars-cloud/providers/aws/infra/#bring-your-own-network","title":"Bring your own network","text":"<p>By default, Polars Cloud is deployed within its own dedicated network (including VPC, subnets, and related resources). However, you can choose to use your own network configuration by selecting the \"Deploy with custom template\" option. This allows you to deploy into an existing VPC and subnets of your choice.</p> <p>You can configure these subnets to align with your business requirements. By default, the Python client accessing Polars Cloud must have access, so the subnet must be public, but allows for access controls such as IP-whitelisting. For proxy mode, outbound traffic must be allowed.</p>"},{"location":"polars-cloud/providers/aws/infra/#custom-requirements","title":"Custom requirements","text":"<p>Depending on your enterprise needs or existing infrastructure, you may not require certain components (e.g. VPC, subnet) of the default setup of Polars Cloud. Or you have additional security requirements in place. Together with our team of engineers we can integrate Polars Cloud with your existing infrastructure. Please contact us directly.</p>"},{"location":"polars-cloud/providers/aws/permissions/","title":"Permissions","text":"<p>The workspace is an isolation for all resources living within your cloud environment. Every workspace has a single instance profile which defines the permissions for the compute. This profile is attached to the compute within your environment. By default, the profile can read and write from S3, but you can easily adjust depending on your own infrastructure stack.</p>"},{"location":"polars-cloud/providers/aws/permissions/#adding-or-removing-permissions","title":"Adding or removing permissions","text":"<p>If you want Polars Cloud to be able to read from other data sources than <code>S3</code> within your cloud environment you must provide the access control from directly within AWS. To do this go to <code>IAM</code> within the aws console and locate the role called <code>polars-&lt;WORKSPACE_NAME&gt;-IAMWorkerRole-&lt;slug&gt;</code>. Here you can adjust the permissions of the workspace for instance:</p> <ul> <li>Narrow down the S3 access to certain buckets</li> <li>Provide IAM access to rds database</li> </ul>"},{"location":"polars-cloud/providers/aws/permissions/#assuming-a-different-role","title":"Assuming a different role","text":"<p>To use a different IAM role with Polars Cloud beyond the default <code>polars-&lt;WORKSPACE_NAME&gt;-IAMWorkerRole-&lt;slug&gt;</code>, you need to configure cross-account role assumption. Set up your target role's trust policy to allow the Polars Cloud role to assume it by following the AWS cross-account role documentation. After configuring the trust relationship, specify the role ARN in the storage_options parameter when reading or writing data to have Polars assume that role for the operation.</p> <pre><code>import polars_cloud as pc\nimport polars as pl\n\nctx = pc.ComputeContext(cpus=1, memory=1)\nlf = pl.scan_parquet(\n    \"s3://your-bucket/foo.parquet\",\n    credential_provider=pl.CredentialProviderAWS(\n        assume_role={\n            \"RoleArn\": \"&lt;INSERT-DESIRED-ARN-HERE&gt;\"\n            \"RoleSessionName\": \"AssumedRole\"\n        },\n    ),\n)\n</code></pre>"},{"location":"polars-cloud/run/distributed-engine/","title":"Distributed queries","text":"<p>With the introduction of Polars Cloud, we also introduced the distributed engine. This engine enables users to horizontally scale workloads across multiple machines.</p> <p>Polars has always been optimized for fast and efficient performance on a single machine. However, when querying large datasets from cloud storage, performance is often constrained by the I/O limitations of a single node. By scaling horizontally, these download limitations can be significantly reduced, allowing users to process data at scale.</p> <p>Distributed engine is in open beta</p> <p>The distributed engine currently supports most of Polars API and datatypes. Follow the tracking issue to stay up to date.</p>"},{"location":"polars-cloud/run/distributed-engine/#using-distributed-engine","title":"Using distributed engine","text":"<p>To execute queries using the distributed engine, you can call the <code>distributed()</code> method.</p> <pre><code>lf: LazyFrame\n\nresult = (\n      lf.remote()\n      .distributed()\n      .execute()\n)\n</code></pre>"},{"location":"polars-cloud/run/distributed-engine/#example","title":"Example","text":"<p>This example demonstrates running query 3 of the PDS-H benchmarkon scale factor 100 (approx. 100GB of data) using Polars Cloud distributed engine.</p> <p>Run the example yourself</p> <p>Copy and paste the code to you environment and run it. The data is hosted in S3 buckets that use AWS Requester Pays, meaning you pay only for pays the cost of the request and the data download from the bucket. The storage costs are covered.</p> <p>First import the required packages and point to the S3 bucket. In this example, we take one of the PDS-H benchmarks queries for demonstration purposes.</p>  Python <pre><code>import polars as pl\nimport polars_cloud as pc\n\nlineitem_sf100 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/lineitem/*.parquet\",\n        storage_options={\"request_payer\": \"true\"})\ncustomer_sf100 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/customer/*.parquet\",\n        storage_options={\"request_payer\": \"true\"})\norders_sf100 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/orders/*.parquet\",\n        storage_options={\"request_payer\": \"true\"})\n</code></pre> <p>After that we define the query. Note that this query will also run on your local machine if you have the data available. You can generate the data with the Polars Benchmark repository.</p>  Python <pre><code>def pdsh_q3(customer, lineitem, orders):\n\n    return (\n        customer.filter(pl.col(\"c_mktsegment\") == \"BUILDING\")\n        .join(orders, left_on=\"c_custkey\", right_on=\"o_custkey\")\n        .join(lineitem, left_on=\"o_orderkey\", right_on=\"l_orderkey\")\n        .filter(pl.col(\"o_orderdate\") &lt; pl.date(1995, 3, 15))\n        .filter(pl.col(\"l_shipdate\") &gt; pl.date(1995, 3, 15))\n        .with_columns(\n            (pl.col(\"l_extendedprice\") * (1 - pl.col(\"l_discount\"))).alias(\"revenue\")\n        )\n        .group_by(\"o_orderkey\", \"o_orderdate\", \"o_shippriority\")\n        .agg(pl.sum(\"revenue\"))\n        .select(\n            pl.col(\"o_orderkey\").alias(\"l_orderkey\"),\n            \"revenue\",\n            \"o_orderdate\",\n            \"o_shippriority\",\n        )\n        .sort(by=[\"revenue\", \"o_orderdate\"], descending=[True, False])\n    )\n</code></pre> <p>The final step is to set the compute context and run the query. Here we're using 5 nodes with 10 CPUs and 10GB memory each. <code>Show()</code> will return the first 10 rows back to your environment. The query takes around xx seconds to execute.</p>  Python <pre><code>ctx = pc.ComputeContext(workspace=\"your-workspace\", cpus=4, memory=4, cluster_size=5)\n\npdsh_q3(customer_sf100, lineitem_sf100, orders_sf100)\n.remote(ctx)\n.distributed()\n.show()\n</code></pre> <p>Try on SF1000 (approx. 1TB of data)</p> <p>You can also run this example on a higher scale factor. The data is available on the same bucket. You can change the URL from <code>sf100</code> to <code>sf1000</code>.</p>"},{"location":"polars-cloud/run/query-profile/","title":"Query profiling","text":"<p>Monitor query execution across workers to identify bottlenecks, understand data flow, and optimize performance. You can see which stages are running, how data moves between workers, and where time is spent during execution.</p> <p>This visibility helps you optimize complex queries and better understand the distributed execution of queries.</p> Example query and dataset  You can copy and paste the example below to explore the feature yourself. Don't forget to change the workspace name to one of your own workspaces.  <pre><code>import polars as pl\nimport polars_cloud as pc\n\npc.authenticate()\n\nctx = pc.ComputeContext(workspace=\"your-workspace\", cpus=12, memory=12, cluster_size=4)\n\ndef pdsh_q3(customer, lineitem, orders):\n    return (\n        customer.filter(pl.col(\"c_mktsegment\") == \"BUILDING\")\n        .join(orders, left_on=\"c_custkey\", right_on=\"o_custkey\")\n        .join(lineitem, left_on=\"o_orderkey\", right_on=\"l_orderkey\")\n        .filter(pl.col(\"o_orderdate\") &lt; pl.date(1995, 3, 15))\n        .filter(pl.col(\"l_shipdate\") &gt; pl.date(1995, 3, 15))\n        .with_columns(\n            (pl.col(\"l_extendedprice\") * (1 - pl.col(\"l_discount\"))).alias(\"revenue\")\n        )\n        .group_by(\"o_orderkey\", \"o_orderdate\", \"o_shippriority\")\n        .agg(pl.sum(\"revenue\"))\n        .select(\n            pl.col(\"o_orderkey\").alias(\"l_orderkey\"),\n            \"revenue\",\n            \"o_orderdate\",\n            \"o_shippriority\",\n        )\n        .sort(by=[\"revenue\", \"o_orderdate\"], descending=[True, False])\n    )\n\nlineitem = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/lineitem/*.parquet\",\n    storage_options={\"request_payer\": \"true\"},\n)\ncustomer = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/customer/*.parquet\",\n    storage_options={\"request_payer\": \"true\"},\n)\norders = pl.scan_parquet(\n    \"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf100/orders/*.parquet\",\n    storage_options={\"request_payer\": \"true\"},\n)\n</code></pre>  Python <pre><code>result = pdsh_q3(customer, lineitem, orders).remote(ctx).distributed().execute()\n</code></pre> <p>The <code>await_profile</code> method can be used to monitor an in-progress query. It returns a QueryProfile object containing a DataFrame with information about which stages are being processed across workers, which can be analyzed in the same way as any Polars query.</p>  Python <pre><code>result.await_profile().data\n</code></pre> <p>Each row represents one worker processing a span. A span represents a chunk of work done by a worker, for example generating the query plan, reading data from another worker, or executing the query on that data. Some spans may output data, which is recorded in the output_rows column.</p> <pre><code>shape: (53, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 stage_number \u2506 span_name    \u2506 worker_id \u2506 start_time          \u2506 end_time           \u2506 output_rows \u2506 shuffle_bytes_written \u2506 shuffle_bytes_read \u2502\n\u2502 ---          \u2506 ---          \u2506 ---       \u2506 ---                 \u2506 ---                \u2506 ---         \u2506 ---                   \u2506                    \u2502\n\u2502 u32          \u2506 str          \u2506 str       \u2506 datetime[ns]        \u2506 datetime[ns]       \u2506 u64         \u2506 u64                   \u2506 u64                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 282794      \u2506 72395264              \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.820228585  \u2506 08:08:52.878229914 \u2506             \u2506                       \u2506                    \u2502\n\u2502 3            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 3643370     \u2506 932702720             \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:45.421053731  \u2506 08:08:45.600081475 \u2506             \u2506                       \u2506                    \u2502\n\u2502 5            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 282044      \u2506 723203264             \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.667547917  \u2506 08:08:52.718114297 \u2506             \u2506                       \u2506                    \u2502\n\u2502 5            \u2506 Shuffle read \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 null        \u2506 null                  \u2506 932702720          \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.694917167  \u2506 08:08:52.720657155 \u2506             \u2506                       \u2506                    \u2502\n\u2502 7            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 145179      \u2506 37165824              \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:53.039771274  \u2506 08:08:53.166535930 \u2506             \u2506                       \u2506                    \u2502\n\u2502 \u2026            \u2506 \u2026            \u2506 \u2026         \u2506 \u2026                   \u2506 \u2026                  \u2506 \u2026           \u2506 \u2026                     \u2506 \u2026                  \u2502\n\u2502 5            \u2506 Shuffle read \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 null        \u2506 null                  \u2506 72503808           \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.649434841  \u2506 08:08:52.667065947 \u2506             \u2506                       \u2506                    \u2502\n\u2502 6            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 283218      \u2506 72503808              \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.818787714  \u2506 08:08:52.880324797 \u2506             \u2506                       \u2506                    \u2502\n\u2502 4            \u2506 Shuffle read \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 null        \u2506 null                  \u2506 3979787264         \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:46.188322234  \u2506 08:08:50.871792346 \u2506             \u2506                       \u2506                    \u2502\n\u2502 1            \u2506 Execute IR   \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 15546044    \u2506 3979787264            \u2506 null               \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:40.325404872  \u2506 08:08:44.030028095 \u2506             \u2506                       \u2506                    \u2502\n\u2502 7            \u2506 Shuffle read \u2506 i-xxx     \u2506 2025-xx-xx          \u2506 2025-xx-xx         \u2506 null        \u2506 null                  \u2506 37165824           \u2502\n\u2502              \u2506              \u2506           \u2506 08:08:52.925442390  \u2506 08:08:52.962600065 \u2506             \u2506                       \u2506                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As each worker starts and completes each stage of the query, it notifies the lead worker. The <code>await_profile</code> method will poll the lead worker until there is an update from any worker, and then return the full profile data of the query.</p> <p>The QueryProfile object also has a summary property to return an aggregated view of each stage.</p>  Python <pre><code>result.await_profile().summary\n</code></pre> <pre><code>shape: (13, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 stage_number \u2506 span_name    \u2506 completed \u2506 worker_ids \u2506 duration     \u2506 output_rows \u2506 shuffle_bytes_written \u2506 shuffle_bytes_read \u2502\n\u2502 ---          \u2506 ---          \u2506 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---                   \u2506 ---                \u2502\n\u2502 u32          \u2506 str          \u2506 bool      \u2506 str        \u2506 duration[\u03bcs] \u2506 u64         \u2506 u64                   \u2506 u64                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6            \u2506 Shuffle read \u2506 true      \u2506 i-xxx      \u2506 1228\u00b5s       \u2506 0           \u2506 0                     \u2506 289546496          \u2502\n\u2502 5            \u2506 Shuffle read \u2506 true      \u2506 i-xxx      \u2506 140759\u00b5s     \u2506 0           \u2506 0                     \u2506 289546496          \u2502\n\u2502 4            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 1s 73534\u00b5s   \u2506 1131041     \u2506 289546496             \u2506 0                  \u2502\n\u2502 2            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 6s 944740\u00b5s  \u2506 3000188     \u2506 768048128             \u2506 0                  \u2502\n\u2502 5            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 167483\u00b5s     \u2506 1131041     \u2506 289546496             \u2506 0                  \u2502\n\u2502 \u2026            \u2506 \u2026            \u2506 \u2026         \u2506 \u2026          \u2506 \u2026            \u2506 \u2026           \u2506 \u2026                     \u2506 \u2026                  \u2502\n\u2502 4            \u2506 Shuffle read \u2506 true      \u2506 i-xxx      \u2506 4s 952005\u00b5s  \u2506 0           \u2506 0                     \u2506 255627121          \u2502\n\u2502 1            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 7s 738907\u00b5s  \u2506 72874383    \u2506 18655842048           \u2506 0                  \u2502\n\u2502 3            \u2506 Shuffle read \u2506 true      \u2506 i-xxx      \u2506 812807\u00b5s     \u2506 0           \u2506 0                     \u2506 768048128          \u2502\n\u2502 0            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 15s 2883\u00b5s   \u2506 323494519   \u2506 82814596864           \u2506 0                  \u2502\n\u2502 7            \u2506 Execute IR   \u2506 true      \u2506 i-xxx      \u2506 356662\u00b5s     \u2506 1131041     \u2506 289546496             \u2506 0                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"polars-cloud/run/remote-query/","title":"Execute remote query","text":"<p>Polars Cloud enables you to execute existing Polars queries on cloud infrastructure with minimal code changes. This approach allows you to process datasets that exceed local resources or use additional compute resources for faster execution.</p> <p>Polars Cloud is set up and connected</p> <p>This page assumes that you have created an organization and connected a workspace to your cloud environment. If you haven't yet, follow the steps on the Connect cloud environment page.</p>"},{"location":"polars-cloud/run/remote-query/#define-your-query-locally","title":"Define your query locally","text":"<p>The following example uses a query from the PDS-H benchmark suite, a derived version of the popular TPC-H benchmark. Data generation tools and additional queries are available in the Polars benchmark repository.</p>  Python <pre><code>import polars as pl\n\ncustomer = pl.scan_parquet(\"data/customer.parquet\")\nlineitem = pl.scan_parquet(\"data/lineitem.parquet\")\norders = pl.scan_parquet(\"data/orders.parquet\")\n\ndef pdsh_q3(customer, lineitem, orders):\n\n    return (\n        customer.filter(pl.col(\"c_mktsegment\") == \"BUILDING\")\n        .join(orders, left_on=\"c_custkey\", right_on=\"o_custkey\")\n        .join(lineitem, left_on=\"o_orderkey\", right_on=\"l_orderkey\")\n        .filter(pl.col(\"o_orderdate\") &lt; pl.date(1995, 3, 15))\n        .filter(pl.col(\"l_shipdate\") &gt; pl.date(1995, 3, 15))\n        .with_columns(\n            (pl.col(\"l_extendedprice\") * (1 - pl.col(\"l_discount\"))).alias(\"revenue\")\n        )\n        .group_by(\"o_orderkey\", \"o_orderdate\", \"o_shippriority\")\n        .agg(pl.sum(\"revenue\"))\n        .select(\n            pl.col(\"o_orderkey\").alias(\"l_orderkey\"),\n            \"revenue\",\n            \"o_orderdate\",\n            \"o_shippriority\",\n        )\n        .sort(by=[\"revenue\", \"o_orderdate\"], descending=[True, False])\n    )\n\n\npdsh_q3(customer, lineitem, orders).collect()\n</code></pre>"},{"location":"polars-cloud/run/remote-query/#scale-to-the-cloud","title":"Scale to the cloud","text":"<p>To execute your query in the cloud, you need to define a compute context. The compute context specifies the hardware to use when executing the query in the cloud. It allows you to set the workspace to execute your query and set compute resources. More elaborate options can be found on the Compute context introduction page.</p>  Python <p> <code>ComputeContext</code> <pre><code>import polars_cloud as pc\n\nctx = pc.ComputeContext(\n    # make sure to enter your own workspace name\n    workspace=\"your-workspace\",\n    memory=16,\n    cpus=12,\n)\n\n# Use a larger dataset available on S3\nlineitem_sf10 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/lineitem.parquet\",\n        storage_options={\"request_payer\": \"true\"})\ncustomer_sf10 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/customer.parquet\",\n        storage_options={\"request_payer\": \"true\"})\norders_sf10 = pl.scan_parquet(\"s3://polars-cloud-samples-us-east-2-prd/pdsh/sf10/orders.parquet\",\n        storage_options={\"request_payer\": \"true\"})\n\n# Your query remains the same\npdsh_q3(customer_sf10, lineitem_sf10, orders_sf10).remote(context=ctx).show()\n</code></pre></p> <p>Run the examples yourself</p> <p>All examples on this page can be executed using the sample datasets hosted on our S3 bucket. By including the <code>storage_option</code> parameter in your queries, you'll only incur S3 data transfer costs. No additional storage fees apply</p> <p>S3 bucket region</p> <p>The example datasets are hosted in the <code>us-east-2 S3 region</code>. Query performance may be affected if you're running operations from a distant geographic location due to network latency.</p>"},{"location":"polars-cloud/run/remote-query/#working-with-remote-query-results","title":"Working with remote query results","text":"<p>Once you've called <code>.remote(context=ctx)</code> on your query, you have several options for how to handle the results, each suited to different use cases and workflows.</p>"},{"location":"polars-cloud/run/remote-query/#write-to-storage","title":"Write to storage","text":"<p>The most straightforward approach for batch processing is to write results directly to cloud storage using <code>.sink_parquet()</code>. This method is ideal when you want to store processed data for later use or as part of a data pipeline:</p>  Python <pre><code># Replace the S3 url with your own to run the query successfully\n\npdsh_q3(customer_sf10, lineitem_sf10, orders_sf10).remote(context=ctx).sink_parquet(\"s3://your-bucket/processed-data/\")\n</code></pre> <p>Running <code>.sink_parquet()</code> will write the results to the defined bucket on S3. The query you execute runs in your cloud environment, and both the data and results remain secure in your own infrastructure. This approach is perfect for ETL workflows, scheduled jobs, or any time you need to persist large datasets without transferring them to your local machine.</p>"},{"location":"polars-cloud/run/remote-query/#inspect-results","title":"Inspect results","text":"<p>Using <code>.show()</code> will display the first 10 rows of the result so you can inspect the structure without having to transfer the whole dataset. This method displays the first 10 rows in your console or notebook.</p>  Python <pre><code>pdsh_q3(customer_sf10, lineitem_sf10,  orders_sf10).remote(context=ctx).show()\n</code></pre> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l_orderkey \u2506 revenue     \u2506 o_orderdate \u2506 o_shippriority \u2502\n\u2502 ---        \u2506 ---         \u2506 ---         \u2506 ---            \u2502\n\u2502 i64        \u2506 f64         \u2506 date        \u2506 i64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4791171    \u2506 440715.2185 \u2506 1995-02-23  \u2506 0              \u2502\n\u2502 46678469   \u2506 439855.325  \u2506 1995-01-27  \u2506 0              \u2502\n\u2502 23906758   \u2506 432728.5737 \u2506 1995-03-14  \u2506 0              \u2502\n\u2502 23861382   \u2506 428739.1368 \u2506 1995-03-09  \u2506 0              \u2502\n\u2502 59393639   \u2506 426036.0662 \u2506 1995-02-12  \u2506 0              \u2502\n\u2502 3355202    \u2506 425100.6657 \u2506 1995-03-04  \u2506 0              \u2502\n\u2502 9806272    \u2506 425088.0568 \u2506 1995-03-13  \u2506 0              \u2502\n\u2502 22810436   \u2506 423231.969  \u2506 1995-01-02  \u2506 0              \u2502\n\u2502 16384100   \u2506 421478.7294 \u2506 1995-03-02  \u2506 0              \u2502\n\u2502 52974151   \u2506 415367.1195 \u2506 1995-02-05  \u2506 0              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>.await_and_scan()</code> method returns a LazyFrame pointing to intermediate results stored temporarily in your S3 environment. These intermediate result files are automatically deleted after several hours. For persistent storage use <code>sink_parquet</code>. The output is a LazyFrame, allowing continued query chaining for further analysis.</p>  Python <pre><code>result = pdsh_q3(customer_sf10, lineitem_sf10,  orders_sf10).remote(context=ctx).await_and_scan()\n\nprint(result.collect())\n</code></pre> <pre><code>shape: (114_003, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l_orderkey \u2506 revenue     \u2506 o_orderdate \u2506 o_shippriority \u2502\n\u2502 ---        \u2506 ---         \u2506 ---         \u2506 ---            \u2502\n\u2502 i64        \u2506 f64         \u2506 date        \u2506 i64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4791171    \u2506 440715.2185 \u2506 1995-02-23  \u2506 0              \u2502\n\u2502 46678469   \u2506 439855.325  \u2506 1995-01-27  \u2506 0              \u2502\n\u2502 23906758   \u2506 432728.5737 \u2506 1995-03-14  \u2506 0              \u2502\n\u2502 23861382   \u2506 428739.1368 \u2506 1995-03-09  \u2506 0              \u2502\n\u2502 59393639   \u2506 426036.0662 \u2506 1995-02-12  \u2506 0              \u2502\n\u2502 \u2026          \u2506 \u2026           \u2506 \u2026           \u2506 \u2026              \u2502\n\u2502 44149381   \u2506 904.3968    \u2506 1995-01-16  \u2506 0              \u2502\n\u2502 34297697   \u2506 897.8464    \u2506 1995-03-06  \u2506 0              \u2502\n\u2502 25478115   \u2506 887.2318    \u2506 1994-11-28  \u2506 0              \u2502\n\u2502 52204674   \u2506 860.25      \u2506 1994-12-18  \u2506 0              \u2502\n\u2502 47255457   \u2506 838.9381    \u2506 1994-11-18  \u2506 0              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"polars-cloud/workspace/settings/","title":"Workspace configuration","text":"<p>On the workspace settings page you can set cluster defaults, create labels, add service accounts and change other workspace related settings.</p>"},{"location":"polars-cloud/workspace/settings/#workspace-configuration_1","title":"Workspace configuration","text":"<p>Configure workspace identity through the name and description fields. Use descriptive naming conventions that align with your project structure or data domains for easier workspace discovery and management.</p>"},{"location":"polars-cloud/workspace/settings/#default-compute-configuration","title":"Default Compute Configuration","text":"<p>Set default computational resources to standardize job execution without requiring explicit compute context configuration for each operation. Two configuration modes are available:</p> <p>Resource-based configuration allows precise specification of vCPUs, RAM, storage, and cluster size. Use this approach when you require control over resource allocation or have specific performance/cost optimization requirements. When the specific configuration is not available on AWS, the cheapest instance with at least the specifications is selected and used.</p> <p>Instance-based configuration leverages cloud provider instance types. This approach simplifies configuration by using pre-optimized instance configurations and is typically more cost-effective for standard workloads.</p> <p>Default compute configurations eliminate the need to explicitly define a compute context. More information on configuration can be found in the section on setting the compute context.</p>"},{"location":"polars-cloud/workspace/settings/#query-and-compute-labels","title":"Query and compute labels","text":"<p>Labels help organize and categorize queries and compute within your workspace. Labels can only be managed through the dashboard interface.</p> <pre><code>ctx= plc.ComputeContext(\n    workspace=\"your-workspace\",\n    labels=[\"docs\", \"user-guide\"],\n)\n</code></pre>"},{"location":"polars-cloud/workspace/settings/#service-account-management","title":"Service Account Management","text":"<p>Service accounts allow scripts to authenticate without human intervention. Each service account generates an authentication token with workspace-scoped permissions.</p> <p>Use clear and descriptive names that match your project structure or data domain to improve clarity. Refer to the Use service accounts section for implementation patterns and security best practices.</p>"},{"location":"polars-cloud/workspace/settings/#disable-workspace","title":"Disable workspace","text":"<p>Workspace admins can disable workspaces removing all data, terminating active processes, and revoking all access tokens. This action cannot be reversed.</p> <p>Ensure data export and backup completion before disabling workspaces containing critical datasets or analysis artifacts.</p>"},{"location":"polars-cloud/workspace/team/","title":"Manage team","text":"<p>The Team page allows you to manage who has access to the workspace. You can invite already existing organization members to join your workspace or invite by email address where they will both join the organization and workspace once they've accepted.</p>"},{"location":"polars-cloud/workspace/team/#roles","title":"Roles","text":"<p>There are two types of roles:</p> <ol> <li><code>Admin</code> - Able to do anything a member can and additionally invite new members, change roles or    workspace settings</li> <li><code>Member</code> - Able to start compute, run queries and inspect results but not able to change settings    or invite others.</li> </ol> <p>There is also the notion of implicit and explicit roles. An organization admin is implicitly also a workspace admin even though they are not an explicit member of the workspace. You can still add them as an explicit member so they keep access to the workspace even when they are demoted down from organization admin.</p> <p>Workspace admins are able to promote others to admin or demote other admins to member. The system enforces that there is always at least one workspace admin. In case you are unable to contact the only admin of a workspace, you can ask an organization admin for help.</p>"},{"location":"polars-on-premises/","title":"Introducing Polars on-premises","text":"<p>Interested in running Polars on-premises? Sign up here to apply.</p> <p>After installing Polars on-premises either on bare-metal or on Kubernetes, you can connect to your cluster using the Polars Cloud Python client.</p> <pre><code>import polars as pl\nimport polars_cloud as pc\n\n# Connect to your Polars on-premises cluster\nctx = pc.ClusterContext(compute_address=\"your-cluster-compute-address\", insecure=True)\nquery = (\n    pl.LazyFrame()\n    .with_columns(a=pl.arange(0, 100000000).sum())\n    .remote(ctx)\n    .distributed()\n    .execute()\n)\nprint(query.await_result())\n</code></pre>"},{"location":"polars-on-premises/bare-metal/environment-variables/","title":"Environment variables","text":""},{"location":"polars-on-premises/bare-metal/environment-variables/#polars","title":"Polars","text":"Variable Description <code>POLARS_TEMP_DIR</code> Override the default temporary directory Polars uses for scratch files and some I/O operations. <p>You may also set any Polars OSS-recognized environment variables.</p>"},{"location":"polars-on-premises/bare-metal/environment-variables/#polars-on-premises","title":"Polars on-premises","text":"Variable Description <code>OTLP_ENDPOINT</code> Target endpoint for sending OTLP traces/metrics/logs to your OpenTelemetry collector/observability stack.e.g. <code>http://otel-collector:4317</code>. <code>OTEL_SERVICE_INSTANCE_ID</code> OpenTelemetry <code>service.instance.id</code> that uniquely identifies this node in telemetry.Must match <code>instance_id</code>. <code>PLC_LOG_LEVEL</code> Controls logging verbosity for the Polars on-premises components (e.g. scheduler/worker).e.g. <code>Info</code>, <code>Debug</code>, <code>Trace</code>, etc. (follows the Rust naming)."},{"location":"polars-on-premises/bare-metal/getting-started/","title":"Getting started","text":"<p>First of all, make sure to obtain a license for Polars on-premises by signing up here. You will receive a link to download our binary named <code>polars-on-premises</code> as well as a JSON-formatted license for running Polars on-premises.</p>"},{"location":"polars-on-premises/bare-metal/getting-started/#reading-the-license","title":"Reading the license","text":"<p>The license can be read by running the following command:</p> <pre><code>$ ./polars-on-premises service --print-eula /path/to/license.json\n</code></pre>"},{"location":"polars-on-premises/bare-metal/getting-started/#running-the-binary","title":"Running the binary","text":"<p>The main entrypoint is as follows:</p> <pre><code>$ ./polars-on-premises service --config-path /etc/polars-cloud/config.toml\n</code></pre> <p>However, the service requires quite some configuration to get started. Below you can find an example scheduler and worker config, and you can find the full configuration reference here.</p>"},{"location":"polars-on-premises/bare-metal/getting-started/#quick-start","title":"Quick start","text":"<p>To get started fast, you can use the following configuration. It enables the scheduler, worker, observatory, and monitoring components. It writes query output data and shuffle data to a local directory.</p> <pre><code>cluster_id = \"polars-cluster\"\ninstance_id = \"node-0\"\nlicense = \"./license.json\" # Path to your Polars on-premises license. This is a JSON file containing your company name, license expiry, and license signature.\n\n# Component that receives the Polars queries from the Python client.\n[scheduler]\nenabled = true\nallow_local_sinks = true\nanonymous_result_location.local.path = \"./results-data\"\nn_workers = 1\n\n# Component that receives and executes tasks from the scheduler.\n[worker]\nenabled = true\nshuffle_location.local.path = \"./shuffle-data-path\"\ntask_service.public_addr = \"127.0.0.1\"\nshuffle_service.public_addr = \"127.0.0.1\"\n\n# Component that receives query profiling and host metrics.\n[observatory]\nenabled = true\nmax_metrics_bytes_total = 30000\ndatabase_path = \"./observatory/\"\n\n# Enables exporting query profiles and host metrics to the observatory service.\n[monitoring]\nenabled = true\n\n# Explicitly define that node-0 is the leader node. The leader node should run the observatory and monitoring components.\n[static_leader]\nleader_instance_id = \"node-0\"\nobservatory_service.public_addr = \"127.0.0.1\"\nscheduler_service.public_addr = \"127.0.0.1\"\n</code></pre>"},{"location":"polars-on-premises/bare-metal/getting-started/#configuration","title":"Configuration","text":"<p>The complete configuration reference can be found here.</p>"},{"location":"polars-on-premises/bare-metal/python-environment/","title":"Python environment","text":"<p>A major point of Polars on-premises is the system requirements regarding the Python version and Python dependencies, which need to be identical on the client, the scheduler, and all workers. The easiest method to achieve this is having a system-wide Python environment and globally installed packages. We recommend however setting up a virtual environment (<code>uv</code> makes this very easy, including maintaining a given Python version).</p> <p>The minimal requirement for running <code>polars-on-premises</code> is the <code>polars</code> package.</p> <p>!!! info \"Version pinning\" Each release of <code>polars-on-premises</code> is pinned to a single <code>polars</code> release, which can be found in the release announcement. <code>shell export PINNED_VERSION=1.35.2 # for instance</code></p>"},{"location":"polars-on-premises/bare-metal/python-environment/#system-wide-installation","title":"System-wide installation","text":"<pre><code>$ uv pip install --break-system-packages -r requirements.txt polars[cloudpickle]==$PINNED_VERSION\n$ ./polars-on-premises service --config-path /etc/polars-cloud/config.toml\n</code></pre>"},{"location":"polars-on-premises/bare-metal/python-environment/#virtual-environment","title":"Virtual Environment","text":"<pre><code>$ uv venv .venv\n$ source .venv/bin/activate\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(uv run python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\")\n$ uv pip install -r requirements.txt polars[cloudpickle]==$PINNED_VERSION\n$ ./polars-on-premises service --config-path /etc/polars-cloud/config.toml\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/anonymous-results/","title":"Anonymous results","text":"<p>For remote Polars queries without a specific output sink, Polars on-premises can automatically add an output sink. We call this sink an anonymous results sink. Infrastructure-wise, these sinks can be backed by S3-compatible storage or another shared filesystem accessible from all worker nodes and the Python client. The data written to this location is not automatically deleted, so you need to configure a retention policy for this data yourself.</p>"},{"location":"polars-on-premises/bare-metal/configuration/anonymous-results/#shared-filesystem","title":"Shared filesystem","text":"<p>If your infrastructure has some shared storage file system, such as NFS (or CephFs, etc.), you can use that here. An example configuration is shown below:</p> <pre><code>[scheduler]\nenabled = true\nallow_local_sinks = true # required for local anonymous results\nanonymous_results.local.path = \"/mnt/storage/polars/anonymous-results\"\n</code></pre> <p>Note that you must enable <code>allow_local_sinks</code> to allow query results to be written to a local path.</p> <p>Make sure that this exact path is reachable from all worker nodes and the Python client. If the Python client does not have access to this path, it won't be able to download the anonymous results, but it will still be able to receive query status updates.</p>"},{"location":"polars-on-premises/bare-metal/configuration/anonymous-results/#s3-compatible-storage","title":"S3 compatible storage","text":"<p>To store anonymous results in S3 compatible storage, you can configure it as shown below. The credentials specified are automatically used in the worker. Once the anonymous results are written, the scheduler also creates a presigned URL for the Python client to download the result from the S3 location.</p> <pre><code>[scheduler]\nenabled = true\nanonymous_results.s3.url = \"s3://bucket/path/to/key\"\nanonymous_results.s3.aws_secret_access_key = \"YOURSECRETKEY\"\nanonymous_results.s3.aws_access_key_id = \"YOURACCESSKEY\"\n</code></pre> <p>If you self-host an S3 compatible storage solution, you can override the <code>aws_endpoint_url</code> configuration option.</p> <pre><code>[scheduler]\nanonymous_results.s3.url = \"s3://bucket/path/to/key\"\nanonymous_results.s3.aws_endpoint_url = \"http://your-s3-compatible-storage-host:8080\"\n</code></pre> <p>Make sure that this endpoint is reachable from all worker nodes and the Python client. If the Python client does not have access to this endpoint, it won't be able to download the anonymous results, but it will still be able to receive query status updates.</p> <p>The allowed keys under <code>anonymous_results.s3</code> are the same as in <code>scan_parquet()</code>(e.g. <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, <code>aws_session_token</code>, <code>aws_region</code>). We currently only support the AWS keys of the <code>storage_options</code> dictionary, but note that you can use any other cloud provider that supports the S3 API, such as MinIO or DigitalOcean Spaces.</p>"},{"location":"polars-on-premises/bare-metal/configuration/example-configurations/","title":"Example configurations","text":""},{"location":"polars-on-premises/bare-metal/configuration/example-configurations/#ports-updates","title":"Ports updates","text":"Cluster with all ports updated (single node) <pre><code>cluster_id = \"cluster-1\"\ninstance_id = \"node-1\"\nlicense = \"/etc/polars/license.json\"\n\n[scheduler]\nenabled = true\nn_workers = 1\nclient_service.bind_addr.port = 4001\nworker_service.bind_addr.port = 4002\n\n[worker]\nenabled = true\ntask_service.bind_addr.port = 4005\ntask_service.public_addr = \"127.0.0.1:4005\"\nshuffle_service.bind_addr.port = 4006\nshuffle_service.public_addr = \"127.0.0.1:4006\"\n\n[observatory]\nenabled = true\nmax_metrics_bytes_total = 180000\ndatabase_path = \"/opt/db/observatory.db\"\nservice.bind_addr.port = 4003\n\n[observatory.rest_api]\nenabled = true\nservice.bind_addr.port = 4004\n\n[monitoring]\nenabled = true\n\n[static_leader]\nleader_instance_id = \"node-1\"\nobservatory_service.public_addr = \"127.0.0.1:4003\"\nscheduler_service.public_addr = \"127.0.0.1:4002\"\n</code></pre> Cluster with all ports updated (multi node)  Scheduler  <pre><code>cluster_id = \"cluster-1\"\ninstance_id = \"node-1\"\n\n[scheduler]\nenabled = true\nn_workers = 1\nclient_service.bind_addr.port = 4001\nworker_service.bind_addr.port = 4002\n\n[observatory]\nenabled = true\nmax_metrics_bytes_total = 180000\ndatabase_path = \"/opt/db/observatory.db\"\nservice.bind_addr.port = 4003\n\n[observatory.rest_api]\nenabled = true\nservice.bind_addr.port = 4004\n\n[monitoring]\nenabled = true\n\n[static_leader]\nleader_instance_id = \"node-1\"\nobservatory_service.public_addr = \"127.0.0.1:4003\"\nscheduler_service.public_addr = \"127.0.0.1:4002\"\n</code></pre>  Worker  <pre><code>cluster_id = \"cluster-1\"\ninstance_id = \"node-2\"\n\n[worker]\nenabled = true\ntask_service.bind_addr.port = 4005\ntask_service.public_addr = \"127.0.0.1:4005\"\nshuffle_service.bind_addr.port = 4006\nshuffle_service.public_addr = \"127.0.0.1:4006\"\n\n[monitoring]\nenabled = true\n\n[static_leader]\nleader_instance_id = \"node-1\"\nobservatory_service.public_addr = \"127.0.0.1:4003\"\nscheduler_service.public_addr = \"127.0.0.1:4002\"\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/license/","title":"License","text":"<p>Polars on-premises requires a license key to run. If you haven't contacted Polars yet, sign up here to apply. A license key looks like this:</p> <pre><code>{ \"params\": { \"expiry\": \"2026-01-31T23:59:59Z\", \"name\": \"Company\" }, \"signature\": \"...\" }\n</code></pre> <p>At the top level of the configuration file, you need to specify the path to the license file using the <code>license</code> key. For example:</p> <pre><code>cluster_id = \"polars-cluster-dev\"\ninstance_id = \"scheduler\"\nlicense = \"/etc/polars/license.json\"\n# ...\n</code></pre> <p>The cluster verifies the license key periodically, and will shutdown once the license expires.</p>"},{"location":"polars-on-premises/bare-metal/configuration/license/#eula-license","title":"EULA license","text":"<p>Polars on-premises is licensed under the End User License Agreement (EULA) which can be found in the Polars on-premises binary. The EULA must be accepted by setting the <code>POLARS_EULA_ACCEPTED</code> environment variable to <code>1</code>. If the environment variable is not set, the executable will print the EULA and exit. You can also manually print the EULA using the following command:</p> <pre><code>polars-on-premises --print-eula\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/monitoring/","title":"Profiling and host metrics","text":"<p>The profiling and host metrics system consists of a sender and receiving side. The observatory component's main task is to receive telemetry data from the cluster's nodes. The observatory component can currently only be enabled on nodes with the scheduler component enabled. The monitoring component is used to send telemetry data to the observatory. An example configuration is shown below.</p> <pre><code># ... global configuration\n\n[scheduler]\nenabled = true\n# ... continue configuration for scheduler\n\n[observatory]\nenabled = true\nmax_metrics_bytes_total = 5760000\ndatabase_path = \"./observatory/\"\n\n[monitoring]\nenabled = true\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/monitoring/#observatory-data-stores","title":"Observatory data stores","text":"<p>The observatory stores profiling data in an SQLite database file. The location of this file can be configured as shown above using the <code>database_path</code> key. The storage needed for this file is in the order of several tens of MB, depending on the number of queries and their complexity.</p> <p>The observatory stores host metrics in a pre-allocated buffer in memory. The size of this buffer can be configured using the <code>max_metrics_bytes_total</code> key. And since the required memory for these metrics can quickly rise if a cluster is kept alive for a long time, there is no default value for this key. You can estimate the required memory at around 50 bytes per second per worker node. So for a cluster with 32 worker nodes, with a history of 1 hour, you would need around <code>50 * 32 * 3600 = 5760000</code> bytes, or 5.7 MB.</p>"},{"location":"polars-on-premises/bare-metal/configuration/monitoring/#host-metrics-exporter","title":"Host metrics exporter","text":"<p>The host metrics exporter supports collecting CPU and memory usage metrics from the process tree, and cgroups v1 and v2. Since traversing the process tree, and collecting metrics for each process is quite compute expensive, and a cgroup's usage can be instantly queried, we recommend to always run Polars on-premises in a cgroup.</p> <p>If you don't use the dashboard's host metrics feature, we recommend disabling the host metrics exporter.</p> <pre><code>[monitoring]\nenabled = true\n\n[monitoring.host_metrics]\nenabled = false\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/monitoring/#disabling-observatory-and-monitoring","title":"Disabling observatory and monitoring","text":"<p>If you do not intend to use the dashboard, or the Python client's <code>result.await_profile()</code> method, you can disable the observatory and monitoring components completely. Queries will still be executed, but profiling data will not be collected.</p> <pre><code>[observatory]\nenabled = false\n\n[monitoring]\nenabled = false\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/network-addresses/","title":"Network addresses","text":"<p>Polars on-premises exposes several network services for communication between nodes. Each of these services can be configured to bind to a specific network address. This can be important when running a node on a specific network interface or when preventing port collisions. Most of the addresses are configured by default, except:</p> <ul> <li>The worker services require a public address when the bind address is <code>0.0.0.0</code>.</li> <li>The <code>static_leader</code> section requires the public address of both the scheduler and observatory   services.</li> </ul>"},{"location":"polars-on-premises/bare-metal/configuration/network-addresses/#scheduler","title":"Scheduler","text":"<p>The scheduler has two main services: the client service and the worker service. The client service is used by the Python client to submit queries and receive query status updates. The worker service is used by the workers to register and send heartbeats. By default, they bind to <code>0.0.0.0:5051</code> and <code>0.0.0.0:5050</code>, respectively.</p> <pre><code>instance_id = \"scheduler\"\n\n[scheduler]\nenabled = true\nclient_service.bind_addr = \"0.0.0.0:5051\"\nworker_service.bind_addr = \"0.0.0.0:5050\"\n# ...\n\n[static_leader]\nleader_instance_id = \"scheduler\"\nscheduler_service.public_addr = \"192.168.1.2:5051\"\n# ...\n</code></pre> <pre><code>instance_id = \"worker-0\"\n\n[worker]\nenabled = true\n# ...\n\n[static_leader]\nleader_instance_id = \"scheduler\"\nscheduler_service.public_addr = \"192.168.1.2:5051\"\n# ...\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/network-addresses/#observatory","title":"Observatory","text":"<p>The observatory consists of two main components: the profiler and the dashboard with its REST API. The profiler service receives OpenTelemetry traces from all cluster nodes. The other service exposes an API for accessing the query plans, profiling data, and host metrics.</p> <p>The observatory service (profiling) binds to <code>0.0.0.0:5049</code> by default. To enable sending traces to the observatory, you need to enable the <code>monitoring</code> section in the configuration of the second node, and configure the observatory service public address in the static leader section on the worker nodes.</p> <pre><code>instance_id = \"scheduler\"\n# ...\n\n[scheduler]\nenabled = true\n# ...\n\n[observatory]\nenabled = true\nservice.bind_addr = \"0.0.0.0:5049\" # Defaults to 0.0.0.0:5049\n[observatory.rest_api]\nenabled = true\nservice.bind_addr = \"0.0.0.0:3001\" # Defaults to 0.0.0.0:3001\n\n[static_leader]\nleader_instance_id = \"scheduler\"\nobservatory_service.public_addr = \"192.168.1.2:5049\"\n# ...\n</code></pre> <pre><code>instance_id = \"worker-0\"\n# ...\n\n[worker]\nenabled = true\n# ...\n\n[monitoring]\nenabled = true\n\n[static_leader]\nleader_instance_id = \"scheduler\"\nobservatory_service.public_addr = \"192.168.1.2:5049\"\n# ...\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/network-addresses/#workers","title":"Workers","text":"<p>The worker has two services, the task service and the shuffle service. The task service is used by the scheduler to send tasks to the worker, and the shuffle service is used by the worker to serve shuffles between each other. By default, they bind to <code>0.0.0.0:5052</code> and <code>0.0.0.0:5053</code>, respectively. The shuffle service is only bound to when using local shuffle storage.</p> <p>The worker services need to also have configured a public address. This address is used by the scheduler to reach the worker services. If the bind address is <code>0.0.0.0</code>, the public address is required. The public address can also be configured when the ports go through a NAT or load balancer.</p> <p>The following example shows how to change the bind and public addresses for the worker services:</p> <pre><code>[worker]\nenabled = true\ntask_service.bind_addr.port = 4005\ntask_service.public_addr = \"192.168.1.1:4005\"\nshuffle_service.bind_addr.port = 4006\nshuffle_service.public_addr = \"192.168.1.1:4006\"\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/reference/","title":"Config file reference","text":"<p>This page describes the different configuration options for Polars on-premises. The config file is a standard TOML file with different sections. Any of the configuration can be overridden using environment variables in the following format: <code>PC_CUBLET__section_name__key</code>.</p> <p>Example configuration files can be found at Example Configurations.</p> <p>See the sidebar for extensive documentation on important components and their configuration together.</p>"},{"location":"polars-on-premises/bare-metal/configuration/reference/#top-level-configuration","title":"Top-level configuration","text":"Key Type Description <code>cluster_id</code> string Logical ID for the cluster; workers and scheduler that share this ID will form a single cluster.e.g. <code>prod-eu-1</code>; must be unique among all clusters. <code>instance_id</code> string Unique ID for this node within the cluster, used for addressing and leader selection.e.g. <code>scheduler</code>, <code>worker_0</code>; must be unique per cluster. <code>license</code> path Absolute path to the Polars on-premises license file required to start the process.e.g. <code>/etc/polars/license.json</code>. <code>memory_limit</code> integer Hard memory budget for all components in this node; enforced via cgroups when delegated.e.g. <code>1073741824</code> (1 GiB), <code>10737418240</code> (10 GiB)."},{"location":"polars-on-premises/bare-metal/configuration/reference/#scheduler-section","title":"<code>[scheduler]</code> section","text":"Key Type Description <code>enabled</code> boolean Whether the scheduler component runs in this process. <code>true</code> for the leader node, <code>false</code> on pure workers. <code>allow_local_sinks</code> boolean Whether workers are allowed to write to a shared/local disk visible to the scheduler. <code>false</code> for fully remote/storage-only setups, <code>true</code> if you have a shared filesystem. <code>n_workers</code> integer Expected number of workers in this cluster; scheduler waits for the latter to be online before running queries.e.g. <code>4</code>. <code>anonymous_result_location</code> object Destination for results of queries that do not have an explicit sink. Currently supported local mounted (must be reachable on the exact same path and <code>allow_local_sinks</code> enabled) and S3-based. Both options must be network reachable by scheduler, workers, and client.e.g. <code>/mnt/storage/polars/results</code>.e.g. <code>s3://bucket/path/to/key</code> <code>anonymous_result_location.local</code> object Object used for local disk-backed anonymous results. <code>anonymous_result_location.local.path</code> path Local path where anonymous results are stored.e.g. <code>/mnt/storage/polars/results</code>. <code>anonymous_result_location.s3</code> object Object used for S3-backed anonymous results. <code>anonymous_result_location.s3.url</code> string S3 bucket url.e.g. <code>s3://bucket/path/to/key</code>. <code>anonymous_result_location.s3.aws_endpoint_url</code> string Storage option configuration, see <code>scan_parquet()</code>. <code>anonymous_result_location.s3.aws_region</code> string Storage option configuration.e.g. <code>eu-east-1</code> <code>anonymous_result_location.s3.aws_access_key_id</code> string Storage option configuration. <code>anonymous_result_location.s3.aws_secret_access_key</code> string Storage option configuration. <code>client_service</code> object Object used for configuring the bind address of the client service. This is the service used by the polars-cloud Python client. Defaults to <code>0.0.0.0:5051</code>. <code>client_service.bind_addr</code> string Bind address for the client service.e.g. <code>0.0.0.0:5051</code>. <code>client_service.bind_addr.ip</code> string IP address for the client service bind address.e.g. <code>192.168.1.1</code>. <code>client_service.bind_addr.port</code> integer Port for the client service bind address.e.g. <code>5051</code>. <code>client_service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-1</code>. <code>worker_service</code> object Object used for configuring the bind address of the worker service. This is an internal service used by the workers. Defaults to <code>0.0.0.0:5050</code>. <code>worker_service.bind_addr</code> string Bind address for the worker service.e.g. <code>0.0.0.0:5050</code>. <code>worker_service.bind_addr.ip</code> string IP address for the worker service bind address.e.g. <code>192.168.1.1</code>. <code>worker_service.bind_addr.port</code> integer Port for the worker service bind address.e.g. <code>5050</code>. <code>worker_service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>."},{"location":"polars-on-premises/bare-metal/configuration/reference/#worker-section","title":"<code>[worker]</code> section","text":"Key Type Description <code>enabled</code> boolean Whether the worker component runs in this process. <code>true</code> on worker nodes, <code>false</code> on the dedicated scheduler. <code>heartbeat_period</code> string Interval for worker heartbeats towards the scheduler, used for liveness and load reporting. Either an ISO 8601 duration format or a jiff friendly duration format (see https://docs.rs/jiff/0.2.18/jiff/fmt/friendly/)e.g. <code>5 secs</code>.e.g. <code>PT5S</code>. <code>shuffle_location</code> object Object used for shuffle data storage. <code>shuffle_location.local</code> object Object used for local disk-backed shuffle data storage. <code>shuffle_location.local.path</code> path Local path where shuffle/intermediate data is stored; fast local SSD is recommended.e.g. <code>/mnt/storage/polars/shuffle</code>. <code>shuffle_location.shared_filesystem</code> object Object used for shared filesystem-backed shuffle data storage. <code>shuffle_location.shared_filesystem.path</code> path Shared filesystem path where shuffle/intermediate data is stored. Must be accessible by all workers on the same path.e.g. <code>/mnt/storage/polars/shuffle</code>. <code>shuffle_location.s3</code> object Object used for S3-backed shuffle data storage. <code>shuffle_location.s3.url</code> path Destination for shuffle/intermediate data.e.g. <code>s3://bucket/path/to/key</code>. <code>shuffle_location.s3.aws_endpoint_url</code> string Storage option configuration, see <code>scan_parquet()</code>. <code>shuffle_location.s3.aws_region</code> string Storage option configuration.e.g. <code>eu-east-1</code> <code>shuffle_location.s3.aws_access_key_id</code> string Storage option configuration. <code>shuffle_location.s3.aws_secret_access_key</code> string Storage option configuration. <code>task_service</code> object Object used for configuring the bind address of the task service. This is an internal service in the worker for receiving tasks from the scheduler. Defaults to <code>0.0.0.0:5052</code>. <code>task_service.bind_addr</code> string Bind address for the task service.e.g. <code>0.0.0.0:5052</code>. <code>task_service.bind_addr.ip</code> string IP address for the task service bind address.e.g. <code>192.168.1.1</code>. <code>task_service.bind_addr.port</code> integer Port for the task service bind address.e.g. <code>5052</code>. <code>task_service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>. <code>task_service.public_addr</code> string Address at which this service is reachable by the scheduler. Defaults to the bind address if not set. This field is required when the bind address is <code>0.0.0.0</code>.e.g. <code>192.168.1.1</code>. <code>task_service.public_addr.ip</code> string IP address for the task service public address.e.g. <code>192.168.1.2</code>. <code>task_service.public_addr.port</code> integer Port for the task service public address.e.g. <code>5052</code>. <code>task_service.public_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>. <code>shuffle_service</code> object Object used for configuring the bind address of the task service. This is an internal service in the worker for receiving tasks from the scheduler. Defaults to <code>0.0.0.0:5052</code>. <code>shuffle_service.bind_addr</code> string Bind address for the task service.e.g. <code>0.0.0.0:5053</code>. <code>shuffle_service.bind_addr.ip</code> string IP address for the task service bind address.e.g. <code>192.168.1.1</code>. <code>shuffle_service.bind_addr.port</code> integer Port for the task service bind address.e.g. <code>5053</code>. <code>shuffle_service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>. <code>shuffle_service.public_addr</code> string Address at which this service is reachable by the scheduler. Defaults to the bind address if not set. This field is required when the bind address is <code>0.0.0.0</code>.e.g. <code>192.168.1.1</code>. <code>shuffle_service.public_addr.ip</code> string IP address for the task service public address.e.g. <code>192.168.1.2</code>. <code>shuffle_service.public_addr.port</code> integer Port for the task service public address.e.g. <code>5053</code>. <code>shuffle_service.public_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>."},{"location":"polars-on-premises/bare-metal/configuration/reference/#observatory-section","title":"<code>[observatory]</code> section","text":"Key Type Description <code>enabled</code> boolean Enable sending/receiving profiling data so clients can call <code>result.await_profile()</code>. <code>true</code> on both scheduler and workers if you want profiles on queries; <code>false</code> to disable. <code>max_metrics_bytes_total</code> integer How many bytes all the worker host metrics will consume in total. If a system-wide memory limit is specified then this is added to the share that the scheduler takes. For every worker, about 50 bytes of metrics are stored per second. <code>database_path</code> string Location to use for storing profiling data. An SQLite database file will be created here, or if a file already exists it will be opened. If this points to a directory, a file in that directory will be created. Polars on-premises will automatically add the <code>cluster_id</code> to this file name to ensure uniqueness within the directory. <code>service</code> object Object used for configuring the bind address of the observatory service. This is an internal service in the scheduler for receiving profiling data from all nodes. Defaults to <code>0.0.0.0:5049</code>. <code>service.bind_addr</code> string Bind address for the observatory service.e.g. <code>0.0.0.0:5049</code>. <code>service.bind_addr.ip</code> string IP address for the observatory service bind address.e.g. <code>192.168.1.1</code>. <code>service.bind_addr.port</code> integer Port for the observatory service bind address.e.g. <code>5049</code>. <code>service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>. <code>rest_api.enabled</code> boolean By default enabled for exposing the observatory REST API. This is a public service for accessing the profiling data and host metrics data through a web interface. <code>rest_api.service</code> object Object used for configuring the bind address of the observatory REST API service. Defaults to <code>0.0.0.0:3001</code>. <code>rest_api.service.bind_addr</code> string Bind address for the observatory REST API service.e.g. <code>0.0.0.0:3001</code>. <code>rest_api.service.bind_addr.ip</code> string IP address for the observatory REST API service bind address.e.g. <code>192.168.1.1</code>. <code>rest_api.service.bind_addr.port</code> integer Port for the observatory REST API service bind address.e.g. <code>3001</code>. <code>rest_api.service.bind_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>."},{"location":"polars-on-premises/bare-metal/configuration/reference/#monitoring-section","title":"<code>[monitoring]</code> section","text":"Key Type Description <code>enabled</code> boolean Enable sending/receiving monitoring data to the observatory service. If enabled, it will use the address specified in <code>observatory_service.public_addr</code>. <code>host_metrics</code> object Object used for configuring the host metrics exporter. <code>host_metrics.enabled</code> boolean Enable/disable exporting host metrics from this node"},{"location":"polars-on-premises/bare-metal/configuration/reference/#static_leader-section","title":"<code>[static_leader]</code> section","text":"Key Type Description <code>leader_instance_id</code> string ID of the leader node; should match the scheduler\u2019s <code>instance_id</code>.Typically <code>scheduler</code> to match your scheduler node. <code>scheduler_service.public_addr</code> string Address at which the scheduler client service is reachable from this node.e.g. <code>192.168.1.1</code>. <code>scheduler_service.public_addr.ip</code> string IP address for the scheduler client service public address.e.g. <code>192.168.1.1</code>. <code>scheduler_service.public_addr.port</code> integer Port for the scheduler client service public address.e.g. <code>5051</code>. <code>scheduler_service.public_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>. <code>observatory_service.public_addr</code> string Address at which the observatory service is reachable from this node.e.g. <code>192.168.1.1</code>. <code>observatory_service.public_addr.ip</code> string IP address for the observatory service public address.e.g. <code>192.168.1.1</code>. <code>observatory_service.public_addr.port</code> integer Port for the observatory service public address.e.g. <code>5049</code>. <code>observatory_service.public_addr.hostname</code> string Alternative to <code>ip</code>, resolved once at startup.e.g. <code>my-host-2</code>."},{"location":"polars-on-premises/bare-metal/configuration/resource-limits/","title":"Resource limits","text":"<p>Polars will always attempt to use all available CPU resources, and consumes memory resources as needed. If possible, assign physical cores to Polars to avoid contention with other processes. Polars on-premises workers consist of a main process and an executor process, with the latter doing most computation. If the executor dies, it will lose all progress of the stage it was working on. All progress of previous stages (i.e. shuffle data) is managed by the main process.</p> <p>In other words, if the system is low on memory, the first process that should be killed, is the executor process. Polars on-premises will already automatically configure <code>oom-score-adj</code> on its executor process.</p> <p>If there are other system critical processes, we recommend either delegating a cgroup to Polars on-premises, or manually setting up cgroup limits for the entire Polars on-premises service.</p>"},{"location":"polars-on-premises/bare-metal/configuration/resource-limits/#delegating-cgroup-to-polars-on-premises","title":"Delegating cgroup to Polars on-premises","text":"<p>Cgroups can contain subgroups, each with independent limits. Polars on-premises can create these cgroups, and choose proper memory limits for each of its components. To use this feature, ensure you delegate cgroups to the Polars On-Premise process and configure <code>memory_limit</code> in the configuration file.</p> <pre><code>cluster_id = \"polars-cluster\"\ninstance_id = \"node-0\"\nlicense = \"/etc/polars/license.json\"\nmemory_limit = 10737418240 # 10 GiB\n# ...\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/resource-limits/#manually-configuring-cgroup-limits","title":"Manually configuring cgroup limits","text":"<p>You can also manually configure a memory limit on a cgroup containing all the processes. For example using Systemd's <code>resource-control</code>. The disadvantage of this approach is that the individual components will contend for the same memory capacity, which may prevent Polars on-premises from gracefully handling OOM errors on the executor.</p>"},{"location":"polars-on-premises/bare-metal/configuration/shuffle-data/","title":"Shuffle data","text":"<p>When running distributed queries, data needs to be transferred in between the nodes. Polars on-premises requires a configuration for this storage location. You should decide and benchmark which location is the best for your infrastructure, as it has a large impact on query execution times.</p>"},{"location":"polars-on-premises/bare-metal/configuration/shuffle-data/#worker-local-storage","title":"Worker local storage","text":"<p>When using local storage, Polars queries write shuffle data directly to a file. This is preferably configured with a node local SSD. For other nodes to access the data, the following sequence happens:</p> <pre><code>worker_1 -[fs write]-&gt; disk\nworker_2 &lt;-[net]- worker_1 &lt;-[fs read]- disk\n</code></pre> <p>Local shuffles can be configured as shown below:</p> <pre><code>[worker]\nenabled = true\nshuffle_location.local.path = \"/mnt/storage/polars/shuffle\"\n# ...\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/shuffle-data/#worker-shared-storage","title":"Worker shared storage","text":"<p>If your infrastructure has some shared storage file system, such as NFS (or CephFs, etc.), Polars on-premises can use that for its shuffle data too. This reduces shuffle complexity, as Polars can directly write to the remote shared disk, and any worker can directly read from it. This setup can lead to improved performance when the network storage provider is fast enough. In addition, it provides automatic shuffle data persistence in case of worker node failure.</p> <pre><code>worker_1 -[net]-&gt; shared storage -[fs]-&gt; disk\nworker_2 &lt;-[net]- shared storage &lt;-[fs]- disk\n</code></pre> <p>A requirement for this to work is that all workers have the same shuffle location configured. An example configuration is shown below:</p> <pre><code>[worker]\nenabled = true\nshuffle_location.shared_filesystem.path = \"/mnt/storage/polars/shuffle\"\n# ...\n</code></pre>"},{"location":"polars-on-premises/bare-metal/configuration/shuffle-data/#s3-compatible-storage","title":"S3 compatible storage","text":"<p>S3 compatible storage is similar to the shared filesystem storage described above, but uses the S3 API. It has the same advantages and disadvantages as the shared filesystem storage. You can configure S3 compatible storage as follows:</p> <pre><code>[worker]\nenabled = true\nshuffle_location.s3.url = \"s3://bucket/path/to/key\"\nshuffle_location.s3.aws_secret_access_key = \"YOURSECRETKEY\"\nshuffle_location.s3.aws_access_key_id = \"YOURACCESSKEY\"\n</code></pre> <p>If you self-host an S3 compatible storage solution, you can override the <code>aws_endpoint_url</code> configuration option.</p> <pre><code>[worker]\nshuffle_location.s3.url = \"s3://bucket/path/to/key\"\nshuffle_location.s3.aws_endpoint_url = \"http://your-s3-compatible-storage-host:8080\"\n</code></pre> <p>The allowed keys under <code>shuffle_location.s3</code> are the same as in <code>scan_parquet()</code>(e.g. <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, <code>aws_session_token</code>, <code>aws_region</code>). We currently only support the AWS keys of the <code>storage_options</code> dictionary, but note that you can use any other cloud provider that supports the S3 API, such as MinIO or DigitalOcean Spaces.</p>"},{"location":"polars-on-premises/bare-metal/configuration/static-leader/","title":"Static leader configuration","text":"<p>All nodes in the cluster need to know whether they are the leader or if not, where to find the leader. Generally, this section can be identical across all nodes. In this section, we need to specify the instance id of the leader node, and the public addresses of the observatory and scheduler services.</p> <pre><code>[static_leader]\nleader_instance_id = \"node-1\"\nobservatory_service.public_addr = \"127.0.0.1:4003\"\nscheduler_service.public_addr = \"127.0.0.1:4002\"\n</code></pre> <p>If the current node's instance id matches the <code>leader_instance_id</code>, it will run the observatory and scheduler services (if those components are enabled). This setup allows you to use a single configuration file across all nodes in the cluster, only overriding the <code>instance_id</code> on each node.</p> <p>You can set any configuration option in the file using an environment variable in the format of <code>PC_CUBLET__&lt;SECTION&gt;__&lt;KEY&gt;</code>. For example, to override the <code>instance_id</code> for a node, you can set the environment variable <code>PC_CUBLET__instance_id=node-2</code>, and to disable the host metrics collection for a node, you can set the environment variable <code>PC_CUBLET__observatory__host_metrics__enabled=false</code>.</p>"},{"location":"polars-on-premises/kubernetes/getting-started/","title":"Getting started","text":"<p>First of all, make sure to obtain a license for Polars on-premises by signing up here. You will receive an access key for our private Docker registry as well as a JSON-formatted license for running Polars on-premises.</p> <p>Polars on-premises for Kubernetes is distributed through our Helm Chart, which can be found in our helm-charts repository.</p>"},{"location":"polars-on-premises/kubernetes/getting-started/#usage","title":"Usage","text":"<p>Helm must be installed to use the charts. Please refer to Helm documentation to get started. Once Helm is set up properly, add the repository as follows:</p> <pre><code>helm repo add polars-inc https://polars-inc.github.io/helm-charts\n</code></pre> <p>You can then run <code>helm search repo polars-inc</code> to see the available charts.</p> <p>Further explanation on the different configuration can be found in the chart <code>README.md</code>.</p>"},{"location":"releases/changelog/","title":"Changelog","text":"<p>Polars uses GitHub to manage both Python and Rust releases.</p> <p>Refer to our GitHub releases page for the changelog associated with each new release.</p>"},{"location":"releases/upgrade/","title":"About","text":"<p>Polars releases an upgrade guide alongside each breaking release. This guide is intended to help you upgrade from an older Polars version to the new version.</p> <p>Each guide contains all breaking changes that were not previously deprecated, as well as any significant new deprecations.</p> <p>A full list of all changes is available in the changelog.</p> <p>Tip</p> <p>It can be useful to upgrade to the latest non-breaking version before upgrading to a new breaking version. This way, you can run your code and address any deprecation warnings. The upgrade to the new breaking version should then go much more smoothly!</p> <p>Tip</p> <p>One of our maintainers has created a tool for automatically upgrading your Polars code to a later version. It's based on the well-known pyupgrade tool. Try out polars-upgrade and let us know what you think!</p> <p>Note</p> <p>There are no upgrade guides yet for Rust releases. These will be added once the rate of breaking changes to the Rust API slows down and a deprecation policy is added.</p>"},{"location":"releases/upgrade/0.19/","title":"Version 0.19","text":""},{"location":"releases/upgrade/0.19/#breaking-changes","title":"Breaking changes","text":""},{"location":"releases/upgrade/0.19/#aggregation-functions-no-longer-support-horizontal-computation","title":"Aggregation functions no longer support horizontal computation","text":"<p>This impacts aggregation functions like <code>sum</code>, <code>min</code>, and <code>max</code>. These functions were overloaded to support both vertical and horizontal computation. Recently, new dedicated functionality for horizontal computation was released, and horizontal computation was deprecated.</p> <p>Restore the old behavior by using the horizontal variant, e.g. <code>sum_horizontal</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})\n&gt;&gt;&gt; df.select(pl.sum('a', 'b'))  # horizontal computation\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 12  \u2502\n\u2502 14  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})\n&gt;&gt;&gt; df.select(pl.sum('a', 'b'))  # vertical computation\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 23  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.19/#update-to-all-any","title":"Update to <code>all</code> / <code>any</code>","text":"<p><code>all</code> will now ignore null values by default, rather than treat them as <code>False</code>.</p> <p>For both <code>any</code> and <code>all</code>, the <code>drop_nulls</code> parameter has been renamed to <code>ignore_nulls</code> and is now keyword-only. Also fixed an issue when setting this parameter to <code>False</code> would erroneously result in <code>None</code> output in some cases.</p> <p>To restore the old behavior, set <code>ignore_nulls</code> to <code>False</code> and check for <code>None</code> output.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series([True, None]).all()\nFalse\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series([True, None]).all()\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.19/#improved-error-types-for-many-methods","title":"Improved error types for many methods","text":"<p>Improving our error messages is an ongoing effort. We did a sweep of our Python code base and made many improvements to error messages and error types. Most notably, many <code>ValueError</code>s were changed to <code>TypeError</code>s.</p> <p>If your code relies on handling Polars exceptions, you may have to make some adjustments.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series(values=15)\n...\nValueError: Series constructor called with unsupported type; got 'int'\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series(values=15)\n...\nTypeError: Series constructor called with unsupported type 'int' for the `values` parameter\n</code></pre>"},{"location":"releases/upgrade/0.19/#updates-to-expression-input-parsing","title":"Updates to expression input parsing","text":"<p>Methods like <code>select</code> and <code>with_columns</code> accept one or more expressions. But they also accept strings, integers, lists, and other inputs that we try to interpret as expressions. We updated our internal logic to parse inputs more consistently.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame({'a': [1, 2]}).with_columns(None)\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame({'a': [1, 2]}).with_columns(None)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 literal \u2502\n\u2502 --- \u2506 ---     \u2502\n\u2502 i64 \u2506 null    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 null    \u2502\n\u2502 2   \u2506 null    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.19/#shuffle-sample-now-use-an-internal-polars-seed","title":"<code>shuffle</code> / <code>sample</code> now use an internal Polars seed","text":"<p>If you used the built-in Python <code>random.seed</code> function to control the randomness of Polars expressions, this will no longer work. Instead, use the new <code>set_random_seed</code> function.</p> <p>Example</p> <p>Before:</p> <pre><code>import random\n\nrandom.seed(1)\n</code></pre> <p>After:</p> <pre><code>import polars as pl\n\npl.set_random_seed(1)\n</code></pre>"},{"location":"releases/upgrade/0.19/#deprecations","title":"Deprecations","text":"<p>Creating a consistent and intuitive API is hard; finding the right name for each function, method, and parameter might be the hardest part. The new version comes with several naming changes, and you will most likely run into deprecation warnings when upgrading to <code>0.19</code>.</p> <p>If you want to upgrade without worrying about deprecation warnings right now, you can add the following snippet to your code:</p> <pre><code>import warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n</code></pre>"},{"location":"releases/upgrade/0.19/#groupby-renamed-to-group_by","title":"<code>groupby</code> renamed to <code>group_by</code>","text":"<p>This is not a change we make lightly, as it will impact almost all our users. But \"group by\" is really two different words, and our naming strategy dictates that these should be separated by an underscore.</p> <p>Most likely, a simple search and replace will be enough to take care of this update:</p> <ul> <li>Search: <code>.groupby(</code></li> <li>Replace: <code>.group_by(</code></li> </ul>"},{"location":"releases/upgrade/0.19/#apply-renamed-to-map_","title":"<code>apply</code> renamed to <code>map_*</code>","text":"<p><code>apply</code> is probably the most misused part of our API. Many Polars users come from pandas, where <code>apply</code> has a completely different meaning.</p> <p>We now consolidate all our functionality for user-defined functions under the name <code>map</code>. This results in the following renaming:</p> Before After <code>Series/Expr.apply</code> <code>map_elements</code> <code>Series/Expr.rolling_apply</code> <code>rolling_map</code> <code>DataFrame.apply</code> <code>map_rows</code> <code>GroupBy.apply</code> <code>map_groups</code> <code>apply</code> <code>map_groups</code> <code>map</code> <code>map_batches</code>"},{"location":"releases/upgrade/0.20/","title":"Version 0.20","text":""},{"location":"releases/upgrade/0.20/#breaking-changes","title":"Breaking changes","text":""},{"location":"releases/upgrade/0.20/#change-default-join-behavior-with-regard-to-null-values","title":"Change default <code>join</code> behavior with regard to null values","text":"<p>Previously, null values in the join key were considered a value like any other value. This meant that null values in the left frame would be joined with null values in the right frame. This is expensive and does not match default behavior in SQL.</p> <p>Default behavior has now been changed to ignore null values in the join key. The previous behavior can be retained by setting <code>join_nulls=True</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df1 = pl.DataFrame({\"a\": [1, 2, None], \"b\": [4, 4, 4]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"a\": [None, 2, 3], \"c\": [5, 5, 5]})\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\")\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b   \u2506 c   \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 4   \u2506 5   \u2502\n\u2502 2    \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\")\nshape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\", nulls_equal=True)  # Keeps previous behavior\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b   \u2506 c   \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 4   \u2506 5   \u2502\n\u2502 2    \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#preserve-left-and-right-join-keys-in-outer-joins","title":"Preserve left and right join keys in outer joins","text":"<p>Previously, the result of an outer join did not contain the join keys of the left and right frames. Rather, it contained a coalesced version of the left key and right key. This loses information and does not conform to default SQL behavior.</p> <p>The behavior has been changed to include the original join keys. Name clashes are solved by appending a suffix (<code>_right</code> by default) to the right join key name. The previous behavior can be retained by setting <code>how=\"outer_coalesce\"</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df1 = pl.DataFrame({\"L1\": [\"a\", \"b\", \"c\"], \"L2\": [1, 2, 3]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"L1\": [\"a\", \"c\", \"d\"], \"R2\": [7, 8, 9]})\n&gt;&gt;&gt; df1.join(df2, on=\"L1\", how=\"outer\")\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1  \u2506 L2   \u2506 R2   \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1    \u2506 7    \u2502\n\u2502 c   \u2506 3    \u2506 8    \u2502\n\u2502 d   \u2506 null \u2506 9    \u2502\n\u2502 b   \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df1.join(df2, on=\"L1\", how=\"outer\")\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1   \u2506 L2   \u2506 L1_right \u2506 R2   \u2502\n\u2502 ---  \u2506 ---  \u2506 ---      \u2506 ---  \u2502\n\u2502 str  \u2506 i64  \u2506 str      \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 1    \u2506 a        \u2506 7    \u2502\n\u2502 b    \u2506 2    \u2506 null     \u2506 null \u2502\n\u2502 c    \u2506 3    \u2506 c        \u2506 8    \u2502\n\u2502 null \u2506 null \u2506 d        \u2506 9    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"outer_coalesce\")  # Keeps previous behavior\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1  \u2506 L2   \u2506 R2   \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1    \u2506 7    \u2502\n\u2502 c   \u2506 3    \u2506 8    \u2502\n\u2502 d   \u2506 null \u2506 9    \u2502\n\u2502 b   \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#count-now-ignores-null-values","title":"<code>count</code> now ignores null values","text":"<p>The <code>count</code> method for <code>Expr</code> and <code>Series</code> now ignores null values. Use <code>len</code> to get the count with null values included.</p> <p>Note that <code>pl.count()</code> and <code>group_by(...).count()</code> are unchanged. These count the number of rows in the context, so nulls are not applicable in the same way.</p> <p>This brings behavior more in line with the SQL standard, where <code>COUNT(col)</code> ignores null values but <code>COUNT(*)</code> counts rows regardless of null values.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, None]})\n&gt;&gt;&gt; df.select(pl.col(\"a\").count())\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").count())\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.col(\"a\").len())  # Mirrors previous behavior\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#nan-values-are-now-considered-equal","title":"<code>NaN</code> values are now considered equal","text":"<p>Floating point <code>NaN</code> values were treated as unequal across Polars operations. This has been corrected to better match user expectation and existing standards.</p> <p>While this is considered a bug fix, it is included in this guide in order to draw attention to possible impact on user workflows that may contain <code>NaN</code> values.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1.0, float(\"nan\"), float(\"inf\")])\n&gt;&gt;&gt; s == s\nshape: (3,)\nSeries: '' [bool]\n[\n        true\n        false\n        true\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s == s\nshape: (3,)\nSeries: '' [bool]\n[\n        true\n        true\n        true\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#assertion-utils-updates-to-exact-checking-and-nan-equality","title":"Assertion utils updates to exact checking and <code>NaN</code> equality","text":"<p>The assertion utility functions <code>assert_frame_equal</code> and <code>assert_series_equal</code> would use the tolerance parameters <code>atol</code> and <code>rtol</code> to do approximate checking, unless <code>check_exact</code> was set to <code>True</code>. This could lead to some surprising behavior, as integers are generally thought of as exact values. Integer values are now always checked exactly. To do inexact checking, convert to float first.</p> <p>Additionally, the <code>nans_compare_equal</code> parameter has been removed and <code>NaN</code> values are now always considered equal, which was the previous default behavior. This parameter had previously been deprecated but has been removed before the end of the standard deprecation period to facilitate the change to <code>NaN</code> equality.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from polars.testing import assert_frame_equal\n&gt;&gt;&gt; df1 = pl.DataFrame({\"id\": [123456]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"id\": [123457]})\n&gt;&gt;&gt; assert_frame_equal(df1, df2)  # Passes\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; assert_frame_equal(df1, df2)\n...\nAssertionError: DataFrames are different (value mismatch for column 'id')\n[left]:  [123456]\n[right]: [123457]\n</code></pre>"},{"location":"releases/upgrade/0.20/#allow-all-datatype-objects-to-be-instantiated","title":"Allow all <code>DataType</code> objects to be instantiated","text":"<p>Polars data types are subclasses of the <code>DataType</code> class. We had a 'hack' in place that automatically converted data types instantiated without any arguments to the <code>class</code>, rather than actually instantiating it. The idea was to allow specifying data types as <code>Int64</code> rather than <code>Int64()</code>, which is more succinct. However, this caused some unexpected behavior when working directly with data type objects, especially as there was a discrepancy with data types like <code>Datetime</code> which will be instantiated in many cases.</p> <p>Going forward, instantiating a data type will always return an instance of that class. Both classes an instances are handled by Polars, so the previous short syntax is still available. Methods that return data types like <code>Series.dtype</code> and <code>DataFrame.schema</code> now always return instantiated data types objects.</p> <p>You may have to update some of your data type checks if you were not already using the equality operator (<code>==</code>), as well as update some type hints.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3], dtype=pl.Int8)\n&gt;&gt;&gt; s.dtype == pl.Int8\nTrue\n&gt;&gt;&gt; s.dtype is pl.Int8\nTrue\n&gt;&gt;&gt; isinstance(s.dtype, pl.Int8)\nFalse\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.dtype == pl.Int8\nTrue\n&gt;&gt;&gt; s.dtype is pl.Int8\nFalse\n&gt;&gt;&gt; isinstance(s.dtype, pl.Int8)\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.20/#update-constructors-for-decimal-and-array-data-types","title":"Update constructors for <code>Decimal</code> and <code>Array</code> data types","text":"<p>The data types <code>Decimal</code> and <code>Array</code> have had their parameters switched around. The new constructors should more closely match user expectations.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Array(2, pl.Int16)\nArray(Int16, 2)\n&gt;&gt;&gt; pl.Decimal(5, 10)\nDecimal(precision=10, scale=5)\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Array(pl.Int16, 2)\nArray(Int16, width=2)\n&gt;&gt;&gt; pl.Decimal(10, 5)\nDecimal(precision=10, scale=5)\n</code></pre>"},{"location":"releases/upgrade/0.20/#datatypeis_nested-changed-from-a-property-to-a-class-method","title":"<code>DataType.is_nested</code> changed from a property to a class method","text":"<p>This is a minor change, but a very important one to properly update. Failure to update accordingly may result in faulty logic, as Python will evaluate the method to <code>True</code>. For example, <code>if dtype.is_nested</code> will now evaluate to <code>True</code> regardless of the data type, because it returns the method, which Python considers truthy.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.List(pl.Int8).is_nested\nTrue\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.List(pl.Int8).is_nested()\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.20/#smaller-integer-data-types-for-datetime-components-dtmonth-dtweek","title":"Smaller integer data types for datetime components <code>dt.month</code>, <code>dt.week</code>","text":"<p>Most datetime components such as <code>month</code> and <code>week</code> would previously return a <code>UInt32</code> type. This has been updated to the smallest appropriate signed integer type. This should reduce memory consumption.</p> Method Dtype old Dtype new year i32 i32 iso_year i32 i32 quarter u32 i8 month u32 i8 week u32 i8 day u32 i8 weekday u32 i8 ordinal_day u32 i16 hour u32 i8 minute u32 i8 second u32 i8 millisecond u32 i32* microsecond u32 i32 nanosecond u32 i32 <p>*Technically, <code>millisecond</code> can be an <code>i16</code>. This may be updated in the future.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; s = pl.Series([date(2023, 12, 31), date(2024, 1, 1)])\n&gt;&gt;&gt; s.dt.month()\nshape: (2,)\nSeries: '' [u32]\n[\n        12\n        1\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.dt.month()\nshape: (2,)\nSeries: '' [u8]\n[\n        12\n        1\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#series-now-defaults-to-null-data-type-when-no-data-is-present","title":"Series now defaults to <code>Null</code> data type when no data is present","text":"<p>This replaces the previous behavior of initializing as a <code>Float32</code> type.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series(\"a\", [None])\nshape: (1,)\nSeries: 'a' [f32]\n[\n        null\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series(\"a\", [None])\nshape: (1,)\nSeries: 'a' [null]\n[\n        null\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#replace-reimplemented-with-slightly-different-behavior","title":"<code>replace</code> reimplemented with slightly different behavior","text":"<p>The new implementation is mostly backwards compatible. Please do note the following:</p> <ol> <li>The logic for determining the return data type has changed. You may want to specify    <code>return_dtype</code> to override the inferred data type, or take advantage of the new function    signature (separate <code>old</code> and <code>new</code> parameters) to influence the return type.</li> <li>The previous workaround for referencing other columns as default by using a struct column no    longer works. It now simply works as expected, no workaround needed.</li> </ol> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, 2, 3], \"b\": [1.5, 2.5, 5.0, 1.0]}, schema={\"a\": pl.Int8, \"b\": pl.Float64})\n&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i8  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 100 \u2502\n\u2502 100 \u2502\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.struct(\"a\", \"b\").replace({2: 100}, default=pl.col(\"b\")))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a     \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.5   \u2502\n\u2502 100.0 \u2502\n\u2502 100.0 \u2502\n\u2502 1.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 100 \u2502\n\u2502 100 \u2502\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}, default=pl.col(\"b\")))  # No struct needed\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a     \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.5   \u2502\n\u2502 100.0 \u2502\n\u2502 100.0 \u2502\n\u2502 1.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#value_counts-resulting-column-renamed-from-counts-to-count","title":"<code>value_counts</code> resulting column renamed from <code>counts</code> to <code>count</code>","text":"<p>The resulting struct field for the <code>value_counts</code> method has been renamed from <code>counts</code> to <code>count</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series(\"a\", [\"x\", \"x\", \"y\"])\n&gt;&gt;&gt; s.value_counts()\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 counts \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 u32    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 x   \u2506 2      \u2502\n\u2502 y   \u2506 1      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.value_counts()\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 count \u2502\n\u2502 --- \u2506 ---   \u2502\n\u2502 str \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 x   \u2506 2     \u2502\n\u2502 y   \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#update-read_parquet-to-use-object-store-rather-than-fsspec","title":"Update <code>read_parquet</code> to use Object Store rather than fsspec","text":"<p>If you were using <code>read_parquet</code>, installing <code>fsspec</code> as an optional dependency is no longer required. The new Object Store implementation was already in use for <code>scan_parquet</code>. It may have slightly different behavior in certain cases, such as how credentials are detected and how downloads are performed.</p> <p>The resulting <code>DataFrame</code> should be identical between versions.</p>"},{"location":"releases/upgrade/0.20/#deprecations","title":"Deprecations","text":""},{"location":"releases/upgrade/0.20/#cumulative-functions-renamed-from-cum-to-cum_","title":"Cumulative functions renamed from <code>cum*</code> to <code>cum_*</code>","text":"<p>Technically, this deprecation was introduced in version <code>0.19.14</code>, but many users will first encounter it when upgrading to <code>0.20</code>. It's a relatively impactful change, which is why we mention it here.</p> Old name New name <code>cumfold</code> <code>cum_fold</code> <code>cumreduce</code> <code>cum_reduce</code> <code>cumsum</code> <code>cum_sum</code> <code>cumprod</code> <code>cum_prod</code> <code>cummin</code> <code>cum_min</code> <code>cummax</code> <code>cum_max</code> <code>cumcount</code> <code>cum_count</code>"},{"location":"releases/upgrade/1/","title":"Version 1","text":""},{"location":"releases/upgrade/1/#breaking-changes","title":"Breaking changes","text":""},{"location":"releases/upgrade/1/#properly-apply-strict-parameter-in-series-constructor","title":"Properly apply <code>strict</code> parameter in Series constructor","text":"<p>The behavior of the Series constructor has been updated. Generally, it will be more strict, unless the user passes <code>strict=False</code>.</p> <p>Strict construction is more efficient than non-strict construction, so make sure to pass values of the same data type to the constructor for the best performance.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3.5])\nshape: (3,)\nSeries: '' [f64]\n[\n        1.0\n        2.0\n        3.5\n]\n&gt;&gt;&gt; s = pl.Series([1, 2, 3.5], strict=False)\nshape: (3,)\nSeries: '' [i64]\n[\n        1\n        2\n        null\n]\n&gt;&gt;&gt; s = pl.Series([1, 2, 3.5], strict=False, dtype=pl.Int8)\nSeries: '' [i8]\n[\n        1\n        2\n        null\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3.5])\nTraceback (most recent call last):\n...\nTypeError: unexpected value while building Series of type Int64; found value of type Float64: 3.5\n\nHint: Try setting `strict=False` to allow passing data with mixed types.\n&gt;&gt;&gt; s = pl.Series([1, 2, 3.5], strict=False)\nshape: (3,)\nSeries: '' [f64]\n[\n        1.0\n        2.0\n        3.5\n]\n&gt;&gt;&gt; s = pl.Series([1, 2, 3.5], strict=False, dtype=pl.Int8)\nSeries: '' [i8]\n[\n        1\n        2\n        3\n]\n</code></pre>"},{"location":"releases/upgrade/1/#change-data-orientation-inference-logic-for-dataframe-construction","title":"Change data orientation inference logic for DataFrame construction","text":"<p>Polars no longer inspects data types to infer the orientation of the data passed to the DataFrame constructor. Data orientation is inferred based on the data and schema dimensions.</p> <p>Additionally, a warning is raised whenever row orientation is inferred. Because of some confusing edge cases, users should pass <code>orient=\"row\"</code> to make explicit that their input is row-based.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; data = [[1, \"a\"], [2, \"b\"]]\n&gt;&gt;&gt; pl.DataFrame(data)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_0 \u2506 column_1 \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 i64      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 a        \u2502\n\u2502 2        \u2506 b        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame(data)\nTraceback (most recent call last):\n...\nTypeError: unexpected value while building Series of type Int64; found value of type String: \"a\"\n\nHint: Try setting `strict=False` to allow passing data with mixed types.\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame(data, orient=\"row\")\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 column_0 \u2506 column_1 \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 i64      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 a        \u2502\n\u2502 2        \u2506 b        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#consistently-convert-to-given-time-zone-in-series-constructor","title":"Consistently convert to given time zone in Series constructor","text":"<p>Danger</p> <p>This change may silently impact the results of your pipelines. If you work with time zones, please make sure to account for this change.</p> <p>Handling of time zone information in the Series and DataFrame constructors was inconsistent. Row-wise construction would convert to the given time zone, while column-wise construction would replace the time zone. The inconsistency has been fixed by always converting to the time zone specified in the data type.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; pl.Series([datetime(2020, 1, 1)], dtype=pl.Datetime('us', 'Europe/Amsterdam'))\nshape: (1,)\nSeries: '' [datetime[\u03bcs, Europe/Amsterdam]]\n[\n        2020-01-01 00:00:00 CET\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; pl.Series([datetime(2020, 1, 1)], dtype=pl.Datetime('us', 'Europe/Amsterdam'))\nshape: (1,)\nSeries: '' [datetime[\u03bcs, Europe/Amsterdam]]\n[\n        2020-01-01 01:00:00 CET\n]\n</code></pre>"},{"location":"releases/upgrade/1/#update-some-error-types-to-more-appropriate-variants","title":"Update some error types to more appropriate variants","text":"<p>We have updated a lot of error types to more accurately represent the problem. Most commonly, <code>ComputeError</code> types were changed to <code>InvalidOperationError</code> or <code>SchemaError</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series(\"a\", [100, 200, 300])\n&gt;&gt;&gt; s.cast(pl.UInt8)\nTraceback (most recent call last):\n...\npolars.exceptions.ComputeError: conversion from `i64` to `u8` failed in column 'a' for 1 out of 3 values: [300]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.cast(pl.UInt8)\nTraceback (most recent call last):\n...\npolars.exceptions.InvalidOperationError: conversion from `i64` to `u8` failed in column 'a' for 1 out of 3 values: [300]\n</code></pre>"},{"location":"releases/upgrade/1/#update-readscan_parquet-to-disable-hive-partitioning-by-default-for-file-inputs","title":"Update <code>read/scan_parquet</code> to disable Hive partitioning by default for file inputs","text":"<p>Parquet reading functions now also support directory inputs. Hive partitioning is enabled by default for directories, but is now disabled by default for file inputs. File inputs include single files, globs, and lists of files. Explicitly pass <code>hive_partitioning=True</code> to restore previous behavior.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.read_parquet(\"dataset/a=1/foo.parquet\")\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 x   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 1.0 \u2502\n\u2502 1   \u2506 2.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.read_parquet(\"dataset/a=1/foo.parquet\")\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2502\n\u2502 --- \u2502\n\u2502 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0 \u2502\n\u2502 2.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; pl.read_parquet(\"dataset/a=1/foo.parquet\", hive_partitioning=True)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 x   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 1.0 \u2502\n\u2502 1   \u2506 2.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#update-reshape-to-return-array-types-instead-of-list-types","title":"Update <code>reshape</code> to return Array types instead of List types","text":"<p><code>reshape</code> now returns an Array type instead of a List type.</p> <p>Users can restore the old functionality by calling <code>.arr.to_list()</code> on the output. Note that this is not more expensive than it would be to create a List type directly, because reshaping into an array is basically free.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3, 4, 5, 6])\n&gt;&gt;&gt; s.reshape((2, 3))\nshape: (2,)\nSeries: '' [list[i64]]\n[\n        [1, 2, 3]\n        [4, 5, 6]\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.reshape((2, 3))\nshape: (2,)\nSeries: '' [array[i64, 3]]\n[\n        [1, 2, 3]\n        [4, 5, 6]\n]\n</code></pre>"},{"location":"releases/upgrade/1/#read-2d-numpy-arrays-as-array-type-instead-of-list","title":"Read 2D NumPy arrays as <code>Array</code> type instead of <code>List</code>","text":"<p>The Series constructor now parses 2D NumPy arrays as an <code>Array</code> type rather than a <code>List</code> type.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; pl.Series(arr)\nshape: (2,)\nSeries: '' [list[i64]]\n[\n        [1, 2]\n        [3, 4]\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; pl.Series(arr)\nshape: (2,)\nSeries: '' [array[i64, 2]]\n[\n        [1, 2]\n        [3, 4]\n]\n</code></pre>"},{"location":"releases/upgrade/1/#split-replace-functionality-into-two-separate-methods","title":"Split <code>replace</code> functionality into two separate methods","text":"<p>The API for <code>replace</code> has proven to be confusing to many users, particularly with regards to the <code>default</code> argument and the resulting data type.</p> <p>It has been split up into two methods: <code>replace</code> and <code>replace_strict</code>. <code>replace</code> now always keeps the existing data type (breaking, see example below) and is meant for replacing some values in your existing column. Its parameters <code>default</code> and <code>return_dtype</code> have been deprecated.</p> <p>The new method <code>replace_strict</code> is meant for creating a new column, mapping some or all of the values of the original column, and optionally specifying a default value. If no default is provided, it raises an error if any non-null values are not mapped.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3])\n&gt;&gt;&gt; s.replace(1, \"a\")\nshape: (3,)\nSeries: '' [str]\n[\n        \"a\"\n        \"2\"\n        \"3\"\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.replace(1, \"a\")\nTraceback (most recent call last):\n...\npolars.exceptions.InvalidOperationError: conversion from `str` to `i64` failed in column 'literal' for 1 out of 1 values: [\"a\"]\n&gt;&gt;&gt; s.replace_strict(1, \"a\", default=s)\nshape: (3,)\nSeries: '' [str]\n[\n        \"a\"\n        \"2\"\n        \"3\"\n]\n</code></pre>"},{"location":"releases/upgrade/1/#preserve-nulls-in-ewm_mean-ewm_std-and-ewm_var","title":"Preserve nulls in <code>ewm_mean</code>, <code>ewm_std</code>, and <code>ewm_var</code>","text":"<p>Polars will no longer forward-fill null values in <code>ewm</code> methods. The user can call <code>.forward_fill()</code> on the output to achieve the same result.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 4, None, 3])\n&gt;&gt;&gt; s.ewm_mean(alpha=.9, ignore_nulls=False)\nshape: (4,)\nSeries: '' [f64]\n[\n        1.0\n        3.727273\n        3.727273\n        3.007913\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.ewm_mean(alpha=.9, ignore_nulls=False)\nshape: (4,)\nSeries: '' [f64]\n[\n        1.0\n        3.727273\n        null\n        3.007913\n]\n</code></pre>"},{"location":"releases/upgrade/1/#update-clip-to-no-longer-propagate-nulls-in-the-given-bounds","title":"Update <code>clip</code> to no longer propagate nulls in the given bounds","text":"<p>Null values in the bounds no longer set the value to null - instead, the original value is retained.</p> <p>Before</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [0, 1, 2], \"min\": [1, None, 1]})\n&gt;&gt;&gt; df.select(pl.col(\"a\").clip(\"min\"))\nshape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2502\n\u2502 ---  \u2502\n\u2502 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2502\n\u2502 null \u2502\n\u2502 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").clip(\"min\"))\nshape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2502\n\u2502 ---  \u2502\n\u2502 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2502\n\u2502 1    \u2502\n\u2502 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#change-strto_datetime-to-default-to-microsecond-precision-for-format-specifiers-f-and-f","title":"Change <code>str.to_datetime</code> to default to microsecond precision for format specifiers <code>\"%f\"</code> and <code>\"%.f\"</code>","text":"<p>In <code>.str.to_datetime</code>, when specifying <code>%.f</code> as the format, the default was to set the resulting datatype to nanosecond precision. This has been changed to microsecond precision.</p>"},{"location":"releases/upgrade/1/#example","title":"Example","text":"<p>Before</p> <pre><code>&gt;&gt;&gt; s = pl.Series([\"2022-08-31 00:00:00.123456789\"])\n&gt;&gt;&gt; s.str.to_datetime(format=\"%Y-%m-%d %H:%M:%S%.f\")\nshape: (1,)\nSeries: '' [datetime[ns]]\n[\n        2022-08-31 00:00:00.123456789\n]\n</code></pre> <p>After</p> <pre><code>&gt;&gt;&gt; s.str.to_datetime(format=\"%Y-%m-%d %H:%M:%S%.f\")\nshape: (1,)\nSeries: '' [datetime[us]]\n[\n        2022-08-31 00:00:00.123456\n]\n</code></pre>"},{"location":"releases/upgrade/1/#update-resulting-column-names-in-pivot-when-pivoting-by-multiple-values","title":"Update resulting column names in <code>pivot</code> when pivoting by multiple values","text":"<p>In <code>DataFrame.pivot</code>, when specifying multiple <code>values</code> columns, the result would redundantly include the <code>column</code> column in the column names. This has been addressed.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"name\": [\"Cady\", \"Cady\", \"Karen\", \"Karen\"],\n...         \"subject\": [\"maths\", \"physics\", \"maths\", \"physics\"],\n...         \"test_1\": [98, 99, 61, 58],\n...         \"test_2\": [100, 100, 60, 60],\n...     }\n... )\n&gt;&gt;&gt; df.pivot(index='name', columns='subject', values=['test_1', 'test_2'])\nshape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 test_1_subject_maths \u2506 test_1_subject_physics \u2506 test_2_subject_maths \u2506 test_2_subject_physics \u2502\n\u2502 ---   \u2506 ---                  \u2506 ---                    \u2506 ---                  \u2506 ---                    \u2502\n\u2502 str   \u2506 i64                  \u2506 i64                    \u2506 i64                  \u2506 i64                    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cady  \u2506 98                   \u2506 99                     \u2506 100                  \u2506 100                    \u2502\n\u2502 Karen \u2506 61                   \u2506 58                     \u2506 60                   \u2506 60                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"name\": [\"Cady\", \"Cady\", \"Karen\", \"Karen\"],\n...         \"subject\": [\"maths\", \"physics\", \"maths\", \"physics\"],\n...         \"test_1\": [98, 99, 61, 58],\n...         \"test_2\": [100, 100, 60, 60],\n...     }\n... )\n&gt;&gt;&gt; df.pivot('subject', index='name')\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 test_1_maths \u2506 test_1_physics \u2506 test_2_maths \u2506 test_2_physics \u2502\n\u2502 ---   \u2506 ---          \u2506 ---            \u2506 ---          \u2506 ---            \u2502\n\u2502 str   \u2506 i64          \u2506 i64            \u2506 i64          \u2506 i64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cady  \u2506 98           \u2506 99             \u2506 100          \u2506 100            \u2502\n\u2502 Karen \u2506 61           \u2506 58             \u2506 60           \u2506 60             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that the function signature has also changed:</p> <ul> <li><code>columns</code> has been renamed to <code>on</code>, and is now the first positional argument.</li> <li><code>index</code> and <code>values</code> are both optional. If <code>index</code> is not specified, then it will use all columns   not specified in <code>on</code> and <code>values</code>. If <code>values</code> is not specified, it will use all columns not   specified in <code>on</code> and <code>index</code>.</li> </ul>"},{"location":"releases/upgrade/1/#support-decimal-types-by-default-when-converting-from-arrow","title":"Support Decimal types by default when converting from Arrow","text":"<p>Update conversion from Arrow to always convert Decimals into Polars Decimals, rather than cast to Float64. <code>Config.activate_decimals</code> has been removed.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from decimal import Decimal as D\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; arr = pa.array([D(\"1.01\"), D(\"2.25\")])\n&gt;&gt;&gt; pl.from_arrow(arr)\nshape: (2,)\nSeries: '' [f64]\n[\n        1.01\n        2.25\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.from_arrow(arr)\nshape: (2,)\nSeries: '' [decimal[3,2]]\n[\n        1.01\n        2.25\n]\n</code></pre>"},{"location":"releases/upgrade/1/#remove-serde-functionality-from-plread_json-and-dataframewrite_json","title":"Remove serde functionality from <code>pl.read_json</code> and <code>DataFrame.write_json</code>","text":"<p><code>pl.read_json</code> no longer supports reading JSON files produced by <code>DataFrame.serialize</code>. Users should use <code>pl.DataFrame.deserialize</code> instead.</p> <p><code>DataFrame.write_json</code> now only writes row-oriented JSON. The parameters <code>row_oriented</code> and <code>pretty</code> have been removed. Users should use <code>DataFrame.serialize</code> to serialize a DataFrame.</p> <p>Example - <code>write_json</code></p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2], \"b\": [3.0, 4.0]})\n&gt;&gt;&gt; df.write_json()\n'{\"columns\":[{\"name\":\"a\",\"datatype\":\"Int64\",\"bit_settings\":\"\",\"values\":[1,2]},{\"name\":\"b\",\"datatype\":\"Float64\",\"bit_settings\":\"\",\"values\":[3.0,4.0]}]}'\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.write_json()  # Same behavior as previously `df.write_json(row_oriented=True)`\n'[{\"a\":1,\"b\":3.0},{\"a\":2,\"b\":4.0}]'\n</code></pre> <p>Example - <code>read_json</code></p> <p>Before:</p> <pre><code>&gt;&gt;&gt; import io\n&gt;&gt;&gt; df_ser = '{\"columns\":[{\"name\":\"a\",\"datatype\":\"Int64\",\"bit_settings\":\"\",\"values\":[1,2]},{\"name\":\"b\",\"datatype\":\"Float64\",\"bit_settings\":\"\",\"values\":[3.0,4.0]}]}'\n&gt;&gt;&gt; pl.read_json(io.StringIO(df_ser))\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3.0 \u2502\n\u2502 2   \u2506 4.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.read_json(io.StringIO(df_ser))  # Format no longer supported: data is treated as a single row\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 columns                         \u2502\n\u2502 ---                             \u2502\n\u2502 list[struct[4]]                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [{\"a\",\"Int64\",\"\",[1.0, 2.0]}, \u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame.deserialize(io.StringIO(df_ser))\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3.0 \u2502\n\u2502 2   \u2506 4.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#seriesequals-no-longer-checks-names-by-default","title":"<code>Series.equals</code> no longer checks names by default","text":"<p>Previously, <code>Series.equals</code> would return <code>False</code> if the Series names didn't match. The method now no longer checks the names by default. The previous behavior can be retained by setting <code>check_names=True</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s1 = pl.Series(\"foo\", [1, 2, 3])\n&gt;&gt;&gt; s2 = pl.Series(\"bar\", [1, 2, 3])\n&gt;&gt;&gt; s1.equals(s2)\nFalse\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s1.equals(s2)\nTrue\n&gt;&gt;&gt; s1.equals(s2, check_names=True)\nFalse\n</code></pre>"},{"location":"releases/upgrade/1/#remove-columns-parameter-from-nth-expression-function","title":"Remove <code>columns</code> parameter from <code>nth</code> expression function","text":"<p>The <code>columns</code> parameter was removed in favor of treating positional inputs as additional indices. Use <code>Expr.get</code> instead to get the same functionality.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4], \"c\": [5, 6]})\n&gt;&gt;&gt; df.select(pl.nth(1, \"a\"))\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.select(pl.nth(1, \"a\"))\n...\nTypeError: argument 'indices': 'str' object cannot be interpreted as an integer\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").get(1))\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#rename-struct-fields-of-rle-output","title":"Rename struct fields of <code>rle</code> output","text":"<p>The struct fields of the <code>rle</code> method have been renamed from <code>lengths/values</code> to <code>len/value</code>. The data type of the <code>len</code> field has also been updated to match the index type (was previously <code>Int32</code>, now <code>UInt32</code>).</p> <p>Before</p> <pre><code>&gt;&gt;&gt; s = pl.Series([\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"])\n&gt;&gt;&gt; s.rle().struct.unnest()\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 lengths \u2506 values \u2502\n\u2502 ---     \u2506 ---    \u2502\n\u2502 i32     \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2       \u2506 a      \u2502\n\u2502 1       \u2506 b      \u2502\n\u2502 3       \u2506 c      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After</p> <pre><code>&gt;&gt;&gt; s.rle().struct.unnest()\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 len \u2506 value \u2502\n\u2502 --- \u2506 ---   \u2502\n\u2502 u32 \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2506 a     \u2502\n\u2502 1   \u2506 b     \u2502\n\u2502 3   \u2506 c     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#update-set_sorted-to-only-accept-a-single-column","title":"Update <code>set_sorted</code> to only accept a single column","text":"<p>Calling <code>set_sorted</code> indicates that a column is sorted individually. Passing multiple columns indicates that each of those columns are also sorted individually. However, many users assumed this meant that the columns were sorted as a group, which led to incorrect results.</p> <p>To help users avoid this pitfall, we removed the possibility to specify multiple columns in <code>set_sorted</code>. To set multiple columns as sorted, simply call <code>set_sorted</code> multiple times.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4.0, 5.0, 6.0], \"c\": [9, 7, 8]})\n&gt;&gt;&gt; df.set_sorted(\"a\", \"b\")\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.set_sorted(\"a\", \"b\")\nTraceback (most recent call last):\n...\nTypeError: DataFrame.set_sorted() takes 2 positional arguments but 3 were given\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; df.set_sorted(\"a\").set_sorted(\"b\")\n</code></pre>"},{"location":"releases/upgrade/1/#default-to-raising-on-out-of-bounds-indices-in-all-getgather-operations","title":"Default to raising on out-of-bounds indices in all <code>get</code>/<code>gather</code> operations","text":"<p>The default behavior was inconsistent between <code>get</code> and <code>gather</code> operations in various places. Now all such operations will raise by default. Pass <code>null_on_oob=True</code> to restore previous behavior.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([[0, 1, 2], [0]])\n&gt;&gt;&gt; s.list.get(1)\nshape: (2,)\nSeries: '' [i64]\n[\n        1\n        null\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.list.get(1)\nTraceback (most recent call last):\n...\npolars.exceptions.ComputeError: get index is out of bounds\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; s.list.get(1, null_on_oob=True)\nshape: (2,)\nSeries: '' [i64]\n[\n        1\n        null\n]\n</code></pre>"},{"location":"releases/upgrade/1/#change-default-engine-for-read_excel-to-calamine","title":"Change default engine for <code>read_excel</code> to <code>\"calamine\"</code>","text":"<p>The <code>calamine</code> engine (available through the <code>fastexcel</code> package) has been added to Polars relatively recently. It's much faster than the other engines, and was already the default for <code>xlsb</code> and <code>xls</code> files. We now made it the default for all Excel files.</p> <p>There may be subtle differences between this engine and the previous default (<code>xlsx2csv</code>). One clear difference is that the <code>calamine</code> engine does not support the <code>engine_options</code> parameter. If you cannot get your desired behavior with the <code>calamine</code> engine, specify <code>engine=\"xlsx2csv\"</code> to restore previous behavior.</p>"},{"location":"releases/upgrade/1/#example_1","title":"Example","text":"<p>Before:</p> <pre><code>&gt;&gt;&gt; pl.read_excel(\"data.xlsx\", engine_options={\"skip_empty_lines\": True})\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.read_excel(\"data.xlsx\", engine_options={\"skip_empty_lines\": True})\nTraceback (most recent call last):\n...\nTypeError: read_excel() got an unexpected keyword argument 'skip_empty_lines'\n</code></pre> <p>Instead, explicitly specify the <code>xlsx2csv</code> engine or omit the <code>engine_options</code>:</p> <pre><code>&gt;&gt;&gt; pl.read_excel(\"data.xlsx\", engine=\"xlsx2csv\", engine_options={\"skip_empty_lines\": True})\n</code></pre>"},{"location":"releases/upgrade/1/#remove-class-variables-from-some-datatypes","title":"Remove class variables from some DataTypes","text":"<p>Some DataType classes had class variables. The <code>Datetime</code> class, for example, had <code>time_unit</code> and <code>time_zone</code> as class variables. This was unintended: these should have been instance variables. This has now been corrected.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; dtype = pl.Datetime\n&gt;&gt;&gt; dtype.time_unit is None\nTrue\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; dtype.time_unit is None\nTraceback (most recent call last):\n...\nAttributeError: type object 'Datetime' has no attribute 'time_unit'\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; getattr(dtype, \"time_unit\", None) is None\nTrue\n</code></pre>"},{"location":"releases/upgrade/1/#change-default-offset-in-group_by_dynamic-from-negative-every-to-zero","title":"Change default <code>offset</code> in <code>group_by_dynamic</code> from 'negative <code>every</code>' to 'zero'","text":"<p>This affects the start of the first window in <code>group_by_dynamic</code>. The new behavior should align more with user expectations.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; df = pl.DataFrame({\n...     \"ts\": [date(2020, 1, 1), date(2020, 1, 2), date(2020, 1, 3)],\n...     \"value\": [1, 2, 3],\n... })\n&gt;&gt;&gt; df.group_by_dynamic(\"ts\", every=\"1d\", period=\"2d\").agg(\"value\")\nshape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts         \u2506 value     \u2502\n\u2502 ---        \u2506 ---       \u2502\n\u2502 date       \u2506 list[i64] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2019-12-31 \u2506 [1]       \u2502\n\u2502 2020-01-01 \u2506 [1, 2]    \u2502\n\u2502 2020-01-02 \u2506 [2, 3]    \u2502\n\u2502 2020-01-03 \u2506 [3]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.group_by_dynamic(\"ts\", every=\"1d\", period=\"2d\").agg(\"value\")\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts         \u2506 value     \u2502\n\u2502 ---        \u2506 ---       \u2502\n\u2502 date       \u2506 list[i64] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 \u2506 [1, 2]    \u2502\n\u2502 2020-01-02 \u2506 [2, 3]    \u2502\n\u2502 2020-01-03 \u2506 [3]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#change-default-serialization-format-of-lazyframedataframeexpr","title":"Change default serialization format of <code>LazyFrame/DataFrame/Expr</code>","text":"<p>The only serialization format available for the <code>serialize/deserialize</code> methods on Polars objects was JSON. We added a more optimized binary format and made this the default. JSON serialization is still available by passing <code>format=\"json\"</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; lf = pl.LazyFrame({\"a\": [1, 2, 3]}).sum()\n&gt;&gt;&gt; serialized = lf.serialize()\n&gt;&gt;&gt; serialized\n'{\"MapFunction\":{\"input\":{\"DataFrameScan\":{\"df\":{\"columns\":[{\"name\":...'\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; pl.LazyFrame.deserialize(StringIO(serialized)).collect()\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; lf = pl.LazyFrame({\"a\": [1, 2, 3]}).sum()\n&gt;&gt;&gt; serialized = lf.serialize()\n&gt;&gt;&gt; serialized\nb'\\xa1kMapFunction\\xa2einput\\xa1mDataFrameScan\\xa4bdf...'\n&gt;&gt;&gt; from io import BytesIO  # Note: using BytesIO instead of StringIO\n&gt;&gt;&gt; pl.LazyFrame.deserialize(BytesIO(serialized)).collect()\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#constrain-access-to-globals-from-dataframesql-in-favor-of-plsql","title":"Constrain access to globals from <code>DataFrame.sql</code> in favor of <code>pl.sql</code>","text":"<p>The <code>sql</code> methods on <code>DataFrame</code> and <code>LazyFrame</code> can no longer access global variables. These methods should be used for operating on the frame itself. For global access, there is now the top-level <code>sql</code> function.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df1 = pl.DataFrame({\"id1\": [1, 2]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"id2\": [3, 4]})\n&gt;&gt;&gt; df1.sql(\"SELECT * FROM df1 CROSS JOIN df2\")\nshape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id2 \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2502\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 3   \u2502\n\u2502 2   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df1.sql(\"SELECT * FROM df1 CROSS JOIN df2\")\nTraceback (most recent call last):\n...\npolars.exceptions.SQLInterfaceError: relation 'df1' was not found\n</code></pre> <p>Use instead:</p> <pre><code>&gt;&gt;&gt; pl.sql(\"SELECT * FROM df1 CROSS JOIN df2\", eager=True)\nshape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id2 \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2502\n\u2502 1   \u2506 4   \u2502\n\u2502 2   \u2506 3   \u2502\n\u2502 2   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/1/#remove-re-export-of-type-aliases","title":"Remove re-export of type aliases","text":"<p>We have a lot of type aliases defined in the <code>polars.type_aliases</code> module. Some of these were re-exported at the top-level and in the <code>polars.datatypes</code> module. These re-exports have been removed.</p> <p>We plan on adding a public <code>polars.typing</code> module in the future with a number of curated type aliases. Until then, please define your own type aliases, or import from our <code>polars.type_aliases</code> module. Note that the <code>type_aliases</code> module is not technically public, so use at your own risk.</p> <p>Example</p> <p>Before:</p> <pre><code>def foo(dtype: pl.PolarsDataType) -&gt; None: ...\n</code></pre> <p>After:</p> <pre><code>PolarsDataType = pl.DataType | type[pl.DataType]\n\ndef foo(dtype: PolarsDataType) -&gt; None: ...\n</code></pre>"},{"location":"releases/upgrade/1/#streamline-optional-dependency-definitions-in-pyprojecttoml","title":"Streamline optional dependency definitions in <code>pyproject.toml</code>","text":"<p>We revisited to optional dependency definitions and made some minor changes. If you were using the extras <code>fastexcel</code>, <code>gevent</code>, <code>matplotlib</code>, or <code>async</code>, this is a breaking change. Please update your Polars installation to use the new extras.</p> <p>Example</p> <p>Before:</p> <pre><code>pip install 'polars[fastexcel,gevent,matplotlib]'\n</code></pre> <p>After:</p> <pre><code>pip install 'polars[calamine,async,graph]'\n</code></pre>"},{"location":"releases/upgrade/1/#deprecations","title":"Deprecations","text":""},{"location":"releases/upgrade/1/#issue-performancewarning-when-lazyframe-properties-schemadtypescolumnswidth-are-used","title":"Issue <code>PerformanceWarning</code> when LazyFrame properties <code>schema/dtypes/columns/width</code> are used","text":"<p>Recent improvements to the correctness of the schema resolving in the lazy engine have had significant performance impact on the cost of resolving the schema. It is no longer 'free' - in fact, in complex pipelines with lazy file reading, resolving the schema can be relatively expensive.</p> <p>Because of this, the schema-related properties on LazyFrame were no longer good API design. Properties represent information that is already available, and just needs to be retrieved. However, for the LazyFrame properties, accessing these may have significant performance cost.</p> <p>To solve this, we added the <code>LazyFrame.collect_schema</code> method, which retrieves the schema and returns a <code>Schema</code> object. The properties raise a <code>PerformanceWarning</code> and tell the user to use <code>collect_schema</code> instead. We chose not to deprecate the properties for now to facilitate writing code that is generic for both DataFrames and LazyFrames.</p>"},{"location":"user-guide/ecosystem/","title":"Ecosystem","text":""},{"location":"user-guide/ecosystem/#introduction","title":"Introduction","text":"<p>On this page you can find a non-exhaustive list of libraries and tools that support Polars. As the data ecosystem is evolving fast, more libraries will likely support Polars in the future. One of the main drivers is that Polars makes adheres its memory layout to the <code>Apache Arrow</code> spec.</p>"},{"location":"user-guide/ecosystem/#table-of-contents","title":"Table of contents:","text":"<ul> <li>Apache Arrow</li> <li>Data visualisation</li> <li>IO</li> <li>Machine learning</li> <li>Other</li> </ul>"},{"location":"user-guide/ecosystem/#apache-arrow","title":"Apache Arrow","text":"<p>Apache Arrow enables zero-copy reads of data within the same process, meaning that data can be directly accessed in its in-memory format without the need for copying or serialisation. This enhances performance when integrating with different tools using Apache Arrow. Polars is compatible with a wide range of libraries that also make use of Apache Arrow, like Pandas and DuckDB.</p>"},{"location":"user-guide/ecosystem/#data-visualisation","title":"Data visualisation","text":"<p>See the dedicated visualization section.</p>"},{"location":"user-guide/ecosystem/#io","title":"IO","text":""},{"location":"user-guide/ecosystem/#delta-lake","title":"Delta Lake","text":"<p>The Delta Lake project aims to unlock the power of the Deltalake for as many users and projects as possible by providing native low-level APIs aimed at developers and integrators, as well as a high-level operations API that lets you query, inspect, and operate your Delta Lake with ease. Delta Lake builds on the native Polars Parquet reader allowing you to write standard Polars queries against a DeltaTable.</p> <p>Read how to use Delta Lake with Polars at Delta Lake.</p>"},{"location":"user-guide/ecosystem/#machine-learning","title":"Machine Learning","text":""},{"location":"user-guide/ecosystem/#scikit-learn","title":"Scikit Learn","text":"<p>The Scikit Learn machine learning package accepts a Polars <code>DataFrame</code> as input/output to all transformers and as input to models. skrub helps encoding DataFrames for scikit-learn estimators (eg converting dates or strings).</p>"},{"location":"user-guide/ecosystem/#xgboost-lightgbm","title":"XGBoost &amp; LightGBM","text":"<p>XGBoost and LightGBM are gradient boosting packages for doing regression or classification on tabular data. XGBoost accepts Polars <code>DataFrame</code> and <code>LazyFrame</code> as input while LightGBM accepts Polars <code>DataFrame</code> as input.</p>"},{"location":"user-guide/ecosystem/#time-series-forecasting","title":"Time series forecasting","text":"<p>The Nixtla time series forecasting packages accept a Polars <code>DataFrame</code> as input.</p>"},{"location":"user-guide/ecosystem/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face is a platform for working with machine learning datasets and models. Polars can be used to work with datasets downloaded from Hugging Face.</p>"},{"location":"user-guide/ecosystem/#deep-learning-frameworks","title":"Deep learning frameworks","text":"<p>A <code>DataFrame</code> can be transformed into a PyTorch format using <code>to_torch</code> or into a JAX format using <code>to_jax</code>.</p>"},{"location":"user-guide/ecosystem/#other","title":"Other","text":""},{"location":"user-guide/ecosystem/#duckdb","title":"DuckDB","text":"<p>DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. Read about integration with Polars on the DuckDB website.</p>"},{"location":"user-guide/ecosystem/#great-tables","title":"Great Tables","text":"<p>With Great Tables anyone can make wonderful-looking tables in Python. Here is a blog post on how to use Great Tables with Polars.</p>"},{"location":"user-guide/ecosystem/#lancedb","title":"LanceDB","text":"<p>LanceDB is a developer-friendly, serverless vector database for AI applications. They have added a direct integration with Polars. LanceDB can ingest Polars dataframes, return results as polars dataframes, and export the entire table as a polars lazyframe. See the LanceDB documentation for more details.</p>"},{"location":"user-guide/ecosystem/#mage","title":"Mage","text":"<p>Mage is an open-source data pipeline tool for transforming and integrating data. Learn about integration between Polars and Mage at docs.mage.ai.</p>"},{"location":"user-guide/ecosystem/#marimo","title":"marimo","text":"<p>marimo is a reactive notebook for Python and SQL that models notebooks as dataflow graphs. It offers built-in support for Polars, allowing seamless integration of Polars dataframes in an interactive, reactive environment - such as displaying rich Polars tables, no-code transformations of Polars dataframes, or selecting points on a Polars-backed reactive chart.</p>"},{"location":"user-guide/ecosystem/#narwhals","title":"Narwhals","text":"<p>Narwhals is a lightweight compatibility layer between dataframe libraries. It mirrors the Polars API and allows to run Polars natively, without any conversion overhead, in libraries like Plotly and others that have adopted it for dataframe interoperability.</p> <p>See the Narwhals ecosystem for more details.</p>"},{"location":"user-guide/getting-started/","title":"Getting started","text":"<p>This chapter is here to help you get started with Polars. It covers all the fundamental features and functionalities of the library, making it easy for new users to familiarise themselves with the basics from initial installation and setup to core functionalities. If you're already an advanced user or familiar with dataframes, feel free to skip ahead to the next chapter about installation options.</p>"},{"location":"user-guide/getting-started/#installing-polars","title":"Installing Polars","text":"Python Rust <pre><code>pip install polars\n</code></pre> <pre><code>cargo add polars -F lazy\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre>"},{"location":"user-guide/getting-started/#reading-writing","title":"Reading &amp; writing","text":"<p>Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small dataframe and show how to write it to disk and read it back.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\nimport datetime as dt\n\ndf = pl.DataFrame(\n    {\n        \"name\": [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n        \"birthdate\": [\n            dt.date(1997, 1, 10),\n            dt.date(1985, 2, 15),\n            dt.date(1983, 3, 22),\n            dt.date(1981, 4, 30),\n        ],\n        \"weight\": [57.9, 72.5, 53.6, 83.1],  # (kg)\n        \"height\": [1.56, 1.77, 1.65, 1.75],  # (m)\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use chrono::prelude::*;\nuse polars::prelude::*;\n\nlet mut df: DataFrame = df!(\n    \"name\" =&gt; [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n    \"birthdate\" =&gt; [\n        NaiveDate::from_ymd_opt(1997, 1, 10).unwrap(),\n        NaiveDate::from_ymd_opt(1985, 2, 15).unwrap(),\n        NaiveDate::from_ymd_opt(1983, 3, 22).unwrap(),\n        NaiveDate::from_ymd_opt(1981, 4, 30).unwrap(),\n    ],\n    \"weight\" =&gt; [57.9, 72.5, 53.6, 83.1],  // (kg)\n    \"height\" =&gt; [1.56, 1.77, 1.65, 1.75],  // (m)\n)\n.unwrap();\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the example below we write the dataframe to a csv file called <code>output.csv</code>. After that, we read it back using <code>read_csv</code> and then print the result for inspection.</p>  Python Rust <p> <code>read_csv</code> \u00b7 <code>write_csv</code> <pre><code>df.write_csv(\"docs/assets/data/output.csv\")\ndf_csv = pl.read_csv(\"docs/assets/data/output.csv\", try_parse_dates=True)\nprint(df_csv)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7 <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>use std::fs::File;\n\nlet mut file = File::create(\"docs/assets/data/output.csv\").expect(\"could not create file\");\nCsvWriter::new(&amp;mut file)\n    .include_header(true)\n    .with_separator(b',')\n    .finish(&amp;mut df)?;\nlet df_csv = CsvReadOptions::default()\n    .with_has_header(true)\n    .with_parse_options(CsvParseOptions::default().with_try_parse_dates(true))\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/output.csv\".into()))?\n    .finish()?;\nprintln!(\"{df_csv}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>For more examples on the CSV file format and other data formats, see the IO section of the user guide.</p>"},{"location":"user-guide/getting-started/#expressions-and-contexts","title":"Expressions and contexts","text":"<p>Expressions are one of the main strengths of Polars because they provide a modular and flexible way of expressing data transformations.</p> <p>Here is an example of a Polars expression:</p> <pre><code>pl.col(\"weight\") / (pl.col(\"height\") ** 2)\n</code></pre> <p>As you might be able to guess, this expression takes the column named \u201cweight\u201d and divides its values by the square of the values in the column \u201cheight\u201d, computing a person's BMI. Note that the code above expresses an abstract computation: it's only inside a Polars context that the expression materalizes into a series with the results.</p> <p>Below, we will show examples of Polars expressions inside different contexts:</p> <ul> <li><code>select</code></li> <li><code>with_columns</code></li> <li><code>filter</code></li> <li><code>group_by</code></li> </ul> <p>For a more detailed exploration of expressions and contexts see the respective user guide section.</p>"},{"location":"user-guide/getting-started/#select","title":"<code>select</code>","text":"<p>The context <code>select</code> allows you to select and manipulate columns from a dataframe. In the simplest case, each expression you provide will map to a column in the result dataframe:</p>  Python Rust <p> <code>select</code> \u00b7 <code>alias</code> \u00b7 <code>dt namespace</code> <pre><code>result = df.select(\n    pl.col(\"name\"),\n    pl.col(\"birthdate\").dt.year().alias(\"birth_year\"),\n    (pl.col(\"weight\") / (pl.col(\"height\") ** 2)).alias(\"bmi\"),\n)\nprint(result)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>alias</code> \u00b7 <code>dt namespace</code> \u00b7  Available on feature temporal <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"name\"),\n        col(\"birthdate\").dt().year().alias(\"birth_year\"),\n        (col(\"weight\") / col(\"height\").pow(2)).alias(\"bmi\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birth_year \u2506 bmi       \u2502\n\u2502 ---            \u2506 ---        \u2506 ---       \u2502\n\u2502 str            \u2506 i32        \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997       \u2506 23.791913 \u2502\n\u2502 Ben Brown      \u2506 1985       \u2506 23.141498 \u2502\n\u2502 Chloe Cooper   \u2506 1983       \u2506 19.687787 \u2502\n\u2502 Daniel Donovan \u2506 1981       \u2506 27.134694 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Polars also supports a feature called \u201cexpression expansion\u201d, in which one expression acts as shorthand for multiple expressions. In the example below, we use expression expansion to manipulate the columns \u201cweight\u201d\u00a0and \u201cheight\u201d with a single expression. When using expression expansion you can use <code>.name.suffix</code> to add a suffix to the names of the original columns:</p>  Python Rust <p> <code>select</code> \u00b7 <code>alias</code> \u00b7 <code>name namespace</code> <pre><code>result = df.select(\n    pl.col(\"name\"),\n    (pl.col(\"weight\", \"height\") * 0.95).round(2).name.suffix(\"-5%\"),\n)\nprint(result)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>alias</code> \u00b7 <code>name namespace</code> \u00b7  Available on feature lazy <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"name\"),\n        (cols([\"weight\", \"height\"]).as_expr() * lit(0.95))\n            .round(2, RoundMode::default())\n            .name()\n            .suffix(\"-5%\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 weight-5% \u2506 height-5% \u2502\n\u2502 ---            \u2506 ---       \u2506 ---       \u2502\n\u2502 str            \u2506 f64       \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 55.0      \u2506 1.48      \u2502\n\u2502 Ben Brown      \u2506 68.88     \u2506 1.68      \u2502\n\u2502 Chloe Cooper   \u2506 50.92     \u2506 1.57      \u2502\n\u2502 Daniel Donovan \u2506 78.94     \u2506 1.66      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can check other sections of the user guide to learn more about basic operations or column selections in expression expansion.</p>"},{"location":"user-guide/getting-started/#with_columns","title":"<code>with_columns</code>","text":"<p>The context <code>with_columns</code> is very similar to the context <code>select</code> but <code>with_columns</code> adds columns to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four columns of the original dataframe plus the two new columns introduced by the expressions inside <code>with_columns</code>:</p>  Python Rust <p> <code>with_columns</code> <pre><code>result = df.with_columns(\n    birth_year=pl.col(\"birthdate\").dt.year(),\n    bmi=pl.col(\"weight\") / (pl.col(\"height\") ** 2),\n)\nprint(result)\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"birthdate\").dt().year().alias(\"birth_year\"),\n        (col(\"weight\") / col(\"height\").pow(2)).alias(\"bmi\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2506 birth_year \u2506 bmi       \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2506 ---        \u2506 ---       \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2506 i32        \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2506 1997       \u2506 23.791913 \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2506 1985       \u2506 23.141498 \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2506 1983       \u2506 19.687787 \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2506 1981       \u2506 27.134694 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the example above we also decided to use named expressions instead of the method <code>alias</code> to specify the names of the new columns. Other contexts like <code>select</code> and <code>group_by</code> also accept named expressions.</p>"},{"location":"user-guide/getting-started/#filter","title":"<code>filter</code>","text":"<p>The context <code>filter</code> allows us to create a second dataframe with a subset of the rows of the original one:</p>  Python Rust <p> <code>filter</code> \u00b7 <code>dt namespace</code> <pre><code>result = df.filter(pl.col(\"birthdate\").dt.year() &lt; 1990)\nprint(result)\n</code></pre></p> <p> <code>filter</code> \u00b7 <code>dt namespace</code> \u00b7  Available on feature temporal <pre><code>let result = df\n    .clone()\n    .lazy()\n    .filter(col(\"birthdate\").dt().year().lt(lit(1990)))\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can also provide multiple predicate expressions as separate parameters, which is more convenient than putting them all together with <code>&amp;</code>:</p>  Python Rust <p> <code>filter</code> \u00b7 <code>is_between</code> <pre><code>result = df.filter(\n    pl.col(\"birthdate\").is_between(dt.date(1982, 12, 31), dt.date(1996, 1, 1)),\n    pl.col(\"height\") &gt; 1.7,\n)\nprint(result)\n</code></pre></p> <p> <code>filter</code> \u00b7 <code>is_between</code> \u00b7  Available on feature is_between <pre><code>let result = df\n    .clone()\n    .lazy()\n    .filter(\n        col(\"birthdate\")\n            .is_between(\n                lit(NaiveDate::from_ymd_opt(1982, 12, 31).unwrap()),\n                lit(NaiveDate::from_ymd_opt(1996, 1, 1).unwrap()),\n                ClosedInterval::Both,\n            )\n            .and(col(\"height\").gt(lit(1.7))),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name      \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---       \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str       \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Ben Brown \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#group_by","title":"<code>group_by</code>","text":"<p>The context <code>group_by</code> can be used to group together the rows of the dataframe that share the same value across one or more expressions. The example below counts how many people were born in each decade:</p>  Python Rust <p> <code>group_by</code> \u00b7 <code>alias</code> \u00b7 <code>dt namespace</code> <pre><code>result = df.group_by(\n    (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n    maintain_order=True,\n).len()\nprint(result)\n</code></pre></p> <p> <code>group_by</code> \u00b7 <code>alias</code> \u00b7 <code>dt namespace</code> \u00b7  Available on feature temporal <pre><code>// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.\nlet result = df\n    .clone()\n    .lazy()\n    .group_by([(col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\")])\n    .agg([len()])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 len \u2502\n\u2502 ---    \u2506 --- \u2502\n\u2502 i32    \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1990   \u2506 1   \u2502\n\u2502 1980   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The keyword argument <code>maintain_order</code> forces Polars to present the resulting groups in the same order as they appear in the original dataframe. This slows down the grouping operation but is used here to ensure reproducibility of the examples.</p> <p>After using the context <code>group_by</code> we can use <code>agg</code> to compute aggregations over the resulting groups:</p>  Python Rust <p> <code>group_by</code> \u00b7 <code>agg</code> <pre><code>result = df.group_by(\n    (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n    maintain_order=True,\n).agg(\n    pl.len().alias(\"sample_size\"),\n    pl.col(\"weight\").mean().round(2).alias(\"avg_weight\"),\n    pl.col(\"height\").max().alias(\"tallest\"),\n)\nprint(result)\n</code></pre></p> <p> <code>group_by</code> \u00b7 <code>agg</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .group_by([(col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\")])\n    .agg([\n        len().alias(\"sample_size\"),\n        col(\"weight\")\n            .mean()\n            .round(2, RoundMode::default())\n            .alias(\"avg_weight\"),\n        col(\"height\").max().alias(\"tallest\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 sample_size \u2506 avg_weight \u2506 tallest \u2502\n\u2502 ---    \u2506 ---         \u2506 ---        \u2506 ---     \u2502\n\u2502 i32    \u2506 u32         \u2506 f64        \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1990   \u2506 1           \u2506 57.9       \u2506 1.56    \u2502\n\u2502 1980   \u2506 3           \u2506 69.73      \u2506 1.77    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#more-complex-queries","title":"More complex queries","text":"<p>Contexts and the expressions within can be chained to create more complex queries according to your needs. In the example below we combine some of the contexts we have seen so far to create a more complex query:</p>  Python Rust <p> <code>group_by</code> \u00b7 <code>agg</code> \u00b7 <code>select</code> \u00b7 <code>with_columns</code> \u00b7 <code>str namespace</code> \u00b7 <code>list namespace</code> <pre><code>result = (\n    df.with_columns(\n        (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n        pl.col(\"name\").str.split(by=\" \").list.first(),\n    )\n    .select(\n        pl.all().exclude(\"birthdate\"),\n    )\n    .group_by(\n        pl.col(\"decade\"),\n        maintain_order=True,\n    )\n    .agg(\n        pl.col(\"name\"),\n        pl.col(\"weight\", \"height\").mean().round(2).name.prefix(\"avg_\"),\n    )\n)\nprint(result)\n</code></pre></p> <p> <code>group_by</code> \u00b7 <code>agg</code> \u00b7 <code>select</code> \u00b7 <code>with_columns</code> \u00b7 <code>str namespace</code> \u00b7 <code>list namespace</code> \u00b7  Available on feature strings <pre><code>let result = df\n    .clone()\n    .lazy()\n    .with_columns([\n        (col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\"),\n        col(\"name\").str().split(lit(\" \")).list().first(),\n    ])\n    .select([all().exclude_cols([\"birthdate\"]).as_expr()])\n    .group_by([col(\"decade\")])\n    .agg([\n        col(\"name\"),\n        cols([\"weight\", \"height\"])\n            .as_expr()\n            .mean()\n            .round(2, RoundMode::default())\n            .name()\n            .prefix(\"avg_\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 name                       \u2506 avg_weight \u2506 avg_height \u2502\n\u2502 ---    \u2506 ---                        \u2506 ---        \u2506 ---        \u2502\n\u2502 i32    \u2506 list[str]                  \u2506 f64        \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1990   \u2506 [\"Alice\"]                  \u2506 57.9       \u2506 1.56       \u2502\n\u2502 1980   \u2506 [\"Ben\", \"Chloe\", \"Daniel\"] \u2506 69.73      \u2506 1.72       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#combining-dataframes","title":"Combining dataframes","text":"<p>Polars provides a number of tools to combine two dataframes. In this section, we show an example of a join and an example of a concatenation.</p>"},{"location":"user-guide/getting-started/#joining-dataframes","title":"Joining dataframes","text":"<p>Polars provides many different join algorithms. The example below shows how to use a left outer join to combine two dataframes when a column can be used as a unique identifier to establish a correspondence between rows across the dataframes:</p>  Python Rust <p> <code>join</code> <pre><code>df2 = pl.DataFrame(\n    {\n        \"name\": [\"Ben Brown\", \"Daniel Donovan\", \"Alice Archer\", \"Chloe Cooper\"],\n        \"parent\": [True, False, False, False],\n        \"siblings\": [1, 2, 3, 4],\n    }\n)\n\nprint(df.join(df2, on=\"name\", how=\"left\"))\n</code></pre></p> <p> <code>join</code> <pre><code>let df2: DataFrame = df!(\n    \"name\" =&gt; [\"Ben Brown\", \"Daniel Donovan\", \"Alice Archer\", \"Chloe Cooper\"],\n    \"parent\" =&gt; [true, false, false, false],\n    \"siblings\" =&gt; [1, 2, 3, 4],\n)\n.unwrap();\n\nlet result = df\n    .clone()\n    .lazy()\n    .join(\n        df2.lazy(),\n        [col(\"name\")],\n        [col(\"name\")],\n        JoinArgs::new(JoinType::Left),\n    )\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2506 parent \u2506 siblings \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2506 ---    \u2506 ---      \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2506 bool   \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2506 false  \u2506 3        \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2506 true   \u2506 1        \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2506 false  \u2506 4        \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2506 false  \u2506 2        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Polars provides many different join algorithms that you can learn about in the joins section of the user guide.</p>"},{"location":"user-guide/getting-started/#concatenating-dataframes","title":"Concatenating dataframes","text":"<p>Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming we have a second dataframe with data from other people, we could use vertical concatenation to create a taller dataframe:</p>  Python Rust <p> <code>concat</code> <pre><code>df3 = pl.DataFrame(\n    {\n        \"name\": [\"Ethan Edwards\", \"Fiona Foster\", \"Grace Gibson\", \"Henry Harris\"],\n        \"birthdate\": [\n            dt.date(1977, 5, 10),\n            dt.date(1975, 6, 23),\n            dt.date(1973, 7, 22),\n            dt.date(1971, 8, 3),\n        ],\n        \"weight\": [67.9, 72.5, 57.6, 93.1],  # (kg)\n        \"height\": [1.76, 1.6, 1.66, 1.8],  # (m)\n    }\n)\n\nprint(pl.concat([df, df3], how=\"vertical\"))\n</code></pre></p> <p> <code>concat</code> <pre><code>let df3: DataFrame = df!(\n    \"name\" =&gt; [\"Ethan Edwards\", \"Fiona Foster\", \"Grace Gibson\", \"Henry Harris\"],\n    \"birthdate\" =&gt; [\n        NaiveDate::from_ymd_opt(1977, 5, 10).unwrap(),\n        NaiveDate::from_ymd_opt(1975, 6, 23).unwrap(),\n        NaiveDate::from_ymd_opt(1973, 7, 22).unwrap(),\n        NaiveDate::from_ymd_opt(1971, 8, 3).unwrap(),\n    ],\n    \"weight\" =&gt; [67.9, 72.5, 57.6, 93.1],  // (kg)\n    \"height\" =&gt; [1.76, 1.6, 1.66, 1.8],  // (m)\n)\n.unwrap();\n\nlet result = concat([df.clone().lazy(), df3.lazy()], UnionArgs::default())?.collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2502 Ethan Edwards  \u2506 1977-05-10 \u2506 67.9   \u2506 1.76   \u2502\n\u2502 Fiona Foster   \u2506 1975-06-23 \u2506 72.5   \u2506 1.6    \u2502\n\u2502 Grace Gibson   \u2506 1973-07-22 \u2506 57.6   \u2506 1.66   \u2502\n\u2502 Henry Harris   \u2506 1971-08-03 \u2506 93.1   \u2506 1.8    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can learn more about these in the concatenations section of the user guide.</p>"},{"location":"user-guide/gpu-support/","title":"GPU Support [Open Beta]","text":"<p>Polars provides an in-memory, GPU-accelerated execution engine for Python users of the Lazy API on NVIDIA GPUs using RAPIDS cuDF. This functionality is available in Open Beta and is undergoing rapid development.</p>"},{"location":"user-guide/gpu-support/#system-requirements","title":"System Requirements","text":"<ul> <li>NVIDIA Volta\u2122 or higher GPU with compute capability 7.0+</li> <li>CUDA 12 (CUDA 11 support ends with RAPIDS v25.06; see   RSN 48; if you're using CUDA 11, see the installation   note below)</li> <li>Linux or Windows Subsystem for Linux 2 (WSL2)</li> </ul> <p>See the RAPIDS installation guide for full details.</p>"},{"location":"user-guide/gpu-support/#installation","title":"Installation","text":"<p>You can install the GPU backend for Polars with a feature flag as part of a normal installation.</p>  Python <pre><code>pip install polars[gpu]\n</code></pre> <p>Note</p> <p>RAPIDS cuDF will drop CUDA 11 support starting with version 25.08. If you are using CUDA 11, you must pin to <code>cudf-polars-cu11==25.06</code>. See the official deprecation notice (RSN 48) for details.</p>  Python <pre><code>pip install polars cudf-polars-cu11\n</code></pre>"},{"location":"user-guide/gpu-support/#usage","title":"Usage","text":"<p>Having built a query using the lazy API as normal, GPU-enabled execution is requested by running <code>.collect(engine=\"gpu\")</code> instead of <code>.collect()</code>.</p>  Python <pre><code>import polars as pl\n\ndf = pl.LazyFrame({\"a\": [1.242, 1.535]})\n\nq = df.select(pl.col(\"a\").round(1))\n\nresult = q.collect(engine=\"gpu\")\nprint(result)\n</code></pre> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.2 \u2502\n\u2502 1.5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>For more detailed control over the execution, for example to specify which GPU to use on a multi-GPU node, we can provide a <code>GPUEngine</code> object. By default, the GPU engine will use a configuration applicable to most use cases.</p>  Python <pre><code>q = df.select((pl.col(\"a\") ** 4))\n\nresult = q.collect(engine=pl.GPUEngine(device=1))\nprint(result)\n</code></pre> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a        \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.379504 \u2502\n\u2502 5.551796 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/gpu-support/#how-it-works","title":"How It Works","text":"<p>When you use the GPU-accelerated engine, Polars creates and optimizes a query plan and dispatches to a RAPIDS cuDF-based physical execution engine to compute the results on NVIDIA GPUs. The final result is returned as a normal CPU-backed Polars dataframe.</p>"},{"location":"user-guide/gpu-support/#whats-supported-on-the-gpu","title":"What's Supported on the GPU?","text":"<p>GPU support is currently in Open Beta and the engine is undergoing rapid development. The engine currently supports many, but not all, of the core expressions and data types.</p> <p>Since expressions are composable, it's not feasible to list a full matrix of expressions supported on the GPU. Instead, we provide a list of the high-level categories of expressions and interfaces that are currently supported and not supported.</p>"},{"location":"user-guide/gpu-support/#supported","title":"Supported","text":"<ul> <li>LazyFrame API</li> <li>SQL API</li> <li>I/O from CSV, Parquet, ndjson, and in-memory CPU DataFrames.</li> <li>Operations on numeric, logical, string, and datetime types</li> <li>String processing</li> <li>Aggregations including grouped and rolling variants</li> <li>Joins</li> <li>Filters</li> <li>Missing data</li> <li>Concatenation</li> </ul>"},{"location":"user-guide/gpu-support/#not-supported","title":"Not Supported","text":"<ul> <li>Eager DataFrame API</li> <li>Streaming API</li> <li>Date, Categorical, Enum, Time, Array, Binary and Object data types</li> <li>Specific expression for Datetime with Timezone and List types</li> <li>Time series resampling</li> <li>Folds</li> <li>User-defined functions</li> <li>Excel and Database file formats</li> </ul>"},{"location":"user-guide/gpu-support/#did-my-query-use-the-gpu","title":"Did my query use the GPU?","text":"<p>The release of the GPU engine in Open Beta implies that we expect things to work well, but there are still some rough edges we're working on. In particular the full breadth of the Polars expression API is not yet supported. With fallback to the CPU, your query should complete, but you might not observe any change in the time it takes to execute. There are two ways to get more information on whether the query ran on the GPU.</p> <p>When running in verbose mode, any queries that cannot execute on the GPU will issue a <code>PerformanceWarning</code>:</p>  Python <pre><code>df = pl.LazyFrame(\n    {\n        \"key\": [1, 1, 1, 2, 3, 3, 2, 2],\n        \"value\": [1, 2, 3, 4, 5, 6, 7, 8],\n    }\n)\n\nq = df.select(pl.col(\"value\").sum().over(\"key\"))\n\n\nwith pl.Config() as cfg:\n    cfg.set_verbose(True)\n    result = q.collect(engine=\"gpu\")\n\nprint(result)\n</code></pre> <pre><code>PerformanceWarning: Query execution with GPU not supported, reason: \n&lt;class 'NotImplementedError'&gt;: Grouped rolling window not implemented\n# some details elided\n\nshape: (8, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6     \u2502\n\u2502 6     \u2502\n\u2502 6     \u2502\n\u2502 19    \u2502\n\u2502 11    \u2502\n\u2502 11    \u2502\n\u2502 19    \u2502\n\u2502 19    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To disable fallback, and have the GPU engine raise an exception if a query is unsupported, we can pass an appropriately configured <code>GPUEngine</code> object:</p>  Python <pre><code>q.collect(engine=pl.GPUEngine(raise_on_fail=True))\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/coder/third-party/polars/py-polars/polars/lazyframe/frame.py\", line 2035, in collect\n    return wrap_df(ldf.collect(callback))\npolars.exceptions.ComputeError: 'cuda' conversion failed: NotImplementedError: Grouped rolling window not implemented\n</code></pre> <p>Currently, only the proximal cause of failure to execute on the GPU is reported, we plan to extend this functionality to report all unsupported operations for a query.</p>"},{"location":"user-guide/gpu-support/#testing","title":"Testing","text":"<p>The Polars and NVIDIA RAPIDS teams run comprehensive unit and integration tests to ensure that the GPU-accelerated Polars backend works smoothly.</p> <p>The full Polars test suite is run on every commit made to the GPU engine, ensuring consistency of results.</p> <p>The GPU engine currently passes 99.2% of the Polars unit tests with CPU fallback enabled. Without CPU fallback, the GPU engine passes 88.8% of the Polars unit tests. With fallback, there are approximately 100 failing tests: around 40 of these fail due to mismatching debug output; there are some cases where the GPU engine produces the a correct result but uses a different data type; the remainder are cases where we do not correctly determine that a query is unsupported and therefore fail at runtime, instead of falling back.</p>"},{"location":"user-guide/gpu-support/#when-should-i-use-a-gpu","title":"When Should I Use a GPU?","text":"<p>Based on our benchmarking, you're most likely to observe speedups using the GPU engine when your workflow's profile is dominated by grouped aggregations and joins. In contrast I/O bound queries typically show similar performance on GPU and CPU. GPUs typically have less RAM than CPU systems, therefore very large datasets will fail due to out of memory errors. Based on our testing, raw datasets of 50-100 GiB fit (depending on the workflow) well with a GPU with 80GiB of memory.</p>"},{"location":"user-guide/gpu-support/#cpu-gpu-interoperability","title":"CPU-GPU Interoperability","text":"<p>Both the CPU and GPU engine use the Apache Arrow columnar memory specification, making it possible to quickly move data between the CPU and GPU. Additionally, files written by one engine can be read by the other engine.</p> <p>When using GPU mode, your workflow won't fail if something isn't supported. When you run <code>collect(engine=\"gpu\")</code>, the optimized query plan is inspected to see whether it can be executed on the GPU. If it can't, it will transparently fall back to the standard Polars engine and run on the CPU.</p> <p>GPU execution is only available in the Lazy API, so materialized DataFrames will reside in CPU memory when the query execution finishes.</p>"},{"location":"user-guide/gpu-support/#providing-feedback","title":"Providing feedback","text":"<p>Please report issues, and missing features, on the Polars issue tracker.</p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Polars is a library and installation is as simple as invoking the package manager of the corresponding programming language.</p>  Python Rust <pre><code>pip install polars\n\n# Or for legacy CPUs without AVX2 support\npip install polars[rtcompat]\n</code></pre> <pre><code>cargo add polars -F lazy\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre>"},{"location":"user-guide/installation/#big-index","title":"Big Index","text":"<p>By default, Polars dataframes are limited to \\(2^{32}\\) rows (~4.3 billion). Increase this limit to \\(2^{64}\\) (~18 quintillion) by enabling the big index extension:</p>  Python Rust <pre><code>pip install polars[rt64]\n</code></pre> <pre><code>cargo add polars -F bigidx\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"bigidx\", ...] }\n</code></pre>"},{"location":"user-guide/installation/#legacy-cpu","title":"Legacy CPU","text":"<p>To install Polars for Python on an old CPU without AVX support, run:</p>  Python <pre><code>pip install polars[rtcompat]\n</code></pre>"},{"location":"user-guide/installation/#importing","title":"Importing","text":"<p>To use the library, simply import it into your project:</p>  Python Rust <pre><code>import polars as pl\n</code></pre> <pre><code>use polars::prelude::*;\n</code></pre>"},{"location":"user-guide/installation/#feature-flags","title":"Feature flags","text":"<p>By using the above command you install the core of Polars onto your system. However, depending on your use case, you might want to install the optional dependencies as well. These are made optional to minimize the footprint. The flags are different depending on the programming language. Throughout the user guide we will mention when a functionality used requires an additional dependency.</p>"},{"location":"user-guide/installation/#python","title":"Python","text":"<pre><code># For example\npip install 'polars[numpy,fsspec]'\n</code></pre>"},{"location":"user-guide/installation/#all","title":"All","text":"Tag Description all Install all optional dependencies."},{"location":"user-guide/installation/#gpu","title":"GPU","text":"Tag Description gpu Run queries on NVIDIA GPUs. <p>Note</p> <p>See GPU support for more detailed instructions and prerequisites.</p>"},{"location":"user-guide/installation/#interoperability","title":"Interoperability","text":"Tag Description pandas Convert data to and from pandas dataframes/series. numpy Convert data to and from NumPy arrays. pyarrow Convert data to and from PyArrow tables/arrays. pydantic Convert data from Pydantic models to Polars."},{"location":"user-guide/installation/#excel","title":"Excel","text":"Tag Description calamine Read from Excel files with the calamine engine. openpyxl Read from Excel files with the openpyxl engine. xlsx2csv Read from Excel files with the xlsx2csv engine. xlsxwriter Write to Excel files with the XlsxWriter engine. excel Install all supported Excel engines."},{"location":"user-guide/installation/#database","title":"Database","text":"Tag Description adbc Read from and write to databases with the Arrow Database Connectivity (ADBC) engine. connectorx Read from databases with the ConnectorX engine. sqlalchemy Write to databases with the SQLAlchemy engine. database Install all supported database engines."},{"location":"user-guide/installation/#cloud","title":"Cloud","text":"Tag Description fsspec Read from and write to remote file systems."},{"location":"user-guide/installation/#other-io","title":"Other I/O","text":"Tag Description deltalake Read from and write to Delta tables. iceberg Read from Apache Iceberg tables."},{"location":"user-guide/installation/#other","title":"Other","text":"Tag Description async Collect LazyFrames asynchronously. cloudpickle Serialize user-defined functions. graph Visualize LazyFrames as a graph. plot Plot dataframes through the <code>plot</code> namespace. style Style dataframes through the <code>style</code> namespace. timezone Timezone support<sup>1</sup>."},{"location":"user-guide/installation/#rust","title":"Rust","text":"<pre><code># Cargo.toml\n[dependencies]\npolars = { version = \"0.26.1\", features = [\"lazy\", \"temporal\", \"describe\", \"json\", \"parquet\", \"dtype-datetime\"] }\n</code></pre> <p>The opt-in features are:</p> <ul> <li>Additional data types:<ul> <li><code>dtype-date</code></li> <li><code>dtype-datetime</code></li> <li><code>dtype-time</code></li> <li><code>dtype-duration</code></li> <li><code>dtype-i8</code></li> <li><code>dtype-i16</code></li> <li><code>dtype-i128</code></li> <li><code>dtype-u8</code></li> <li><code>dtype-u16</code></li> <li><code>dtype-u128</code></li> <li><code>dtype-categorical</code></li> <li><code>dtype-struct</code></li> </ul> </li> <li><code>lazy</code> - Lazy API:<ul> <li><code>regex</code> - Use regexes in column selection.</li> <li><code>dot_diagram</code> - Create dot diagrams from lazy logical plans.</li> </ul> </li> <li><code>sql</code> - Pass SQL queries to Polars.</li> <li><code>streaming</code> - Be able to process datasets that are larger than RAM.</li> <li><code>random</code> - Generate arrays with randomly sampled values</li> <li><code>ndarray</code>- Convert from <code>DataFrame</code> to <code>ndarray</code></li> <li><code>temporal</code> - Conversions between Chrono and Polars for temporal data types</li> <li><code>timezones</code> - Activate timezone support.</li> <li><code>strings</code> - Extra string utilities for <code>StringChunked</code>:<ul> <li><code>string_pad</code> - for <code>pad_start</code>, <code>pad_end</code>, <code>zfill</code>.</li> <li><code>string_to_integer</code> - for <code>parse_int</code>.</li> </ul> </li> <li><code>object</code> - Support for generic ChunkedArrays called <code>ObjectChunked&lt;T&gt;</code> (generic over <code>T</code>).   These are downcastable from Series through the Any trait.</li> <li>Performance related:<ul> <li><code>nightly</code> - Several nightly only features such as SIMD and specialization.</li> <li><code>performant</code> - more fast paths, slower compile times.</li> <li><code>bigidx</code> - Activate this feature if you expect &gt;&gt; \\(2^{32}\\) rows. This allows polars to scale up way beyond that by using <code>u64</code> as an index. Polars will be a bit slower with this feature activated as many data structures are less cache efficient.</li> <li><code>cse</code> - Activate common subplan elimination optimization.</li> </ul> </li> <li>IO related:<ul> <li><code>serde</code> - Support for serde serialization and deserialization. Can be used for JSON and more serde supported serialization formats.</li> <li><code>serde-lazy</code> - Support for serde serialization and deserialization. Can be used for JSON and more serde supported serialization formats.</li> <li><code>parquet</code> - Read Apache Parquet format.</li> <li><code>json</code> - JSON serialization.</li> <li><code>ipc</code> - Arrow's IPC format serialization.</li> <li><code>decompress</code> - Automatically infer compression of csvs and decompress them. Supported compressions:</li> <li>gzip</li> <li>zlib</li> <li>zstd</li> </ul> </li> <li>Dataframe operations:<ul> <li><code>dynamic_group_by</code> - Group by based on a time window instead of predefined keys. Also activates rolling window group by operations.</li> <li><code>sort_multiple</code> - Allow sorting a dataframe on multiple columns.</li> <li><code>rows</code> - Create dataframe from rows and extract rows from <code>dataframes</code>. Also activates <code>pivot</code> and <code>transpose</code> operations.</li> <li><code>join_asof</code> - Join ASOF, to join on nearest keys instead of exact equality match.</li> <li><code>cross_join</code> - Create the Cartesian product of two dataframes.</li> <li><code>semi_anti_join</code> - SEMI and ANTI joins.</li> <li><code>row_hash</code> - Utility to hash dataframe rows to <code>UInt64Chunked</code>.</li> <li><code>diagonal_concat</code> - Diagonal concatenation thereby combining different schemas.</li> <li><code>dataframe_arithmetic</code> - Arithmetic between dataframes and other dataframes or series.</li> <li><code>partition_by</code> - Split into multiple dataframes partitioned by groups.</li> </ul> </li> <li>Series/expression operations:<ul> <li><code>is_in</code> - Check for membership in Series.</li> <li><code>zip_with</code> - Zip two <code>Series</code> / <code>ChunkedArray</code>s.</li> <li><code>round_series</code> - round underlying float types of series.</li> <li><code>repeat_by</code> - Repeat element in an array a number of times specified by another array.</li> <li><code>is_first_distinct</code> - Check if element is first unique value.</li> <li><code>is_last_distinct</code> - Check if element is last unique value.</li> <li><code>checked_arithmetic</code> - checked arithmetic returning <code>None</code> on invalid operations.</li> <li><code>dot_product</code> - Dot/inner product on series and expressions.</li> <li><code>concat_str</code> - Concatenate string data in linear time.</li> <li><code>reinterpret</code> - Utility to reinterpret bits to signed/unsigned.</li> <li><code>take_opt_iter</code> - Take from a series with <code>Iterator&lt;Item=Option&lt;usize&gt;&gt;</code>.</li> <li><code>mode</code> - Return the most frequently occurring value(s).</li> <li><code>cum_agg</code> - <code>cum_sum</code>, <code>cum_min</code>, and <code>cum_max</code>, aggregations.</li> <li><code>rolling_window</code> - rolling window functions, like <code>rolling_mean</code>.</li> <li><code>interpolate</code> - Interpolate intermediate <code>None</code> values.</li> <li><code>extract_jsonpath</code> - Run <code>jsonpath</code> queries on <code>StringChunked</code>.</li> <li><code>list</code> - List utils:</li> <li><code>list_gather</code> - take sublist by multiple indices.</li> <li><code>rank</code> - Ranking algorithms.</li> <li><code>moment</code> - Kurtosis and skew statistics.</li> <li><code>ewma</code> - Exponential moving average windows.</li> <li><code>abs</code> - Get absolute values of series.</li> <li><code>arange</code> - Range operation on series.</li> <li><code>product</code> - Compute the product of a series.</li> <li><code>diff</code> - <code>diff</code> operation.</li> <li><code>pct_change</code> - Compute change percentages.</li> <li><code>unique_counts</code> - Count unique values in expressions.</li> <li><code>log</code> - Logarithms for series.</li> <li><code>list_to_struct</code> - Convert <code>List</code> to <code>Struct</code> data types.</li> <li><code>list_count</code> - Count elements in lists.</li> <li><code>list_eval</code> - Apply expressions over list elements.</li> <li><code>cumulative_eval</code> - Apply expressions over cumulatively increasing windows.</li> <li><code>arg_where</code> - Get indices where condition holds.</li> <li><code>search_sorted</code> - Find indices where elements should be inserted to maintain order.</li> <li><code>offset_by</code> - Add an offset to dates that take months and leap years into account.</li> <li><code>trigonometry</code> - Trigonometric functions.</li> <li><code>sign</code> - Compute the element-wise sign of a series.</li> <li><code>propagate_nans</code> - <code>NaN</code>-propagating min/max aggregations.</li> </ul> </li> <li>Dataframe pretty printing:<ul> <li><code>fmt</code> - Activate dataframe formatting.</li> </ul> </li> </ul> <ol> <li> <p>Only needed if you are on Windows.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/concepts/","title":"Concepts","text":"<p>This chapter describes the core concepts of the Polars API. Understanding these will help you optimise your queries on a daily basis. We will cover the following topics:</p> <ul> <li>Data types and structures</li> <li>Expressions and contexts</li> <li>Lazy API</li> </ul>"},{"location":"user-guide/concepts/data-types-and-structures/","title":"Data types and structures","text":""},{"location":"user-guide/concepts/data-types-and-structures/#data-types","title":"Data types","text":"<p>Polars supports a variety of data types that fall broadly under the following categories:</p> <ul> <li>Numeric data types: signed integers, unsigned integers, floating point numbers, and decimals.</li> <li>Nested data types: lists, structs, and arrays.</li> <li>Temporal: dates, datetimes, times, and time deltas.</li> <li>Miscellaneous: strings, binary data, Booleans, categoricals, enums, and objects.</li> </ul> <p>All types support missing values represented by the special value <code>null</code>. This is not to be conflated with the special value <code>NaN</code> in floating number data types; see the section about floating point numbers for more information.</p> <p>You can also find a full table with all data types supported in the appendix with notes on when to use each data type and with links to relevant parts of the documentation.</p>"},{"location":"user-guide/concepts/data-types-and-structures/#series","title":"Series","text":"<p>The core base data structures provided by Polars are series and dataframes. A series is a 1-dimensional homogeneous data structure. By \u201chomogeneous\u201d we mean that all elements inside a series have the same data type. The snippet below shows how to create a named series:</p>  Python Rust <p> <code>Series</code> <pre><code>import polars as pl\n\ns = pl.Series(\"ints\", [1, 2, 3, 4, 5])\nprint(s)\n</code></pre></p> <p> <code>Series</code> <pre><code>use polars::prelude::*;\n\nlet s = Series::new(\"ints\".into(), &amp;[1, 2, 3, 4, 5]);\n\nprintln!(\"{s}\");\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'ints' [i64]\n[\n    1\n    2\n    3\n    4\n    5\n]\n</code></pre> <p>When creating a series, Polars will infer the data type from the values you provide. You can specify a concrete data type to override the inference mechanism:</p>  Python Rust <p> <code>Series</code> <pre><code>s1 = pl.Series(\"ints\", [1, 2, 3, 4, 5])\ns2 = pl.Series(\"uints\", [1, 2, 3, 4, 5], dtype=pl.UInt64)\nprint(s1.dtype, s2.dtype)\n</code></pre></p> <p> <code>Series</code> <pre><code>let s1 = Series::new(\"ints\".into(), &amp;[1, 2, 3, 4, 5]);\nlet s2 = Series::new(\"uints\".into(), &amp;[1, 2, 3, 4, 5])\n    .cast(&amp;DataType::UInt64) // Here, we actually cast after inference.\n    .unwrap();\nprintln!(\"{} {}\", s1.dtype(), s2.dtype()); // i32 u64\n</code></pre></p> <pre><code>Int64 UInt64\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#dataframe","title":"Dataframe","text":"<p>A dataframe is a 2-dimensional heterogeneous data structure that contains uniquely named series. By holding your data in a dataframe you will be able to use the Polars API to write queries that manipulate your data. You will be able to do this by using the contexts and expressions provided by Polars that we will talk about next.</p> <p>The snippet below shows how to create a dataframe from a dictionary of lists:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>from datetime import date\n\ndf = pl.DataFrame(\n    {\n        \"name\": [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n        \"birthdate\": [\n            date(1997, 1, 10),\n            date(1985, 2, 15),\n            date(1983, 3, 22),\n            date(1981, 4, 30),\n        ],\n        \"weight\": [57.9, 72.5, 53.6, 83.1],  # (kg)\n        \"height\": [1.56, 1.77, 1.65, 1.75],  # (m)\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use chrono::prelude::*;\n\nlet df: DataFrame = df!(\n    \"name\" =&gt; [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n    \"birthdate\" =&gt; [\n        NaiveDate::from_ymd_opt(1997, 1, 10).unwrap(),\n        NaiveDate::from_ymd_opt(1985, 2, 15).unwrap(),\n        NaiveDate::from_ymd_opt(1983, 3, 22).unwrap(),\n        NaiveDate::from_ymd_opt(1981, 4, 30).unwrap(),\n    ],\n    \"weight\" =&gt; [57.9, 72.5, 53.6, 83.1],  // (kg)\n    \"height\" =&gt; [1.56, 1.77, 1.65, 1.75],  // (m)\n)\n.unwrap();\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#inspecting-a-dataframe","title":"Inspecting a dataframe","text":"<p>In this subsection we will show some useful methods to quickly inspect a dataframe. We will use the dataframe we created earlier as a starting point.</p>"},{"location":"user-guide/concepts/data-types-and-structures/#head","title":"Head","text":"<p>The function <code>head</code> shows the first rows of a dataframe. By default, you get the first 5 rows but you can also specify the number of rows you want:</p>  Python Rust <p> <code>head</code> <pre><code>print(df.head(3))\n</code></pre></p> <p> <code>head</code> <pre><code>let df_head = df.head(Some(3));\n\nprintln!(\"{df_head}\");\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name         \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---          \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str          \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown    \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#glimpse","title":"Glimpse","text":"<p>The function <code>glimpse</code> is another function that shows the values of the first few rows of a dataframe, but formats the output differently from <code>head</code>. Here, each line of the output corresponds to a single column, making it easier to inspect wider dataframes:</p>  Python <p> <code>glimpse</code></p> <pre><code>print(df.glimpse(return_as_string=True))\n</code></pre> <pre><code>Rows: 4\nColumns: 4\n$ name       &lt;str&gt; 'Alice Archer', 'Ben Brown', 'Chloe Cooper', 'Daniel Donovan'\n$ birthdate &lt;date&gt; 1997-01-10, 1985-02-15, 1983-03-22, 1981-04-30\n$ weight     &lt;f64&gt; 57.9, 72.5, 53.6, 83.1\n$ height     &lt;f64&gt; 1.56, 1.77, 1.65, 1.75\n</code></pre> <p>Info</p> <p><code>glimpse</code> is only available for Python users.</p>"},{"location":"user-guide/concepts/data-types-and-structures/#tail","title":"Tail","text":"<p>The function <code>tail</code> shows the last rows of a dataframe. By default, you get the last 5 rows but you can also specify the number of rows you want, similar to how <code>head</code> works:</p>  Python Rust <p> <code>tail</code> <pre><code>print(df.tail(3))\n</code></pre></p> <p> <code>tail</code> <pre><code>let df_tail = df.tail(Some(3));\n\nprintln!(\"{df_tail}\");\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#sample","title":"Sample","text":"<p>If you think the first or last rows of your dataframe are not representative of your data, you can use <code>sample</code> to get an arbitrary number of randomly selected rows from the DataFrame. Note that the rows are not necessarily returned in the same order as they appear in the dataframe:</p>  Python Rust <p> <code>sample</code> <pre><code>import random\n\nrandom.seed(42)  # For reproducibility.\n\nprint(df.sample(2))\n</code></pre></p> <p> <code>sample_n</code> <pre><code>let n = Series::new(\"\".into(), &amp;[2]);\nlet sampled_df = df.sample_n(&amp;n, false, false, None).unwrap();\n\nprintln!(\"{sampled_df}\");\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name         \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---          \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str          \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown    \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#describe","title":"Describe","text":"<p>You can also use <code>describe</code> to compute summary statistics for all columns of your dataframe:</p>  Python Rust <p> <code>describe</code> <pre><code>print(df.describe())\n</code></pre></p> <p> <code>describe</code> \u00b7  Available on feature describe <pre><code>// Not available in Rust\n</code></pre></p> <pre><code>shape: (9, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 statistic  \u2506 name           \u2506 birthdate           \u2506 weight    \u2506 height   \u2502\n\u2502 ---        \u2506 ---            \u2506 ---                 \u2506 ---       \u2506 ---      \u2502\n\u2502 str        \u2506 str            \u2506 str                 \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 count      \u2506 4              \u2506 4                   \u2506 4.0       \u2506 4.0      \u2502\n\u2502 null_count \u2506 0              \u2506 0                   \u2506 0.0       \u2506 0.0      \u2502\n\u2502 mean       \u2506 null           \u2506 1986-09-04 00:00:00 \u2506 66.775    \u2506 1.6825   \u2502\n\u2502 std        \u2506 null           \u2506 null                \u2506 13.560082 \u2506 0.097082 \u2502\n\u2502 min        \u2506 Alice Archer   \u2506 1981-04-30          \u2506 53.6      \u2506 1.56     \u2502\n\u2502 25%        \u2506 null           \u2506 1983-03-22          \u2506 57.9      \u2506 1.65     \u2502\n\u2502 50%        \u2506 null           \u2506 1985-02-15          \u2506 72.5      \u2506 1.75     \u2502\n\u2502 75%        \u2506 null           \u2506 1985-02-15          \u2506 72.5      \u2506 1.75     \u2502\n\u2502 max        \u2506 Daniel Donovan \u2506 1997-01-10          \u2506 83.1      \u2506 1.77     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#schema","title":"Schema","text":"<p>When talking about data (in a dataframe or otherwise) we can refer to its schema. The schema is a mapping of column or series names to the data types of those same columns or series.</p> <p>You can check the schema of a dataframe with <code>schema</code>:</p>  Python Rust <pre><code>print(df.schema)\n</code></pre> <pre><code>println!(\"{:?}\", df.schema());\n</code></pre> <pre><code>Schema({'name': String, 'birthdate': Date, 'weight': Float64, 'height': Float64})\n</code></pre> <p>Much like with series, Polars will infer the schema of a dataframe when you create it but you can override the inference system if needed.</p> <p>In Python, you can specify an explicit schema by using a dictionary to map column names to data types. You can use the value <code>None</code> if you do not wish to override inference for a given column:</p> <pre><code>df = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Ben\", \"Chloe\", \"Daniel\"],\n        \"age\": [27, 39, 41, 43],\n    },\n    schema={\"name\": None, \"age\": pl.UInt8},\n)\n\nprint(df)\n</code></pre> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name   \u2506 age \u2502\n\u2502 ---    \u2506 --- \u2502\n\u2502 str    \u2506 u8  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice  \u2506 27  \u2502\n\u2502 Ben    \u2506 39  \u2502\n\u2502 Chloe  \u2506 41  \u2502\n\u2502 Daniel \u2506 43  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you only need to override the inference of some columns, the parameter <code>schema_overrides</code> tends to be more convenient because it lets you omit columns for which you do not want to override the inference:</p> <pre><code>df = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Ben\", \"Chloe\", \"Daniel\"],\n        \"age\": [27, 39, 41, 43],\n    },\n    schema_overrides={\"age\": pl.UInt8},\n)\n\nprint(df)\n</code></pre> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name   \u2506 age \u2502\n\u2502 ---    \u2506 --- \u2502\n\u2502 str    \u2506 u8  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice  \u2506 27  \u2502\n\u2502 Ben    \u2506 39  \u2502\n\u2502 Chloe  \u2506 41  \u2502\n\u2502 Daniel \u2506 43  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types-and-structures/#data-types-internals","title":"Data types internals","text":"<p>Polars utilizes the Arrow Columnar Format for its data orientation. Following this specification allows Polars to transfer data to/from other tools that also use the Arrow specification with little to no overhead.</p> <p>Polars gets most of its performance from its query engine, the optimizations it performs on your query plans, and from the parallelization that it employs when running your expressions.</p>"},{"location":"user-guide/concepts/data-types-and-structures/#floating-point-numbers","title":"Floating point numbers","text":"<p>Polars generally follows the IEEE 754 floating point standard for <code>Float32</code> and <code>Float64</code>, with some exceptions:</p> <ul> <li>Any <code>NaN</code> compares equal to any other <code>NaN</code>, and greater than any non-<code>NaN</code> value.</li> <li>Operations do not guarantee any particular behavior on the sign of zero or <code>NaN</code>, nor on the   payload of <code>NaN</code> values. This is not just limited to arithmetic operations, e.g. a sort or group   by operation may canonicalize all zeroes to +0 and all <code>NaN</code>s to a positive <code>NaN</code> without payload   for efficient equality checks.</li> </ul> <p>Polars always attempts to provide reasonably accurate results for floating point computations but does not provide guarantees on the error unless mentioned otherwise. Generally speaking 100% accurate results are infeasibly expensive to achieve (requiring much larger internal representations than 64-bit floats), and thus some error is always to be expected.</p>"},{"location":"user-guide/concepts/data-types-and-structures/#appendix-full-data-types-table","title":"Appendix: full data types table","text":"Type(s) Details <code>Boolean</code> Boolean type that is bit packed efficiently. <code>Int8</code>, <code>Int16</code>, <code>Int32</code>, <code>Int64</code>, <code>Int128</code> Varying-precision signed integer types. <code>UInt8</code>, <code>UInt16</code>, <code>UInt32</code>, <code>UInt64</code>, <code>UInt128</code> Varying-precision unsigned integer types. <code>Float16</code>, <code>Float32</code>, <code>Float64</code> Varying-precision signed floating point numbers. <code>Decimal</code> Decimal 128-bit type with optional precision and non-negative scale. Use this if you need fine-grained control over the precision of your floats and the operations you make on them. See Python's <code>decimal.Decimal</code> for documentation on what a decimal data type is. <code>String</code> Variable length UTF-8 encoded string data, typically Human-readable. <code>Binary</code> Stores arbitrary, varying length raw binary data. <code>Date</code> Represents a calendar date. <code>Time</code> Represents a time of day. <code>Datetime</code> Represents a calendar date and time of day. <code>Duration</code> Represents a time duration. <code>Array</code> Arrays with a known, fixed shape per series; akin to numpy arrays. Learn more about how arrays and lists differ and how to work with both. <code>List</code> Homogeneous 1D container with variable length. Learn more about how arrays and lists differ and how to work with both. <code>Object</code> Wraps arbitrary Python objects. <code>Categorical</code> Efficient encoding of string data where the categories are inferred at runtime. Learn more about how categoricals and enums differ and how to work with both. <code>Enum</code> Efficient ordered encoding of a set of predetermined string categories. Learn more about how categoricals and enums differ and how to work with both. <code>Struct</code> Composite product type that can store multiple fields. Learn more about the data type <code>Struct</code> in its dedicated documentation section.. <code>Null</code> Represents null values."},{"location":"user-guide/concepts/expressions-and-contexts/","title":"Expressions and contexts","text":"<p>Polars has developed its own Domain Specific Language (DSL) for transforming data. The language is very easy to use and allows for complex queries that remain human readable. Expressions and contexts, which will be introduced here, are very important in achieving this readability while also allowing the Polars query engine to optimize your queries to make them run as fast as possible.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#expressions","title":"Expressions","text":"<p>In Polars, an expression is a lazy representation of a data transformation. Expressions are modular and flexible, which means you can use them as building blocks to build more complex expressions. Here is an example of a Polars expression:</p> <pre><code>import polars as pl\n\npl.col(\"weight\") / (pl.col(\"height\") ** 2)\n</code></pre> <p>As you might be able to guess, this expression takes a column named \u201cweight\u201d and divides its values by the square of the values in a column \u201cheight\u201d, computing a person's BMI.</p> <p>The code above expresses an abstract computation that we can save in a variable, manipulate further, or just print:</p> <pre><code>bmi_expr = pl.col(\"weight\") / (pl.col(\"height\") ** 2)\nprint(bmi_expr)\n</code></pre> <pre><code>[(col(\"weight\")) / (col(\"height\").pow([dyn int: 2]))]\n</code></pre> <p>Because expressions are lazy, no computations have taken place yet. That's what we need contexts for.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#contexts","title":"Contexts","text":"<p>Polars expressions need a context in which they are executed to produce a result. Depending on the context it is used in, the same Polars expression can produce different results. In this section, we will learn about the four most common contexts that Polars provides<sup>1</sup>:</p> <ol> <li><code>select</code></li> <li><code>with_columns</code></li> <li><code>filter</code></li> <li><code>group_by</code></li> </ol> <p>We use the dataframe below to show how each of the contexts works.</p>  Python Rust <pre><code>from datetime import date\n\ndf = pl.DataFrame(\n    {\n        \"name\": [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n        \"birthdate\": [\n            date(1997, 1, 10),\n            date(1985, 2, 15),\n            date(1983, 3, 22),\n            date(1981, 4, 30),\n        ],\n        \"weight\": [57.9, 72.5, 53.6, 83.1],  # (kg)\n        \"height\": [1.56, 1.77, 1.65, 1.75],  # (m)\n    }\n)\n\nprint(df)\n</code></pre> <pre><code>use chrono::prelude::*;\nuse polars::prelude::*;\n\nlet df: DataFrame = df!(\n    \"name\" =&gt; [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n    \"birthdate\" =&gt; [\n        NaiveDate::from_ymd_opt(1997, 1, 10).unwrap(),\n        NaiveDate::from_ymd_opt(1985, 2, 15).unwrap(),\n        NaiveDate::from_ymd_opt(1983, 3, 22).unwrap(),\n        NaiveDate::from_ymd_opt(1981, 4, 30).unwrap(),\n    ],\n    \"weight\" =&gt; [57.9, 72.5, 53.6, 83.1],  // (kg)\n    \"height\" =&gt; [1.56, 1.77, 1.65, 1.75],  // (m)\n)\n.unwrap();\nprintln!(\"{df}\");\n</code></pre> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/expressions-and-contexts/#select","title":"<code>select</code>","text":"<p>The selection context <code>select</code> applies expressions over columns. The context <code>select</code> may produce new columns that are aggregations, combinations of other columns, or literals:</p>  Python Rust <p> <code>select</code> <pre><code>result = df.select(\n    bmi=bmi_expr,\n    avg_bmi=bmi_expr.mean(),\n    ideal_max_bmi=25,\n)\nprint(result)\n</code></pre></p> <p> <code>select</code> <pre><code>let bmi = col(\"weight\") / col(\"height\").pow(2);\nlet result = df\n    .clone()\n    .lazy()\n    .select([\n        bmi.clone().alias(\"bmi\"),\n        bmi.clone().mean().alias(\"avg_bmi\"),\n        lit(25).alias(\"ideal_max_bmi\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bmi       \u2506 avg_bmi   \u2506 ideal_max_bmi \u2502\n\u2502 ---       \u2506 ---       \u2506 ---           \u2502\n\u2502 f64       \u2506 f64       \u2506 i32           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 23.791913 \u2506 23.438973 \u2506 25            \u2502\n\u2502 23.141498 \u2506 23.438973 \u2506 25            \u2502\n\u2502 19.687787 \u2506 23.438973 \u2506 25            \u2502\n\u2502 27.134694 \u2506 23.438973 \u2506 25            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The expressions in a context <code>select</code> must produce series that are all the same length or they must produce a scalar. Scalars will be broadcast to match the length of the remaining series. Literals, like the number used above, are also broadcast.</p> <p>Note that broadcasting can also occur within expressions. For instance, consider the expression below:</p>  Python Rust <p> <code>select</code> <pre><code>result = df.select(deviation=(bmi_expr - bmi_expr.mean()) / bmi_expr.std())\nprint(result)\n</code></pre></p> <p> <code>select</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([((bmi.clone() - bmi.clone().mean()) / bmi.clone().std(1)).alias(\"deviation\")])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 deviation \u2502\n\u2502 ---       \u2502\n\u2502 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.115645  \u2502\n\u2502 -0.097471 \u2502\n\u2502 -1.22912  \u2502\n\u2502 1.210946  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Both the subtraction and the division use broadcasting within the expression because the subexpressions that compute the mean and the standard deviation evaluate to single values.</p> <p>The context <code>select</code> is very flexible and powerful and allows you to evaluate arbitrary expressions independent of, and in parallel to, each other. This is also true of the other contexts that we will see next.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#with_columns","title":"<code>with_columns</code>","text":"<p>The context <code>with_columns</code> is very similar to the context <code>select</code>. The main difference between the two is that the context <code>with_columns</code> creates a new dataframe that contains the columns from the original dataframe and the new columns according to its input expressions, whereas the context <code>select</code> only includes the columns selected by its input expressions:</p>  Python Rust <p> <code>with_columns</code> <pre><code>result = df.with_columns(\n    bmi=bmi_expr,\n    avg_bmi=bmi_expr.mean(),\n    ideal_max_bmi=25,\n)\nprint(result)\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .with_columns([\n        bmi.clone().alias(\"bmi\"),\n        bmi.mean().alias(\"avg_bmi\"),\n        lit(25).alias(\"ideal_max_bmi\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name           \u2506 birthdate  \u2506 weight \u2506 height \u2506 bmi       \u2506 avg_bmi   \u2506 ideal_max_bmi \u2502\n\u2502 ---            \u2506 ---        \u2506 ---    \u2506 ---    \u2506 ---       \u2506 ---       \u2506 ---           \u2502\n\u2502 str            \u2506 date       \u2506 f64    \u2506 f64    \u2506 f64       \u2506 f64       \u2506 i32           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice Archer   \u2506 1997-01-10 \u2506 57.9   \u2506 1.56   \u2506 23.791913 \u2506 23.438973 \u2506 25            \u2502\n\u2502 Ben Brown      \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2506 23.141498 \u2506 23.438973 \u2506 25            \u2502\n\u2502 Chloe Cooper   \u2506 1983-03-22 \u2506 53.6   \u2506 1.65   \u2506 19.687787 \u2506 23.438973 \u2506 25            \u2502\n\u2502 Daniel Donovan \u2506 1981-04-30 \u2506 83.1   \u2506 1.75   \u2506 27.134694 \u2506 23.438973 \u2506 25            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Because of this difference between <code>select</code> and <code>with_columns</code>, the expressions used in a context <code>with_columns</code> must produce series that have the same length as the original columns in the dataframe, whereas it is enough for the expressions in the context <code>select</code> to produce series that have the same length among them.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#filter","title":"<code>filter</code>","text":"<p>The context <code>filter</code> filters the rows of a dataframe based on one or more expressions that evaluate to the Boolean data type.</p>  Python Rust <p> <code>filter</code> <pre><code>result = df.filter(\n    pl.col(\"birthdate\").is_between(date(1982, 12, 31), date(1996, 1, 1)),\n    pl.col(\"height\") &gt; 1.7,\n)\nprint(result)\n</code></pre></p> <p> <code>filter</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .filter(\n        col(\"birthdate\")\n            .is_between(\n                lit(NaiveDate::from_ymd_opt(1982, 12, 31).unwrap()),\n                lit(NaiveDate::from_ymd_opt(1996, 1, 1).unwrap()),\n                ClosedInterval::Both,\n            )\n            .and(col(\"height\").gt(lit(1.7))),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name      \u2506 birthdate  \u2506 weight \u2506 height \u2502\n\u2502 ---       \u2506 ---        \u2506 ---    \u2506 ---    \u2502\n\u2502 str       \u2506 date       \u2506 f64    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Ben Brown \u2506 1985-02-15 \u2506 72.5   \u2506 1.77   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/expressions-and-contexts/#group_by-and-aggregations","title":"<code>group_by</code> and aggregations","text":"<p>In the context <code>group_by</code>, rows are grouped according to the unique values of the grouping expressions. You can then apply expressions to the resulting groups, which may be of variable lengths.</p> <p>When using the context <code>group_by</code>, you can use an expression to compute the groupings dynamically:</p>  Python Rust <p> <code>group_by</code> <pre><code>result = df.group_by(\n    (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n).agg(pl.col(\"name\"))\nprint(result)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .group_by([(col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\")])\n    .agg([col(\"name\")])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 name                            \u2502\n\u2502 ---    \u2506 ---                             \u2502\n\u2502 i32    \u2506 list[str]                       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1990   \u2506 [\"Alice Archer\"]                \u2502\n\u2502 1980   \u2506 [\"Ben Brown\", \"Chloe Cooper\", \u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After using <code>group_by</code> we use <code>agg</code> to apply aggregating expressions to the groups. Since in the example above we only specified the name of a column, we get the groups of that column as lists.</p> <p>We can specify as many grouping expressions as we'd like and the context <code>group_by</code> will group the rows according to the distinct values across the expressions specified. Here, we group by a combination of decade of birth and whether the person is shorter than 1.7 metres:</p>  Python Rust <p> <code>group_by</code> <pre><code>result = df.group_by(\n    (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n    (pl.col(\"height\") &lt; 1.7).alias(\"short?\"),\n).agg(pl.col(\"name\"))\nprint(result)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .group_by([\n        (col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\"),\n        (col(\"height\").lt(lit(1.7)).alias(\"short?\")),\n    ])\n    .agg([col(\"name\")])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 short? \u2506 name                            \u2502\n\u2502 ---    \u2506 ---    \u2506 ---                             \u2502\n\u2502 i32    \u2506 bool   \u2506 list[str]                       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1990   \u2506 true   \u2506 [\"Alice Archer\"]                \u2502\n\u2502 1980   \u2506 true   \u2506 [\"Chloe Cooper\"]                \u2502\n\u2502 1980   \u2506 false  \u2506 [\"Ben Brown\", \"Daniel Donovan\"\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The resulting dataframe, after applying aggregating expressions, contains one column per each grouping expression on the left and then as many columns as needed to represent the results of the aggregating expressions. In turn, we can specify as many aggregating expressions as we want:</p>  Python Rust <p> <code>group_by</code> <pre><code>result = df.group_by(\n    (pl.col(\"birthdate\").dt.year() // 10 * 10).alias(\"decade\"),\n    (pl.col(\"height\") &lt; 1.7).alias(\"short?\"),\n).agg(\n    pl.len(),\n    pl.col(\"height\").max().alias(\"tallest\"),\n    pl.col(\"weight\", \"height\").mean().name.prefix(\"avg_\"),\n)\nprint(result)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .group_by([\n        (col(\"birthdate\").dt().year() / lit(10) * lit(10)).alias(\"decade\"),\n        (col(\"height\").lt(lit(1.7)).alias(\"short?\")),\n    ])\n    .agg([\n        len(),\n        col(\"height\").max().alias(\"tallest\"),\n        cols([\"weight\", \"height\"])\n            .as_expr()\n            .mean()\n            .name()\n            .prefix(\"avg_\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 decade \u2506 short? \u2506 len \u2506 tallest \u2506 avg_weight \u2506 avg_height \u2502\n\u2502 ---    \u2506 ---    \u2506 --- \u2506 ---     \u2506 ---        \u2506 ---        \u2502\n\u2502 i32    \u2506 bool   \u2506 u32 \u2506 f64     \u2506 f64        \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1980   \u2506 false  \u2506 2   \u2506 1.77    \u2506 77.8       \u2506 1.76       \u2502\n\u2502 1980   \u2506 true   \u2506 1   \u2506 1.65    \u2506 53.6       \u2506 1.65       \u2502\n\u2502 1990   \u2506 true   \u2506 1   \u2506 1.56    \u2506 57.9       \u2506 1.56       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See also <code>group_by_dynamic</code> and <code>rolling</code> for other grouping contexts.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#expression-expansion","title":"Expression expansion","text":"<p>The last example contained two grouping expressions and three aggregating expressions, and yet the resulting dataframe contained six columns instead of five. If we look closely, the last aggregating expression mentioned two different columns: \u201cweight\u201d and \u201cheight\u201d.</p> <p>Polars expressions support a feature called expression expansion. Expression expansion is like a shorthand notation for when you want to apply the same transformation to multiple columns. As we have seen, the expression</p> <pre><code>pl.col(\"weight\", \"height\").mean().name.prefix(\"avg_\")\n</code></pre> <p>will compute the mean value of the columns \u201cweight\u201d and \u201cheight\u201d and will rename them as \u201cavg_weight\u201d and \u201cavg_height\u201d, respectively. In fact, the expression above is equivalent to using the two following expressions:</p> <pre><code>[\n    pl.col(\"weight\").mean().alias(\"avg_weight\"),\n    pl.col(\"height\").mean().alias(\"avg_height\"),\n]\n</code></pre> <p>In this case, this expression expands into two independent expressions that Polars can execute in parallel. In other cases, we may not be able to know in advance how many independent expressions an expression will unfold into.</p> <p>Consider this simple but elucidative example:</p> <pre><code>(pl.col(pl.Float64) * 1.1).name.suffix(\"*1.1\")\n</code></pre> <p>This expression will multiply all columns with data type <code>Float64</code> by <code>1.1</code>. The number of columns this applies to depends on the schema of each dataframe. In the case of the dataframe we have been using, it applies to two columns:</p>  Python Rust <p> <code>group_by</code> <pre><code>expr = (pl.col(pl.Float64) * 1.1).name.suffix(\"*1.1\")\nresult = df.select(expr)\nprint(result)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let expr = (dtype_col(&amp;DataType::Float64).as_selector().as_expr() * lit(1.1))\n    .name()\n    .suffix(\"*1.1\");\nlet result = df.lazy().select([expr.clone()]).collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 weight*1.1 \u2506 height*1.1 \u2502\n\u2502 ---        \u2506 ---        \u2502\n\u2502 f64        \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 63.69      \u2506 1.716      \u2502\n\u2502 79.75      \u2506 1.947      \u2502\n\u2502 58.96      \u2506 1.815      \u2502\n\u2502 91.41      \u2506 1.925      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the case of the dataframe <code>df2</code> below, the same expression expands to 0 columns because no column has the data type <code>Float64</code>:</p>  Python Rust <p> <code>group_by</code> <pre><code>df2 = pl.DataFrame(\n    {\n        \"ints\": [1, 2, 3, 4],\n        \"letters\": [\"A\", \"B\", \"C\", \"D\"],\n    }\n)\nresult = df2.select(expr)\nprint(result)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df2: DataFrame = df!(\n    \"ints\" =&gt; [1, 2, 3, 4],\n    \"letters\" =&gt; [\"A\", \"B\", \"C\", \"D\"],\n)\n.unwrap();\nlet result = df2.lazy().select([expr]).collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (0, 0)\n\u250c\u2510\n\u255e\u2561\n\u2514\u2518\n</code></pre> <p>It is equally easy to imagine a scenario where the same expression would expand to dozens of columns.</p> <p>Next, you will learn about the lazy API and the function <code>explain</code>, which you can use to preview what an expression will expand to given a schema.</p>"},{"location":"user-guide/concepts/expressions-and-contexts/#conclusion","title":"Conclusion","text":"<p>Because expressions are lazy, when you use an expression inside a context Polars can try to simplify your expression before running the data transformation it expresses. Separate expressions within a context are embarrassingly parallel and Polars will take advantage of that, while also parallelizing expression execution when using expression expansion. Further performance gains can be obtained when using the lazy API of Polars, which is introduced next.</p> <p>We have only scratched the surface of the capabilities of expressions. There are a ton more expressions and they can be combined in a variety of ways. See the section on expressions for a deeper dive on the different types of expressions available.</p> <ol> <li> <p>There are additional List and SQL contexts which are covered later in this guide. But for simplicity, we leave them out of scope for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/concepts/lazy-api/","title":"Lazy API","text":"<p>Polars supports two modes of operation: lazy and eager. The examples so far have used the eager API, in which the query is executed immediately. In the lazy API, the query is only evaluated once it is collected. Deferring the execution to the last minute can have significant performance advantages and is why the lazy API is preferred in most cases. Let us demonstrate this with an example:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/assets/data/iris.csv\")\ndf_small = df.filter(pl.col(\"sepal_length\") &gt; 5)\ndf_agg = df_small.group_by(\"species\").agg(pl.col(\"sepal_width\").mean())\nprint(df_agg)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReadOptions::default()\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/iris.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap();\nlet mask = df.column(\"sepal_length\")?.f64()?.gt(5.0);\nlet df_small = df.filter(&amp;mask)?;\n#[allow(deprecated)]\nlet df_agg = df_small\n    .group_by([\"species\"])?\n    .select([\"sepal_width\"])\n    .mean()?;\nprintln!(\"{df_agg}\");\n</code></pre></p> <p>In this example we use the eager API to:</p> <ol> <li>Read the iris dataset.</li> <li>Filter the dataset based on sepal length.</li> <li>Calculate the mean of the sepal width per species.</li> </ol> <p>Every step is executed immediately returning the intermediate results. This can be very wasteful as we might do work or load extra data that is not being used. If we instead used the lazy API and waited on execution until all the steps are defined then the query planner could perform various optimizations. In this case:</p> <ul> <li>Predicate pushdown: Apply filters as early as possible while reading the dataset, thus only   reading rows with sepal length greater than 5.</li> <li>Projection pushdown: Select only the columns that are needed while reading the dataset, thus   removing the need to load additional columns (e.g., petal length and petal width).</li> </ul>  Python Rust <p> <code>scan_csv</code> <pre><code>q = (\n    pl.scan_csv(\"docs/assets/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let q = LazyCsvReader::new(PlRefPath::new(\"docs/assets/data/iris.csv\"))\n    .with_has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"sepal_width\").mean()]);\nlet df = q.collect()?;\nprintln!(\"{df}\");\n</code></pre></p> <p>These will significantly lower the load on memory &amp; CPU thus allowing you to fit bigger datasets in memory and process them faster. Once the query is defined you call <code>collect</code> to inform Polars that you want to execute it. You can learn more about the lazy API in its dedicated chapter.</p> <p>Eager API</p> <p>In many cases the eager API is actually calling the lazy API under the hood and immediately collecting the result. This has the benefit that within the query itself optimization(s) made by the query planner can still take place.</p>"},{"location":"user-guide/concepts/lazy-api/#when-to-use-which","title":"When to use which","text":"<p>In general, the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don't know yet what your query is going to look like.</p>"},{"location":"user-guide/concepts/lazy-api/#previewing-the-query-plan","title":"Previewing the query plan","text":"<p>When using the lazy API you can use the function <code>explain</code> to ask Polars to create a description of the query plan that will be executed once you collect the results. This can be useful if you want to see what types of optimizations Polars performs on your queries. We can ask Polars to explain the query <code>q</code> we defined above:</p>  Python Rust <p> <code>explain</code> <pre><code>print(q.explain())\n</code></pre></p> <p> <code>explain</code> <pre><code>let q = LazyCsvReader::new(PlRefPath::new(\"docs/assets/data/iris.csv\"))\n    .with_has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"sepal_width\").mean()]);\nprintln!(\"{}\", q.explain(true)?);\n</code></pre></p> <pre><code>AGGREGATE[maintain_order: false]\n  [col(\"sepal_width\").mean()] BY [col(\"species\")]\n  FROM\n  Csv SCAN [docs/assets/data/iris.csv]\n  PROJECT 3/5 COLUMNS\n  SELECTION: [(col(\"sepal_length\")) &gt; (5.0)]\n  ESTIMATED ROWS: 167\n</code></pre> <p>Immediately, we can see in the explanation that Polars did apply predicate pushdown, as it is only reading rows where the sepal length is greater than 5, and it did apply projection pushdown, as it is only reading the columns that are needed by the query.</p> <p>The function <code>explain</code> can also be used to see how expression expansion will unfold in the context of a given schema. Consider the example expression from the section on expression expansion:</p> <pre><code>(pl.col(pl.Float64) * 1.1).name.suffix(\"*1.1\")\n</code></pre> <p>We can use <code>explain</code> to see how this expression would evaluate against an arbitrary schema:</p>  Python <p> <code>explain</code></p> <pre><code>schema = pl.Schema(\n    {\n        \"int_1\": pl.Int16,\n        \"int_2\": pl.Int32,\n        \"float_1\": pl.Float64,\n        \"float_2\": pl.Float64,\n        \"float_3\": pl.Float64,\n    }\n)\n\nprint(\n    pl.LazyFrame(schema=schema)\n    .select((pl.col(pl.Float64) * 1.1).name.suffix(\"*1.1\"))\n    .explain()\n)\n</code></pre> <pre><code>SELECT [[(col(\"float_1\")) * (1.1)].alias(\"float_1*1.1\"), [(col(\"float_2\")) * (1.1)].alias(\"float_2*1.1\"), [(col(\"float_3\")) * (1.1)].alias(\"float_3*1.1\")]\n  DF [\"int_1\", \"int_2\", \"float_1\", \"float_2\", ...]; PROJECT[\"float_1\", \"float_2\", \"float_3\"] 3/5 COLUMNS\n</code></pre>"},{"location":"user-guide/concepts/streaming/","title":"Streaming","text":"<p>One additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing all the data at once, Polars can execute the query in batches allowing you to process datasets that do not fit in memory. Besides memory pressure, the streaming engine also is more performant than Polars' in-memory engine.</p> <p>To tell Polars we want to execute a query in streaming mode we pass the <code>engine=\"streaming\"</code> argument to <code>collect</code>:</p>  Python Rust <p> <code>collect</code> <pre><code>q1 = (\n    pl.scan_csv(\"docs/assets/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\ndf = q1.collect(engine=\"streaming\")\n</code></pre></p> <p> <code>collect</code> \u00b7  Available on feature streaming <pre><code>let q1 = LazyCsvReader::new(PlRefPath::new(\"docs/assets/data/iris.csv\"))\n    .with_has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"sepal_width\").mean()]);\n\nlet df = q1.clone().with_new_streaming(true).collect()?;\nprintln!(\"{df}\");\n</code></pre></p>"},{"location":"user-guide/concepts/streaming/#inspecting-a-streaming-query","title":"Inspecting a streaming query","text":"<p>Polars can run many operations in a streaming manner. Some operations are inherently non-streaming, or are not implemented in a streaming manner (yet). In the latter case, Polars will fall back to the in-memory engine for those operations. A user doesn't have to know about this, but it can be interesting for debugging memory or performance issues.</p> <p>To inspect the physical plan of streaming query, you can plot the physical graph. The legend shows how memory intensive the operation can be.</p> <pre><code>q1 = (\n    pl.scan_csv(\"docs/assets/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(\n        mean_width=pl.col(\"sepal_width\").mean(),\n        mean_width2=pl.col(\"sepal_width\").sum() / pl.col(\"sepal_length\").count(),\n    )\n    .show_graph(plan_stage=\"physical\", engine=\"streaming\")\n)\n</code></pre> <p></p>"},{"location":"user-guide/expressions/","title":"Expressions","text":"<p>We introduced the concept of \u201cexpressions\u201d in a previous section. In this section we will focus on exploring the types of expressions that Polars offers. Each section gives an overview of what they do and provides additional examples.</p> <ul> <li>Essentials:<ul> <li>Basic operations \u2013 how to do basic operations on dataframe columns, like arithmetic calculations, comparisons, and other common, general-purpose operations</li> <li>Expression expansion \u2013 what is expression expansion and how to use it</li> <li>Casting \u2013 how to convert / cast values to different data types</li> </ul> </li> <li>How to work with specific types of data or data type namespaces:<ul> <li>Strings \u2013 how to work with strings and the namespace <code>str</code></li> <li>Lists and arrays \u2013 the differences between the data types <code>List</code> and <code>Array</code>, when to use them, and how to use them</li> <li>Categorical data and enums \u2013 the differences between the data types <code>Categorical</code> and <code>Enum</code>, when to use them, and how to use them</li> <li>Structs \u2013 when to use the data type <code>Struct</code> and how to use it</li> <li>Missing data \u2013 how to work with missing data and how to fill missing data</li> </ul> </li> <li>Types of operations:<ul> <li>Aggregation \u2013 how to work with aggregating contexts like <code>group_by</code></li> <li>Window functions \u2013 how to apply window functions over columns in a dataframe</li> <li>Folds \u2013 how to perform arbitrary computations horizontally across columns</li> </ul> </li> <li>User-defined Python functions \u2013 how to apply user-defined Python functions to dataframe columns or to column values</li> <li>Numpy functions \u2013 how to use NumPy native functions on Polars dataframes and series</li> </ul>"},{"location":"user-guide/expressions/aggregation/","title":"Aggregation","text":"<p>The Polars context <code>group_by</code> lets you apply expressions on subsets of columns, as defined by the unique values of the column over which the data is grouped. This is a very powerful capability that we explore in this section of the user guide.</p> <p>We start by reading in a US congress <code>dataset</code>:</p>  Python Rust <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> <pre><code>import polars as pl\n\nurl = \"hf://datasets/nameexhaustion/polars-docs/legislators-historical.csv\"\n\nschema_overrides = {\n    \"first_name\": pl.Categorical,\n    \"gender\": pl.Categorical,\n    \"type\": pl.Categorical,\n    \"state\": pl.Categorical,\n    \"party\": pl.Categorical,\n}\n\ndataset = (\n    pl.read_csv(url, schema_overrides=schema_overrides)\n    .with_columns(pl.col(\"first\", \"middle\", \"last\").name.suffix(\"_name\"))\n    .with_columns(pl.col(\"birthday\").str.to_date(strict=False))\n)\n</code></pre></p> <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> \u00b7  Available on feature dtype-categorical <pre><code>use std::io::Cursor;\n\nuse polars::prelude::*;\nuse reqwest::blocking::Client;\n\nlet url = \"https://huggingface.co/datasets/nameexhaustion/polars-docs/resolve/main/legislators-historical.csv\";\n\nlet mut schema = Schema::default();\nschema.with_column(\n    \"first_name\".into(),\n    DataType::from_categories(Categories::global()),\n);\nschema.with_column(\n    \"gender\".into(),\n    DataType::from_categories(Categories::global()),\n);\nschema.with_column(\n    \"type\".into(),\n    DataType::from_categories(Categories::global()),\n);\nschema.with_column(\n    \"state\".into(),\n    DataType::from_categories(Categories::global()),\n);\nschema.with_column(\n    \"party\".into(),\n    DataType::from_categories(Categories::global()),\n);\nschema.with_column(\"birthday\".into(), DataType::Date);\n\nlet data = Client::new().get(url).send()?.bytes()?;\n\nlet dataset = CsvReadOptions::default()\n    .with_has_header(true)\n    .with_schema_overwrite(Some(Arc::new(schema)))\n    .map_parse_options(|parse_options| parse_options.with_try_parse_dates(true))\n    .into_reader_with_file_handle(Cursor::new(data))\n    .finish()?\n    .lazy()\n    .with_columns([\n        col(\"first\").name().suffix(\"_name\"),\n        col(\"middle\").name().suffix(\"_name\"),\n        col(\"last\").name().suffix(\"_name\"),\n    ])\n    .collect()?;\n\nprintln!(\"{}\", &amp;dataset);\n</code></pre></p> <p></p>"},{"location":"user-guide/expressions/aggregation/#basic-aggregations","title":"Basic aggregations","text":"<p>You can easily apply multiple expressions to your aggregated values. Simply list all of the expressions you want inside the function <code>agg</code>. There is no upper bound on the number of aggregations you can do and you can make any combination you want. In the snippet below we will group the data based on the column \u201cfirst_name\u201d and then we will apply the following aggregations:</p> <ul> <li>count the number of rows in the group (which means we count how many people in the data set have   each unique first name);</li> <li>combine the values of the column \u201cgender\u201d into a list by referring the column but omitting an   aggregate function; and</li> <li>get the first value of the column \u201clast_name\u201d within the group.</li> </ul> <p>After computing the aggregations, we immediately sort the result and limit it to the top five rows so that we have a nice summary overview:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"first_name\")\n    .agg(\n        pl.len(),\n        pl.col(\"gender\"),\n        pl.first(\"last_name\"),  # Short for `pl.col(\"last_name\").first()`\n    )\n    .sort(\"len\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"first_name\"])\n    .agg([len(), col(\"gender\"), col(\"last_name\").first()])\n    .sort(\n        [\"len\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first_name \u2506 len  \u2506 gender            \u2506 last_name \u2502\n\u2502 ---        \u2506 ---  \u2506 ---               \u2506 ---       \u2502\n\u2502 str        \u2506 u32  \u2506 list[cat]         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 John       \u2506 4227 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Walker    \u2502\n\u2502 William    \u2506 3309 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Few       \u2502\n\u2502 James      \u2506 2414 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Armstrong \u2502\n\u2502 Charles    \u2506 1514 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Carroll   \u2502\n\u2502 Thomas     \u2506 1502 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Tucker    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>It's that easy! Let's turn it up a notch.</p>"},{"location":"user-guide/expressions/aggregation/#conditionals","title":"Conditionals","text":"<p>Let's say we want to know how many delegates of a state are \u201cPro\u201d or \u201cAnti\u201d administration. We can query that directly in the aggregation without the need for a <code>lambda</code> or grooming the dataframe:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"state\")\n    .agg(\n        (pl.col(\"party\") == \"Anti-Administration\").sum().alias(\"anti\"),\n        (pl.col(\"party\") == \"Pro-Administration\").sum().alias(\"pro\"),\n    )\n    .sort(\"pro\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\"])\n    .agg([\n        (col(\"party\").eq(lit(\"Anti-Administration\")))\n            .sum()\n            .alias(\"anti\"),\n        (col(\"party\").eq(lit(\"Pro-Administration\")))\n            .sum()\n            .alias(\"pro\"),\n    ])\n    .sort(\n        [\"pro\"],\n        SortMultipleOptions::default().with_order_descending(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 anti \u2506 pro \u2502\n\u2502 ---   \u2506 ---  \u2506 --- \u2502\n\u2502 cat   \u2506 u32  \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CT    \u2506 0    \u2506 5   \u2502\n\u2502 NJ    \u2506 0    \u2506 3   \u2502\n\u2502 DE    \u2506 1    \u2506 3   \u2502\n\u2502 NC    \u2506 2    \u2506 2   \u2502\n\u2502 MA    \u2506 0    \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#filtering","title":"Filtering","text":"<p>We can also filter the groups. Let's say we want to compute a mean per group, but we don't want to include all values from that group, and we also don't want to actually filter the rows from the dataframe because we need those rows for another aggregation.</p> <p>In the example below we show how this can be done.</p> <p>Note</p> <p>Note that we can define Python functions for clarity. These functions don't cost us anything because they return Polars expressions, we don't apply a custom function over a series during runtime of the query. Of course, you can write functions that return expressions in Rust, too.</p>  Python Rust <p> <code>group_by</code> <pre><code>from datetime import date\n\n\ndef compute_age():\n    return date.today().year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_age(gender: str) -&gt; pl.Expr:\n    return (\n        compute_age()\n        .filter(pl.col(\"gender\") == gender)\n        .mean()\n        .alias(f\"avg {gender} age\")\n    )\n\n\nq = (\n    dataset.lazy()\n    .group_by(\"state\")\n    .agg(\n        avg_age(\"M\"),\n        avg_age(\"F\"),\n        (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n        (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>fn compute_age() -&gt; Expr {\n    lit(2024) - col(\"birthday\").dt().year()\n}\n\nfn avg_birthday(gender: &amp;str) -&gt; Expr {\n    compute_age()\n        .filter(col(\"gender\").eq(lit(gender)))\n        .mean()\n        .alias(format!(\"avg {gender} birthday\"))\n}\n\nlet df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\"])\n    .agg([\n        avg_birthday(\"M\"),\n        avg_birthday(\"F\"),\n        (col(\"gender\").eq(lit(\"M\"))).sum().alias(\"# male\"),\n        (col(\"gender\").eq(lit(\"F\"))).sum().alias(\"# female\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 avg M age  \u2506 avg F age \u2506 # male \u2506 # female \u2502\n\u2502 ---   \u2506 ---        \u2506 ---       \u2506 ---    \u2506 ---      \u2502\n\u2502 cat   \u2506 f64        \u2506 f64       \u2506 u32    \u2506 u32      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 SD    \u2506 136.383648 \u2506 71.7      \u2506 159    \u2506 10       \u2502\n\u2502 MA    \u2506 184.137395 \u2506 120.0     \u2506 1558   \u2506 33       \u2502\n\u2502 KY    \u2506 173.77512  \u2506 95.142857 \u2506 1116   \u2506 7        \u2502\n\u2502 TN    \u2506 162.299803 \u2506 98.294118 \u2506 1066   \u2506 17       \u2502\n\u2502 AS    \u2506 84.411765  \u2506 null      \u2506 17     \u2506 0        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Do the average age values look nonsensical? That's because we are working with historical data that dates back to the 1800s and we are doing our computations assuming everyone represented in the dataset is still alive and kicking.</p>"},{"location":"user-guide/expressions/aggregation/#nested-grouping","title":"Nested grouping","text":"<p>The two previous queries could have been done with a nested <code>group_by</code>, but that wouldn't have let us show off some of these features. \ud83d\ude09 To do a nested <code>group_by</code>, simply list the columns that will be used for grouping.</p> <p>First, we use a nested <code>group_by</code> to figure out how many delegates of a state are \u201cPro\u201d or \u201cAnti\u201d administration:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"state\", \"party\")\n    .agg(pl.len().alias(\"count\"))\n    .filter(\n        (pl.col(\"party\") == \"Anti-Administration\")\n        | (pl.col(\"party\") == \"Pro-Administration\")\n    )\n    .sort(\"count\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\", \"party\"])\n    .agg([len().alias(\"count\")])\n    .filter(\n        col(\"party\")\n            .eq(lit(\"Anti-Administration\"))\n            .or(col(\"party\").eq(lit(\"Pro-Administration\"))),\n    )\n    .sort(\n        [\"count\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 party               \u2506 count \u2502\n\u2502 ---   \u2506 ---                 \u2506 ---   \u2502\n\u2502 cat   \u2506 cat                 \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CT    \u2506 Pro-Administration  \u2506 5     \u2502\n\u2502 VA    \u2506 Anti-Administration \u2506 5     \u2502\n\u2502 NJ    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2502 PA    \u2506 Anti-Administration \u2506 3     \u2502\n\u2502 DE    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Next, we use a nested <code>group_by</code> to compute the average age of delegates per state and per gender:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"state\", \"gender\")\n    .agg(\n        # The function `avg_age` is not needed:\n        compute_age().mean().alias(\"avg age\"),\n        pl.len().alias(\"#\"),\n    )\n    .sort(\"#\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\", \"gender\"])\n    .agg([compute_age().mean().alias(\"avg birthday\"), len().alias(\"#\")])\n    .sort(\n        [\"#\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 gender \u2506 avg age    \u2506 #    \u2502\n\u2502 ---   \u2506 ---    \u2506 ---        \u2506 ---  \u2502\n\u2502 cat   \u2506 cat    \u2506 f64        \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NY    \u2506 M      \u2506 165.204634 \u2506 3965 \u2502\n\u2502 PA    \u2506 M      \u2506 167.008592 \u2506 3205 \u2502\n\u2502 OH    \u2506 M      \u2506 157.579961 \u2506 2142 \u2502\n\u2502 IL    \u2506 M      \u2506 146.069482 \u2506 1895 \u2502\n\u2502 CA    \u2506 M      \u2506 115.400464 \u2506 1725 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that we get the same results but the format of the data is different. Depending on the situation, one format may be more suitable than the other.</p>"},{"location":"user-guide/expressions/aggregation/#sorting","title":"Sorting","text":"<p>It is common to see a dataframe being sorted for the sole purpose of managing the ordering during a grouping operation. Let's say that we want to get the names of the oldest and youngest politicians per state. We could start by sorting and then grouping:</p>  Python Rust <p> <code>group_by</code> <pre><code>def get_name() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>fn get_name() -&gt; Expr {\n    col(\"first_name\") + lit(\" \") + col(\"last_name\")\n}\n\nlet df = dataset\n    .clone()\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest              \u2506 oldest          \u2502\n\u2502 ---   \u2506 ---                   \u2506 ---             \u2502\n\u2502 cat   \u2506 str                   \u2506 str             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 SC    \u2506 Ralph Izard           \u2506 Thomas Sumter   \u2502\n\u2502 NJ    \u2506 Lambert Cadwalader    \u2506 Abraham Clark   \u2502\n\u2502 NY    \u2506 Cornelius Schoonmaker \u2506 Philip Schuyler \u2502\n\u2502 GA    \u2506 Thomas Carnes         \u2506 George Mathews  \u2502\n\u2502 IL    \u2506 Benjamin Stephenson   \u2506 Shadrack Bond   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, if we also want to sort the names alphabetically, we need to perform an extra sort operation. Luckily, we can sort in a <code>group_by</code> context without changing the sorting of the underlying dataframe:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n        get_name().sort().first().alias(\"alphabetical_first\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n        get_name()\n            .sort(Default::default())\n            .first()\n            .alias(\"alphabetical_first\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest        \u2506 oldest               \u2506 alphabetical_first \u2502\n\u2502 ---   \u2506 ---             \u2506 ---                  \u2506 ---                \u2502\n\u2502 cat   \u2506 str             \u2506 str                  \u2506 str                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 DK    \u2506 George Mathews  \u2506 John Todd            \u2506 George Mathews     \u2502\n\u2502 DE    \u2506 Samuel White    \u2506 George Read          \u2506 Albert Polk        \u2502\n\u2502 CO    \u2506 Yadira Caraveo  \u2506 Allen Bradford       \u2506 Allen Bradford     \u2502\n\u2502 NE    \u2506 Samuel Daily    \u2506 Experience Estabrook \u2506 Albert Jefferis    \u2502\n\u2502 FL    \u2506 Charles Downing \u2506 Joseph White         \u2506 Abijah Gilbert     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can even sort a column with the order induced by another column, and this also works inside the context <code>group_by</code>. This modification to the previous query lets us check if the delegate with the first name is male or female:</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n        get_name().sort().first().alias(\"alphabetical_first\"),\n        pl.col(\"gender\").sort_by(get_name()).first(),\n    )\n    .sort(\"state\")\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_name().first().alias(\"youngest\"),\n        get_name().last().alias(\"oldest\"),\n        get_name()\n            .sort(Default::default())\n            .first()\n            .alias(\"alphabetical_first\"),\n        col(\"gender\")\n            .sort_by([\"first_name\"], SortMultipleOptions::default())\n            .first(),\n    ])\n    .sort([\"state\"], SortMultipleOptions::default())\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest         \u2506 oldest         \u2506 alphabetical_first \u2506 gender \u2502\n\u2502 ---   \u2506 ---              \u2506 ---            \u2506 ---                \u2506 ---    \u2502\n\u2502 cat   \u2506 str              \u2506 str            \u2506 str                \u2506 cat    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AK    \u2506 Mary Peltola     \u2506 Thomas Cale    \u2506 Anthony Dimond     \u2506 M      \u2502\n\u2502 AL    \u2506 John McKee       \u2506 Israel Pickens \u2506 Albert Goodwyn     \u2506 M      \u2502\n\u2502 AR    \u2506 Archibald Yell   \u2506 James Bates    \u2506 Albert Rust        \u2506 M      \u2502\n\u2502 AS    \u2506 Eni Faleomavaega \u2506 Fof\u00f3 Sunia     \u2506 Eni Faleomavaega   \u2506 M      \u2502\n\u2502 AZ    \u2506 Ben Quayle       \u2506 Coles Bashford \u2506 Ann Kirkpatrick    \u2506 F      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#do-not-kill-parallelization","title":"Do not kill parallelization","text":"<p>Python users only</p> <p>The following section is specific to Python, and doesn't apply to Rust. Within Rust, blocks and closures (lambdas) can, and will, be executed concurrently.</p> <p>Python is generally slower than Rust. Besides the overhead of running \u201cslow\u201d bytecode, Python has to remain within the constraints of the Global Interpreter Lock (GIL). This means that if you were to use a <code>lambda</code> or a custom Python function to apply during a parallelized phase, Polars' speed is capped running Python code, preventing any multiple threads from executing the function.</p> <p>Polars will try to parallelize the computation of the aggregating functions over the groups, so it is recommended that you avoid using <code>lambda</code>s and custom Python functions as much as possible. Instead, try to stay within the realm of the Polars expression API. This is not always possible, though, so if you want to learn more about using <code>lambda</code>s you can go the user guide section on using user-defined functions.</p>"},{"location":"user-guide/expressions/basic-operations/","title":"Basic operations","text":"<p>This section shows how to do basic operations on dataframe columns, like do basic arithmetic calculations, perform comparisons, and other general-purpose operations. We will use the following dataframe for the examples that follow:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\nimport numpy as np\n\nnp.random.seed(42)  # For reproducibility.\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"A\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use polars::prelude::*;\n\nlet df = df! (\n    \"nrs\" =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],\n    \"names\" =&gt; &amp;[\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n    \"random\" =&gt; &amp;[0.37454, 0.950714, 0.731994, 0.598658, 0.156019],\n    \"groups\" =&gt; &amp;[\"A\", \"A\", \"B\", \"A\", \"B\"],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.37454  \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.950714 \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.731994 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.598658 \u2506 A      \u2502\n\u2502 5    \u2506 spam  \u2506 0.156019 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/basic-operations/#basic-arithmetic","title":"Basic arithmetic","text":"<p>Polars supports basic arithmetic between series of the same length, or between series and literals. When literals are mixed with series, the literals are broadcast to match the length of the series they are being used with.</p>  Python Rust <p> <code>operators</code> <pre><code>result = df.select(\n    (pl.col(\"nrs\") + 5).alias(\"nrs + 5\"),\n    (pl.col(\"nrs\") - 5).alias(\"nrs - 5\"),\n    (pl.col(\"nrs\") * pl.col(\"random\")).alias(\"nrs * random\"),\n    (pl.col(\"nrs\") / pl.col(\"random\")).alias(\"nrs / random\"),\n    (pl.col(\"nrs\") ** 2).alias(\"nrs ** 2\"),\n    (pl.col(\"nrs\") % 3).alias(\"nrs % 3\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>operators</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        (col(\"nrs\") + lit(5)).alias(\"nrs + 5\"),\n        (col(\"nrs\") - lit(5)).alias(\"nrs - 5\"),\n        (col(\"nrs\") * col(\"random\")).alias(\"nrs * random\"),\n        (col(\"nrs\") / col(\"random\")).alias(\"nrs / random\"),\n        (col(\"nrs\").pow(lit(2))).alias(\"nrs ** 2\"),\n        (col(\"nrs\") % lit(3)).alias(\"nrs % 3\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs + 5 \u2506 nrs - 5 \u2506 nrs * random \u2506 nrs / random \u2506 nrs ** 2 \u2506 nrs % 3 \u2502\n\u2502 ---     \u2506 ---     \u2506 ---          \u2506 ---          \u2506 ---      \u2506 ---     \u2502\n\u2502 i64     \u2506 i64     \u2506 f64          \u2506 f64          \u2506 i64      \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 -4      \u2506 0.37454      \u2506 2.669941     \u2506 1        \u2506 1       \u2502\n\u2502 7       \u2506 -3      \u2506 1.901429     \u2506 2.103681     \u2506 4        \u2506 2       \u2502\n\u2502 8       \u2506 -2      \u2506 2.195982     \u2506 4.098395     \u2506 9        \u2506 0       \u2502\n\u2502 null    \u2506 null    \u2506 null         \u2506 null         \u2506 null     \u2506 null    \u2502\n\u2502 10      \u2506 0       \u2506 0.780093     \u2506 32.047453    \u2506 25       \u2506 2       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The example above shows that when an arithmetic operation takes <code>null</code> as one of its operands, the result is <code>null</code>.</p> <p>Polars uses operator overloading to allow you to use your language's native arithmetic operators within your expressions. If you prefer, in Python you can use the corresponding named functions, as the snippet below demonstrates:</p> <pre><code># Python only:\nresult_named_operators = df.select(\n    (pl.col(\"nrs\").add(5)).alias(\"nrs + 5\"),\n    (pl.col(\"nrs\").sub(5)).alias(\"nrs - 5\"),\n    (pl.col(\"nrs\").mul(pl.col(\"random\"))).alias(\"nrs * random\"),\n    (pl.col(\"nrs\").truediv(pl.col(\"random\"))).alias(\"nrs / random\"),\n    (pl.col(\"nrs\").pow(2)).alias(\"nrs ** 2\"),\n    (pl.col(\"nrs\").mod(3)).alias(\"nrs % 3\"),\n)\n\nprint(result.equals(result_named_operators))\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"user-guide/expressions/basic-operations/#comparisons","title":"Comparisons","text":"<p>Like with arithmetic operations, Polars supports comparisons via the overloaded operators or named functions:</p>  Python Rust <p> <code>operators</code> <pre><code>result = df.select(\n    (pl.col(\"nrs\") &gt; 1).alias(\"nrs &gt; 1\"),  # .gt\n    (pl.col(\"nrs\") &gt;= 3).alias(\"nrs &gt;= 3\"),  # ge\n    (pl.col(\"random\") &lt; 0.2).alias(\"random &lt; .2\"),  # .lt\n    (pl.col(\"random\") &lt;= 0.5).alias(\"random &lt;= .5\"),  # .le\n    (pl.col(\"nrs\") != 1).alias(\"nrs != 1\"),  # .ne\n    (pl.col(\"nrs\") == 1).alias(\"nrs == 1\"),  # .eq\n)\nprint(result)\n</code></pre></p> <p> <code>operators</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"nrs\").gt(1).alias(\"nrs &gt; 1\"),\n        col(\"nrs\").gt_eq(3).alias(\"nrs &gt;= 3\"),\n        col(\"random\").lt_eq(0.2).alias(\"random &lt; .2\"),\n        col(\"random\").lt_eq(0.5).alias(\"random &lt;= .5\"),\n        col(\"nrs\").neq(1).alias(\"nrs != 1\"),\n        col(\"nrs\").eq(1).alias(\"nrs == 1\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs &gt; 1 \u2506 nrs &gt;= 3 \u2506 random &lt; .2 \u2506 random &lt;= .5 \u2506 nrs != 1 \u2506 nrs == 1 \u2502\n\u2502 ---     \u2506 ---      \u2506 ---         \u2506 ---          \u2506 ---      \u2506 ---      \u2502\n\u2502 bool    \u2506 bool     \u2506 bool        \u2506 bool         \u2506 bool     \u2506 bool     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false   \u2506 false    \u2506 false       \u2506 true         \u2506 false    \u2506 true     \u2502\n\u2502 true    \u2506 false    \u2506 false       \u2506 false        \u2506 true     \u2506 false    \u2502\n\u2502 true    \u2506 true     \u2506 false       \u2506 false        \u2506 true     \u2506 false    \u2502\n\u2502 null    \u2506 null     \u2506 false       \u2506 false        \u2506 null     \u2506 null     \u2502\n\u2502 true    \u2506 true     \u2506 true        \u2506 true         \u2506 true     \u2506 false    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/basic-operations/#boolean-and-bitwise-operations","title":"Boolean and bitwise operations","text":"<p>Depending on the language, you may use the operators <code>&amp;</code>, <code>|</code>, and <code>~</code>, for the Boolean operations \u201cand\u201d, \u201cor\u201d, and \u201cnot\u201d, respectively, or the functions of the same name:</p>  Python Rust <p> <code>operators</code> <pre><code># Boolean operators &amp; | ~\nresult = df.select(\n    ((~pl.col(\"nrs\").is_null()) &amp; (pl.col(\"groups\") == \"A\")).alias(\n        \"number not null and group A\"\n    ),\n    ((pl.col(\"random\") &lt; 0.5) | (pl.col(\"groups\") == \"B\")).alias(\n        \"random &lt; 0.5 or group B\"\n    ),\n)\n\nprint(result)\n\n# Corresponding named functions `and_`, `or_`, and `not_`.\nresult2 = df.select(\n    (pl.col(\"nrs\").is_null().not_().and_(pl.col(\"groups\") == \"A\")).alias(\n        \"number not null and group A\"\n    ),\n    ((pl.col(\"random\") &lt; 0.5).or_(pl.col(\"groups\") == \"B\")).alias(\n        \"random &lt; 0.5 or group B\"\n    ),\n)\nprint(result.equals(result2))\n</code></pre></p> <p> <code>operators</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        ((col(\"nrs\").is_null()).not().and(col(\"groups\").eq(lit(\"A\"))))\n            .alias(\"number not null and group A\"),\n        (col(\"random\").lt(lit(0.5)).or(col(\"groups\").eq(lit(\"B\"))))\n            .alias(\"random &lt; 0.5 or group B\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 number not null and group A \u2506 random &lt; 0.5 or group B \u2502\n\u2502 ---                         \u2506 ---                     \u2502\n\u2502 bool                        \u2506 bool                    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 true                        \u2506 true                    \u2502\n\u2502 true                        \u2506 false                   \u2502\n\u2502 false                       \u2506 true                    \u2502\n\u2502 false                       \u2506 false                   \u2502\n\u2502 false                       \u2506 true                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nTrue\n</code></pre> Python trivia <p>The Python functions are called <code>and_</code>, <code>or_</code>, and <code>not_</code>, because the words <code>and</code>, <code>or</code>, and <code>not</code> are reserved keywords in Python. Similarly, we cannot use the keywords <code>and</code>, <code>or</code>, and <code>not</code>, as the Boolean operators because these Python keywords will interpret their operands in the context of Truthy and Falsy through the dunder method <code>__bool__</code>. Thus, we overload the bitwise operators <code>&amp;</code>, <code>|</code>, and <code>~</code>, as the Boolean operators because they are the second best choice.</p> <p>These operators/functions can also be used for the respective bitwise operations, alongside the bitwise operator <code>^</code> / function <code>xor</code>:</p>  Python Rust <pre><code>result = df.select(\n    pl.col(\"nrs\"),\n    (pl.col(\"nrs\") &amp; 6).alias(\"nrs &amp; 6\"),\n    (pl.col(\"nrs\") | 6).alias(\"nrs | 6\"),\n    (~pl.col(\"nrs\")).alias(\"not nrs\"),\n    (pl.col(\"nrs\") ^ 6).alias(\"nrs ^ 6\"),\n)\n\nprint(result)\n</code></pre> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"nrs\"),\n        col(\"nrs\").and(lit(6)).alias(\"nrs &amp; 6\"),\n        col(\"nrs\").or(lit(6)).alias(\"nrs | 6\"),\n        col(\"nrs\").not().alias(\"not nrs\"),\n        col(\"nrs\").xor(lit(6)).alias(\"nrs ^ 6\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 nrs &amp; 6 \u2506 nrs | 6 \u2506 not nrs \u2506 nrs ^ 6 \u2502\n\u2502 ---  \u2506 ---     \u2506 ---     \u2506 ---     \u2506 ---     \u2502\n\u2502 i64  \u2506 i64     \u2506 i64     \u2506 i64     \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 0       \u2506 7       \u2506 -2      \u2506 7       \u2502\n\u2502 2    \u2506 2       \u2506 6       \u2506 -3      \u2506 4       \u2502\n\u2502 3    \u2506 2       \u2506 7       \u2506 -4      \u2506 5       \u2502\n\u2502 null \u2506 null    \u2506 null    \u2506 null    \u2506 null    \u2502\n\u2502 5    \u2506 4       \u2506 7       \u2506 -6      \u2506 3       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/basic-operations/#counting-unique-values","title":"Counting (unique) values","text":"<p>Polars has two functions to count the number of unique values in a series. The function <code>n_unique</code> can be used to count the exact number of unique values in a series. However, for very large data sets, this operation can be quite slow. In those cases, if an approximation is good enough, you can use the function <code>approx_n_unique</code> that uses the algorithm HyperLogLog++ to estimate the result.</p> <p>The example below shows an example series where the <code>approx_n_unique</code> estimation is wrong by 0.9%:</p>  Python Rust <p> <code>n_unique</code> \u00b7 <code>approx_n_unique</code> <pre><code>long_df = pl.DataFrame({\"numbers\": np.random.randint(0, 100_000, 100_000)})\n\nresult = long_df.select(\n    pl.col(\"numbers\").n_unique().alias(\"n_unique\"),\n    pl.col(\"numbers\").approx_n_unique().alias(\"approx_n_unique\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>n_unique</code> \u00b7 <code>approx_n_unique</code> \u00b7  Available on feature approx_unique <pre><code>use rand::distr::{Distribution, Uniform};\nuse rand::rng;\n\nlet mut rng = rng();\nlet between = Uniform::new_inclusive(0, 100_000).unwrap();\nlet arr: Vec&lt;u32&gt; = between.sample_iter(&amp;mut rng).take(100_100).collect();\n\nlet long_df = df!(\n    \"numbers\" =&gt; &amp;arr\n)?;\n\nlet result = long_df\n    .lazy()\n    .select([\n        col(\"numbers\").n_unique().alias(\"n_unique\"),\n        col(\"numbers\").approx_n_unique().alias(\"approx_n_unique\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 n_unique \u2506 approx_n_unique \u2502\n\u2502 ---      \u2506 ---             \u2502\n\u2502 u32      \u2506 u32             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 63218    \u2506 62649           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can get more information about the unique values and their counts with the function <code>value_counts</code>, that Polars also provides:</p>  Python Rust <p> <code>value_counts</code> <pre><code>result = df.select(\n    pl.col(\"names\").value_counts().alias(\"value_counts\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>value_counts</code> \u00b7  Available on feature dtype-struct <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([col(\"names\")\n        .value_counts(false, false, \"count\", false)\n        .alias(\"value_counts\")])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value_counts \u2502\n\u2502 ---          \u2502\n\u2502 struct[2]    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 {\"egg\",1}    \u2502\n\u2502 {\"foo\",1}    \u2502\n\u2502 {\"spam\",2}   \u2502\n\u2502 {\"ham\",1}    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The function <code>value_counts</code> returns the results in structs, a data type that we will explore in a later section.</p> <p>Alternatively, if you only need a series with the unique values or a series with the unique counts, they are one function away:</p>  Python Rust <p> <code>unique</code> \u00b7 <code>unique_counts</code> <pre><code>result = df.select(\n    pl.col(\"names\").unique(maintain_order=True).alias(\"unique\"),\n    pl.col(\"names\").unique_counts().alias(\"unique_counts\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>unique</code> \u00b7 <code>unique_counts</code> \u00b7  Available on feature unique_counts <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"names\").unique_stable().alias(\"unique\"),\n        col(\"names\").unique_counts().alias(\"unique_counts\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unique \u2506 unique_counts \u2502\n\u2502 ---    \u2506 ---           \u2502\n\u2502 str    \u2506 u32           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 foo    \u2506 1             \u2502\n\u2502 ham    \u2506 1             \u2502\n\u2502 spam   \u2506 2             \u2502\n\u2502 egg    \u2506 1             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that we need to specify <code>maintain_order=True</code> in the function <code>unique</code> so that the order of the results is consistent with the order of the results in <code>unique_counts</code>. See the API reference for more information.</p>"},{"location":"user-guide/expressions/basic-operations/#conditionals","title":"Conditionals","text":"<p>Polars supports something akin to a ternary operator through the function <code>when</code>, which is followed by one function <code>then</code> and an optional function <code>otherwise</code>.</p> <p>The function <code>when</code> accepts a predicate expression. The values that evaluate to <code>True</code> are replaced by the corresponding values of the expression inside the function <code>then</code>. The values that evaluate to <code>False</code> are replaced by the corresponding values of the expression inside the function <code>otherwise</code> or <code>null</code>, if <code>otherwise</code> is not provided.</p> <p>The example below applies one step of the Collatz conjecture to the numbers in the column \u201cnrs\u201d:</p>  Python Rust <p> <code>when</code> <pre><code>result = df.select(\n    pl.col(\"nrs\"),\n    pl.when(pl.col(\"nrs\") % 2 == 1)  # Is the number odd?\n    .then(3 * pl.col(\"nrs\") + 1)  # If so, multiply by 3 and add 1.\n    .otherwise(pl.col(\"nrs\") // 2)  # If not, divide by 2.\n    .alias(\"Collatz\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>when</code> <pre><code>let result = df\n    .lazy()\n    .select([\n        col(\"nrs\"),\n        when((col(\"nrs\") % lit(2)).eq(lit(1)))\n            .then(lit(3) * col(\"nrs\") + lit(1))\n            .otherwise(col(\"nrs\") / lit(2))\n            .alias(\"Collatz\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 Collatz \u2502\n\u2502 ---  \u2506 ---     \u2502\n\u2502 i64  \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 4       \u2502\n\u2502 2    \u2506 1       \u2502\n\u2502 3    \u2506 10      \u2502\n\u2502 null \u2506 null    \u2502\n\u2502 5    \u2506 16      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can also emulate a chain of an arbitrary number of conditionals, akin to Python's <code>elif</code> statement, by chaining an arbitrary number of consecutive blocks of <code>.when(...).then(...)</code>. In those cases, and for each given value, Polars will only consider a replacement expression that is deeper within the chain if the previous predicates all failed for that value.</p>"},{"location":"user-guide/expressions/casting/","title":"Casting","text":"<p>Casting converts the underlying data type of a column to a new one. Casting is available through the function <code>cast</code>.</p> <p>The function <code>cast</code> includes a parameter <code>strict</code> that determines how Polars behaves when it encounters a value that cannot be converted from the source data type to the target data type. The default behaviour is <code>strict=True</code>, which means that Polars will thrown an error to notify the user of the failed conversion while also providing details on the values that couldn't be cast. On the other hand, if <code>strict=False</code>, any values that cannot be converted to the target data type will be quietly converted to <code>null</code>.</p>"},{"location":"user-guide/expressions/casting/#basic-example","title":"Basic example","text":"<p>Let's take a look at the following dataframe which contains both integers and floating point numbers:</p>  Python Rust <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"integers\": [1, 2, 3],\n        \"big_integers\": [10000002, 2, 30000003],\n        \"floats\": [4.0, 5.8, -6.3],\n    }\n)\n\nprint(df)\n</code></pre> <pre><code>use polars::prelude::*;\n\nlet df = df! (\n    \"integers\"=&gt; [1, 2, 3],\n    \"big_integers\"=&gt; [10000002, 2, 30000003],\n    \"floats\"=&gt; [4.0, 5.8, -6.3],\n)?;\n\nprintln!(\"{df}\");\n</code></pre> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 big_integers \u2506 floats \u2502\n\u2502 ---      \u2506 ---          \u2506 ---    \u2502\n\u2502 i64      \u2506 i64          \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 10000002     \u2506 4.0    \u2502\n\u2502 2        \u2506 2            \u2506 5.8    \u2502\n\u2502 3        \u2506 30000003     \u2506 -6.3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To perform casting operations between floats and integers, or vice versa, we use the function <code>cast</code>:</p>  Python Rust <p> <code>cast</code> <pre><code>result = df.select(\n    pl.col(\"integers\").cast(pl.Float32).alias(\"integers_as_floats\"),\n    pl.col(\"floats\").cast(pl.Int32).alias(\"floats_as_integers\"),\n)\nprint(result)\n</code></pre></p> <p> <code>cast</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"integers\")\n            .cast(DataType::Float32)\n            .alias(\"integers_as_floats\"),\n        col(\"floats\")\n            .cast(DataType::Int32)\n            .alias(\"floats_as_integers\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_as_floats \u2506 floats_as_integers \u2502\n\u2502 ---                \u2506 ---                \u2502\n\u2502 f32                \u2506 i32                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0                \u2506 4                  \u2502\n\u2502 2.0                \u2506 5                  \u2502\n\u2502 3.0                \u2506 -6                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that floating point numbers are truncated when casting to an integer data type.</p>"},{"location":"user-guide/expressions/casting/#downcasting-numerical-data-types","title":"Downcasting numerical data types","text":"<p>You can reduce the memory footprint of a column by changing the precision associated with its numeric data type. As an illustration, the code below demonstrates how casting from <code>Int64</code> to <code>Int16</code> and from <code>Float64</code> to <code>Float32</code> can be used to lower memory usage:</p>  Python Rust <p> <code>cast</code> \u00b7 <code>estimated_size</code> <pre><code>print(f\"Before downcasting: {df.estimated_size()} bytes\")\nresult = df.with_columns(\n    pl.col(\"integers\").cast(pl.Int16),\n    pl.col(\"floats\").cast(pl.Float32),\n)\nprint(f\"After downcasting: {result.estimated_size()} bytes\")\n</code></pre></p> <p> <code>cast</code> \u00b7 <code>estimated_size</code> <pre><code>println!(\"Before downcasting: {} bytes\", df.estimated_size());\nlet result = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"integers\").cast(DataType::Int16),\n        col(\"floats\").cast(DataType::Float32),\n    ])\n    .collect()?;\nprintln!(\"After downcasting: {} bytes\", result.estimated_size());\n</code></pre></p> <pre><code>Before downcasting: 72 bytes\nAfter downcasting: 42 bytes\n</code></pre> <p>When performing downcasting it is crucial to ensure that the chosen number of bits (such as 64, 32, or 16) is sufficient to accommodate the largest and smallest numbers in the column. For example, a 32-bit signed integer (<code>Int32</code>) represents integers between -2147483648 and 2147483647, inclusive, while an 8-bit signed integer only represents integers between -128 and 127, inclusive. Attempting to downcast to a data type with insufficient precision results in an error thrown by Polars:</p>  Python Rust <p> <code>cast</code> <pre><code>from polars.exceptions import InvalidOperationError\n\ntry:\n    result = df.select(pl.col(\"big_integers\").cast(pl.Int8))\n    print(result)\nexcept InvalidOperationError as err:\n    print(err)\n</code></pre></p> <p> <code>cast</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([col(\"big_integers\").strict_cast(DataType::Int8)])\n    .collect();\nif let Err(e) = result {\n    println!(\"{e}\")\n};\n</code></pre></p> <pre><code>conversion from `i64` to `i8` failed in column 'big_integers' for 2 out of 3 values: [10000002, 30000003]\n</code></pre> <p>If you set the parameter <code>strict</code> to <code>False</code> the overflowing/underflowing values are converted to <code>null</code>:</p>  Python Rust <p> <code>cast</code> <pre><code>result = df.select(pl.col(\"big_integers\").cast(pl.Int8, strict=False))\nprint(result)\n</code></pre></p> <p> <code>cast</code> <pre><code>let result = df\n    .lazy()\n    .select([col(\"big_integers\").cast(DataType::Int8)])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 big_integers \u2502\n\u2502 ---          \u2502\n\u2502 i8           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null         \u2502\n\u2502 2            \u2502\n\u2502 null         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#converting-strings-to-numeric-data-types","title":"Converting strings to numeric data types","text":"<p>Strings that represent numbers can be converted to the appropriate data types via casting. The opposite conversion is also possible:</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers_as_strings\": [\"1\", \"2\", \"3\"],\n        \"floats_as_strings\": [\"4.0\", \"5.8\", \"-6.3\"],\n        \"floats\": [4.0, 5.8, -6.3],\n    }\n)\n\nresult = df.select(\n    pl.col(\"integers_as_strings\").cast(pl.Int32),\n    pl.col(\"floats_as_strings\").cast(pl.Float64),\n    pl.col(\"floats\").cast(pl.String),\n)\nprint(result)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\n    \"integers_as_strings\" =&gt; [\"1\", \"2\", \"3\"],\n    \"floats_as_strings\" =&gt; [\"4.0\", \"5.8\", \"-6.3\"],\n    \"floats\" =&gt; [4.0, 5.8, -6.3],\n)?;\n\nlet result = df\n    .lazy()\n    .select([\n        col(\"integers_as_strings\").cast(DataType::Int32),\n        col(\"floats_as_strings\").cast(DataType::Float64),\n        col(\"floats\").cast(DataType::String),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_as_strings \u2506 floats_as_strings \u2506 floats \u2502\n\u2502 ---                 \u2506 ---               \u2506 ---    \u2502\n\u2502 i32                 \u2506 f64               \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1                   \u2506 4.0               \u2506 4.0    \u2502\n\u2502 2                   \u2506 5.8               \u2506 5.8    \u2502\n\u2502 3                   \u2506 -6.3              \u2506 -6.3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In case the column contains a non-numerical value, or a poorly formatted one, Polars will throw an error with details on the conversion error. You can set <code>strict=False</code> to circumvent the error and get a <code>null</code> value instead.</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"floats\": [\"4.0\", \"5.8\", \"- 6 . 3\"],\n    }\n)\ntry:\n    result = df.select(pl.col(\"floats\").cast(pl.Float64))\nexcept InvalidOperationError as err:\n    print(err)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\"floats\" =&gt; [\"4.0\", \"5.8\", \"- 6 . 3\"])?;\n\nlet result = df\n    .lazy()\n    .select([col(\"floats\").strict_cast(DataType::Float64)])\n    .collect();\nif let Err(e) = result {\n    println!(\"{e}\")\n};\n</code></pre></p> <pre><code>conversion from `str` to `f64` failed in column 'floats' for 1 out of 3 values: [\"- 6 . 3\"]\n</code></pre>"},{"location":"user-guide/expressions/casting/#booleans","title":"Booleans","text":"<p>Booleans can be expressed as either 1 (<code>True</code>) or 0 (<code>False</code>). It's possible to perform casting operations between a numerical data type and a Boolean, and vice versa.</p> <p>When converting numbers to Booleans, the number 0 is converted to <code>False</code> and all other numbers are converted to <code>True</code>, in alignment with Python's Truthy and Falsy values for numbers:</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [-1, 0, 2, 3, 4],\n        \"floats\": [0.0, 1.0, 2.0, 3.0, 4.0],\n        \"bools\": [True, False, True, False, True],\n    }\n)\n\nresult = df.select(\n    pl.col(\"integers\").cast(pl.Boolean),\n    pl.col(\"floats\").cast(pl.Boolean),\n    pl.col(\"bools\").cast(pl.Int8),\n)\nprint(result)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\n        \"integers\"=&gt; [-1, 0, 2, 3, 4],\n        \"floats\"=&gt; [0.0, 1.0, 2.0, 3.0, 4.0],\n        \"bools\"=&gt; [true, false, true, false, true],\n)?;\n\nlet result = df\n    .lazy()\n    .select([\n        col(\"integers\").cast(DataType::Boolean),\n        col(\"floats\").cast(DataType::Boolean),\n        col(\"bools\").cast(DataType::UInt8),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 floats \u2506 bools \u2502\n\u2502 ---      \u2506 ---    \u2506 ---   \u2502\n\u2502 bool     \u2506 bool   \u2506 i8    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 true     \u2506 false  \u2506 1     \u2502\n\u2502 false    \u2506 true   \u2506 0     \u2502\n\u2502 true     \u2506 true   \u2506 1     \u2502\n\u2502 true     \u2506 true   \u2506 0     \u2502\n\u2502 true     \u2506 true   \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#parsing-formatting-temporal-data-types","title":"Parsing / formatting temporal data types","text":"<p>All temporal data types are represented internally as the number of time units elapsed since a reference moment, usually referred to as the epoch. For example, values of the data type <code>Date</code> are stored as the number of days since the epoch. For the data type <code>Datetime</code> the time unit is the microsecond (us) and for <code>Time</code> the time unit is the nanosecond (ns).</p> <p>Casting between numerical types and temporal data types is allowed and exposes this relationship:</p>  Python Rust <p> <code>cast</code> <pre><code>from datetime import date, datetime, time\n\ndf = pl.DataFrame(\n    {\n        \"date\": [\n            date(1970, 1, 1),  # epoch\n            date(1970, 1, 10),  # 9 days later\n        ],\n        \"datetime\": [\n            datetime(1970, 1, 1, 0, 0, 0),  # epoch\n            datetime(1970, 1, 1, 0, 1, 0),  # 1 minute later\n        ],\n        \"time\": [\n            time(0, 0, 0),  # reference time\n            time(0, 0, 1),  # 1 second later\n        ],\n    }\n)\n\nresult = df.select(\n    pl.col(\"date\").cast(pl.Int64).alias(\"days_since_epoch\"),\n    pl.col(\"datetime\").cast(pl.Int64).alias(\"us_since_epoch\"),\n    pl.col(\"time\").cast(pl.Int64).alias(\"ns_since_midnight\"),\n)\nprint(result)\n</code></pre></p> <p> <code>cast</code> <pre><code>use chrono::prelude::*;\n\nlet df = df!(\n    \"date\" =&gt; [\n        NaiveDate::from_ymd_opt(1970, 1, 1).unwrap(),  // epoch\n        NaiveDate::from_ymd_opt(1970, 1, 10).unwrap(),  // 9 days later\n    ],\n    \"datetime\" =&gt; [\n        NaiveDate::from_ymd_opt(1970, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),  // epoch\n        NaiveDate::from_ymd_opt(1970, 1, 1).unwrap().and_hms_opt(0, 1, 0).unwrap(),  // 1 minute later\n    ],\n    \"time\" =&gt; [\n        NaiveTime::from_hms_opt(0, 0, 0).unwrap(),  // reference time\n        NaiveTime::from_hms_opt(0, 0, 1).unwrap(),  // 1 second later\n    ]\n)\n.unwrap()\n.lazy()\n// Make the time unit match that of Python's for the same results.\n.with_column(col(\"datetime\").cast(DataType::Datetime(TimeUnit::Microseconds, None)))\n.collect()?;\n\nlet result = df\n    .lazy()\n    .select([\n        col(\"date\").cast(DataType::Int64).alias(\"days_since_epoch\"),\n        col(\"datetime\")\n            .cast(DataType::Int64)\n            .alias(\"us_since_epoch\"),\n        col(\"time\").cast(DataType::Int64).alias(\"ns_since_midnight\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 days_since_epoch \u2506 us_since_epoch \u2506 ns_since_midnight \u2502\n\u2502 ---              \u2506 ---            \u2506 ---               \u2502\n\u2502 i64              \u2506 i64            \u2506 i64               \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0                \u2506 0              \u2506 0                 \u2502\n\u2502 9                \u2506 60000000       \u2506 1000000000        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To format temporal data types as strings we can use the function <code>dt.to_string</code> and to parse temporal data types from strings we can use the function <code>str.to_datetime</code>. Both functions adopt the chrono format syntax for formatting.</p>  Python Rust <p> <code>dt.to_string</code> \u00b7 <code>str.to_date</code> <pre><code>df = pl.DataFrame(\n    {\n        \"date\": [date(2022, 1, 1), date(2022, 1, 2)],\n        \"string\": [\"2022-01-01\", \"2022-01-02\"],\n    }\n)\n\nresult = df.select(\n    pl.col(\"date\").dt.to_string(\"%Y-%m-%d\"),\n    pl.col(\"string\").str.to_datetime(\"%Y-%m-%d\"),\n)\nprint(result)\n</code></pre></p> <p> <code>dt.to_string</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature temporal \u00b7  Available on feature dtype-date <pre><code>let df = df! (\n        \"date\" =&gt; [\n            NaiveDate::from_ymd_opt(2022, 1, 1).unwrap(),\n            NaiveDate::from_ymd_opt(2022, 1, 2).unwrap(),\n        ],\n        \"string\" =&gt; [\n            \"2022-01-01\",\n            \"2022-01-02\",\n        ],\n)?;\n\nlet result = df\n    .lazy()\n    .select([\n        col(\"date\").dt().to_string(\"%Y-%m-%d\"),\n        col(\"string\").str().to_datetime(\n            Some(TimeUnit::Microseconds),\n            None,\n            StrptimeOptions::default(),\n            lit(\"raise\"),\n        ),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 string              \u2502\n\u2502 ---        \u2506 ---                 \u2502\n\u2502 str        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2022-01-01 \u2506 2022-01-01 00:00:00 \u2502\n\u2502 2022-01-02 \u2506 2022-01-02 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>It's worth noting that <code>str.to_datetime</code> features additional options that support timezone functionality. Refer to the API documentation for further information.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/","title":"Categorical data and enums","text":"<p>A column that holds string values that can only take on one of a limited number of possible values is a column that holds categorical data. Usually, the number of possible values is much smaller than the length of the column. Some typical examples include your nationality, the operating system of your computer, or the license that your favorite open source project uses.</p> <p>When working with categorical data you can use Polars' dedicated types, <code>Categorical</code> and <code>Enum</code>, to make your queries more performant. Now, we will see what are the differences between the two data types <code>Categorical</code> and <code>Enum</code> and when you should use one data type or the other. We also include some notes on why the data types <code>Categorical</code> and <code>Enum</code> are more efficient than using the plain string values in the end of this user guide section.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#enum-vs-categorical","title":"<code>Enum</code> vs <code>Categorical</code>","text":"<p>In short, you should prefer <code>Enum</code> over <code>Categorical</code> whenever possible. When the categories are fixed and known up front, use <code>Enum</code>. When you don't know the categories or they are not fixed then you must use <code>Categorical</code>. In case your requirements change along the way you can always cast from one to the other.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#data-type-enum","title":"Data type <code>Enum</code>","text":""},{"location":"user-guide/expressions/categorical-data-and-enums/#creating-an-enum","title":"Creating an <code>Enum</code>","text":"<p>The data type <code>Enum</code> is an ordered categorical data type. To use the data type <code>Enum</code> you have to specify the categories in advance to create a new data type that is a variant of an <code>Enum</code>. Then, when creating a new series, a new dataframe, or when casting a string column, you can use that <code>Enum</code> variant.</p>  Python <p> <code>Enum</code> <pre><code>import polars as pl\n\nbears_enum = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\nbears = pl.Series([\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=bears_enum)\nprint(bears)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [enum]\n[\n    \"Polar\"\n    \"Panda\"\n    \"Brown\"\n    \"Brown\"\n    \"Polar\"\n]\n</code></pre>"},{"location":"user-guide/expressions/categorical-data-and-enums/#invalid-values","title":"Invalid values","text":"<p>Polars will raise an error if you try to specify a data type <code>Enum</code> whose categories do not include all the values present:</p>  Python <p> <code>Enum</code> <pre><code>from polars.exceptions import InvalidOperationError\n\ntry:\n    bears_kind_of = pl.Series(\n        [\"Polar\", \"Panda\", \"Brown\", \"Polar\", \"Shark\"],\n        dtype=bears_enum,\n    )\nexcept InvalidOperationError as exc:\n    print(\"InvalidOperationError:\", exc)\n</code></pre></p> <pre><code>InvalidOperationError: conversion from `str` to `enum` failed in column '' for 1 out of 5 values: [\"Shark\"]\n\nEnsure that all values in the input column are present in the categories of the enum datatype.\n</code></pre> <p>If you are in a position where you cannot know all of the possible values in advance and erroring on unknown values is semantically wrong, you may need to use the data type <code>Categorical</code>.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#category-ordering-and-comparison","title":"Category ordering and comparison","text":"<p>The data type <code>Enum</code> is ordered and the order is induced by the order in which you specify the categories. The example below uses log levels as an example of where an ordered <code>Enum</code> is useful:</p>  Python <p> <code>Enum</code> <pre><code>log_levels = pl.Enum([\"debug\", \"info\", \"warning\", \"error\"])\n\nlogs = pl.DataFrame(\n    {\n        \"level\": [\"debug\", \"info\", \"debug\", \"error\"],\n        \"message\": [\n            \"process id: 525\",\n            \"Service started correctly\",\n            \"startup time: 67ms\",\n            \"Cannot connect to DB!\",\n        ],\n    },\n    schema_overrides={\n        \"level\": log_levels,\n    },\n)\n\nnon_debug_logs = logs.filter(\n    pl.col(\"level\") &gt; \"debug\",\n)\nprint(non_debug_logs)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 level \u2506 message                   \u2502\n\u2502 ---   \u2506 ---                       \u2502\n\u2502 enum  \u2506 str                       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 info  \u2506 Service started correctly \u2502\n\u2502 error \u2506 Cannot connect to DB!     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This example shows that we can compare <code>Enum</code> values with a string, but this only works if the string matches one of the <code>Enum</code> values. If we compared the column \u201clevel\u201d with any string other than <code>\"debug\"</code>, <code>\"info\"</code>, <code>\"warning\"</code>, or <code>\"error\"</code>, Polars would raise an exception.</p> <p>Columns with the data type <code>Enum</code> can also be compared with other columns that have the same data type <code>Enum</code> or columns that hold strings, but only if all the strings are valid <code>Enum</code> values.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#data-type-categorical","title":"Data type <code>Categorical</code>","text":"<p>The data type <code>Categorical</code> can be seen as a more flexible version of <code>Enum</code>.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#creating-a-categorical-series","title":"Creating a <code>Categorical</code> series","text":"<p>To use the data type <code>Categorical</code>, you can cast a column of strings or specify <code>Categorical</code> as the data type of a series or dataframe column:</p>  Python <p> <code>Categorical</code> <pre><code>bears_cat = pl.Series(\n    [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=pl.Categorical\n)\nprint(bears_cat)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [cat]\n[\n    \"Polar\"\n    \"Panda\"\n    \"Brown\"\n    \"Brown\"\n    \"Polar\"\n]\n</code></pre> <p>Having Polars infer the categories for you may sound strictly better than listing the categories beforehand, but this inference comes with a performance cost. That is why, whenever possible, you should use <code>Enum</code>. You can learn more by reading the subsection about the data type <code>Categorical</code> and its encodings.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#lexical-comparison-with-strings","title":"Lexical comparison with strings","text":"<p>When comparing a <code>Categorical</code> column with a string, Polars will perform a lexical comparison:</p>  Python <p> <code>Categorical</code> <pre><code>print(bears_cat &lt; \"Cat\")\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [bool]\n[\n    false\n    false\n    true\n    true\n    false\n]\n</code></pre> <p>You can also compare a column of strings with your <code>Categorical</code> column, and the comparison will also be lexical:</p>  Python <p> <code>Categorical</code> <pre><code>bears_str = pl.Series(\n    [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"],\n)\nprint(bears_cat == bears_str)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [bool]\n[\n    false\n    false\n    true\n    false\n    true\n]\n</code></pre> <p>Although it is possible to compare a string column with a categorical column, it is typically more efficient to compare two categorical columns. We will see how to do that next.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#comparing-categorical-columns-and-the-string-cache","title":"Comparing <code>Categorical</code> columns and the string cache","text":"<p>You are told that comparing columns with the data type <code>Categorical</code> is more efficient than if one of them is a string column. So, you change your code so that the second column is also a categorical column and then you perform your comparison... But Polars raises an exception:</p>  Python <p> <code>Categorical</code> <pre><code>from polars.exceptions import StringCacheMismatchError\n\nbears_cat2 = pl.Series(\n    [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"],\n    dtype=pl.Categorical,\n)\n\ntry:\n    print(bears_cat == bears_cat2)\nexcept StringCacheMismatchError as exc:\n    exc_str = str(exc).splitlines()[0]\n    print(\"StringCacheMismatchError:\", exc_str)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [bool]\n[\n    false\n    false\n    true\n    false\n    true\n]\n</code></pre> <p>By default, the values in columns with the data type <code>Categorical</code> are encoded in the order they are seen in the column, and independently from other columns, which means that Polars cannot compare efficiently two categorical columns that were created independently.</p> <p>Enabling the Polars string cache and creating the columns with the cache enabled fixes this issue:</p>  Python <p> <code>StringCache</code> \u00b7 <code>Categorical</code> <pre><code>with pl.StringCache():\n    bears_cat = pl.Series(\n        [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=pl.Categorical\n    )\n    bears_cat2 = pl.Series(\n        [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n    )\n\nprint(bears_cat == bears_cat2)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [bool]\n[\n    false\n    false\n    true\n    false\n    true\n]\n</code></pre> <p>Note that using the string cache comes at a performance cost.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#combining-categorical-columns","title":"Combining <code>Categorical</code> columns","text":"<p>The string cache is also useful in any operation that combines or mixes two columns with the data type <code>Categorical</code> in any way. An example of this is when concatenating two dataframes vertically:</p>  Python <p> <code>StringCache</code> \u00b7 <code>Categorical</code> <pre><code>import warnings\n\nfrom polars.exceptions import CategoricalRemappingWarning\n\nmale_bears = pl.DataFrame(\n    {\n        \"species\": [\"Polar\", \"Brown\", \"Panda\"],\n        \"weight\": [450, 500, 110],  # kg\n    },\n    schema_overrides={\"species\": pl.Categorical},\n)\nfemale_bears = pl.DataFrame(\n    {\n        \"species\": [\"Brown\", \"Polar\", \"Panda\"],\n        \"weight\": [340, 200, 90],  # kg\n    },\n    schema_overrides={\"species\": pl.Categorical},\n)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=CategoricalRemappingWarning)\n    bears = pl.concat([male_bears, female_bears], how=\"vertical\")\n\nprint(bears)\n</code></pre></p> <pre><code>shape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species \u2506 weight \u2502\n\u2502 ---     \u2506 ---    \u2502\n\u2502 cat     \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Polar   \u2506 450    \u2502\n\u2502 Brown   \u2506 500    \u2502\n\u2502 Panda   \u2506 110    \u2502\n\u2502 Brown   \u2506 340    \u2502\n\u2502 Polar   \u2506 200    \u2502\n\u2502 Panda   \u2506 90     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case, Polars issues a warning complaining about an expensive reenconding that implies taking a performance hit. Polars then suggests using the data type <code>Enum</code> if possible, or using the string cache. To understand the issue with this operation and why Polars raises an error, please read the final section about the performance considerations of using categorical data types.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#comparison-between-categorical-columns-is-lexical","title":"Comparison between <code>Categorical</code> columns is lexical","text":"<p>Since Polars 1.32.0, when comparing two columns with data type <code>Categorical</code>, Polars always performs lexical (alphabetical) comparison between the values. The <code>ordering</code> parameter has been deprecated and is now ignored.</p> <p>Prior to Polars version 1.32.0, when comparing two columns with data type <code>Categorical</code>, Polars does not perform lexical comparison between the values by default. If you want lexical ordering, you need to specify so when creating the column:</p>  Python <p> <code>StringCache</code> \u00b7 <code>Categorical</code> <pre><code>import warnings\n\nwith pl.StringCache():\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n        bears_cat = pl.Series(\n            [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"],\n            dtype=pl.Categorical(ordering=\"lexical\"),\n        )\n    bears_cat2 = pl.Series(\n        [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n    )\n\nprint(bears_cat &gt; bears_cat2)\n</code></pre></p> <p>```python result=\"text\" session=\"expressions/categoricals\" import warnings</p> <p>with pl.StringCache():     with warnings.catch_warnings():         warnings.filterwarnings(\"ignore\", category=DeprecationWarning)</p> <pre><code>    bears_cat = pl.Series(\n        [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"],\n        dtype=pl.Categorical(ordering=\"lexical\"),\n    )\nbears_cat2 = pl.Series(\n    [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n)\n</code></pre> <p>print(bears_cat &gt; bears_cat2) <pre><code>Otherwise, the order is inferred together with the values:\n\n=== \":fontawesome-brands-python: Python\"\n    [:material-api:  `StringCache`](https://docs.pola.rs/api/python/stable/reference/api/polars.StringCache.html) \u00b7[:material-api:  `Categorical`](https://docs.pola.rs/api/python/stable/reference/api/polars.datatypes.Categorical.html)\n    ```python\n    with pl.StringCache():\n        bears_cat = pl.Series(\n            # Polar &lt;  Panda &lt;  Brown\n            [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"],\n            dtype=pl.Categorical,\n        )\n        bears_cat2 = pl.Series(\n            [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n        )\n\n    print(bears_cat &gt; bears_cat2)\n    ```\n\n\n```python\nwith pl.StringCache():\n    bears_cat = pl.Series(\n        # Polar &lt;  Panda &lt;  Brown\n        [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"],\n        dtype=pl.Categorical,\n    )\n    bears_cat2 = pl.Series(\n        [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n    )\n\nprint(bears_cat &gt; bears_cat2)\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [bool]\n[\n    false\n    false\n    false\n    true\n    false\n]\n</code></pre>"},{"location":"user-guide/expressions/categorical-data-and-enums/#performance-considerations-on-categorical-data-types","title":"Performance considerations on categorical data types","text":"<p>This part of the user guide explains</p> <ul> <li>why categorical data types are more performant than the string literals; and</li> <li>why Polars needs a string cache when doing some operations with the data type <code>Categorical</code>.</li> </ul>"},{"location":"user-guide/expressions/categorical-data-and-enums/#encodings","title":"Encodings","text":"<p>Categorical data represents string data where the values in the column have a finite set of values (usually way smaller than the length of the column). Storing these values as plain strings is a waste of memory and performance as we will be repeating the same string over and over again. Additionally, in operations like joins we have to perform expensive string comparisons.</p> <p>Categorical data types like <code>Enum</code> and <code>Categorical</code> let you encode the string values in a cheaper way, establishing a relationship between a cheap encoding value and the original string literal.</p> <p>As an example of a sensible encoding, Polars could choose to represent the finite set of categories as positive integers. With that in mind, the diagram below shows a regular string column and a possible representation of a Polars column with the categorical data type:</p> String Column Categorical Column Series Polar Panda Brown Panda Brown Brown Polar Physical 0 1 2 1 2 2 0 Categories Polar Panda Brown <p>The physical <code>0</code> in this case encodes (or maps) to the value 'Polar', the value <code>1</code> encodes to 'Panda', and the value <code>2</code> to 'Brown'. This encoding has the benefit of only storing the string values once. Additionally, when we perform operations (e.g. sorting, counting) we can work directly on the physical representation which is much faster than the working with string data.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#encodings-for-the-data-type-enum-are-global","title":"Encodings for the data type <code>Enum</code> are global","text":"<p>When working with the data type <code>Enum</code> we specify the categories in advance. This way, Polars can ensure different columns and even different datasets have the same encoding and there is no need for expensive re-encoding or cache lookups.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#data-type-categorical-and-encodings","title":"Data type <code>Categorical</code> and encodings","text":"<p>The fact that the categories for the data type <code>Categorical</code> are inferred come at a cost. The main cost here is that we have no control over our encodings.</p> <p>Consider the following scenario where we append the following two categorical series:</p> <p>Polars encodes the string values in the order they appear. So, the series would look like this:</p> cat_series cat2_series Physical 0 1 2 2 0 Categories Polar Panda Brown Physical 0 1 1 2 2 Categories Panda Brown Polar <p>Combining the series becomes a non-trivial task which is expensive as the physical value of <code>0</code> represents something different in both series. Polars does support these types of operations for convenience, however these should be avoided due to its slower performance as it requires making both encodings compatible first before doing any merge operations.</p>"},{"location":"user-guide/expressions/categorical-data-and-enums/#using-the-global-string-cache","title":"Using the global string cache","text":"<p>One way to handle this reencoding problem is to enable the string cache. Under the string cache, the diagram would instead look like this:</p> SeriesString cache cat_seriescat2_series Physical 0 1 2 2 0 Physical 1 2 2 0 0 Categories Polar Panda Brown <p>When you enable the string cache, strings are no longer encoded in the order they appear on a per-column basis. Instead, the encoding is shared across columns. The value 'Polar' will always be encoded by the same value for all categorical columns created under the string cache. Merge operations (e.g. appends, joins) become cheap again as there is no need to make the encodings compatible first, solving the problem we had above.</p> <p>However, the string cache does come at a small performance hit during construction of the series as we need to look up or insert the string values in the cache. Therefore, it is preferred to use the data type <code>Enum</code> if you know your categories in advance.</p>"},{"location":"user-guide/expressions/expression-expansion/","title":"Expression expansion","text":"<p>As you've seen in the section about expressions and contexts, expression expansion is a feature that enables you to write a single expression that can expand to multiple different expressions, possibly depending on the schema of the context in which the expression is used.</p> <p>This feature isn't just decorative or syntactic sugar. It allows for a very powerful application of DRY principles in your code: a single expression that specifies multiple columns expands into a list of expressions, which means you can write one single expression and reuse the computation that it represents.</p> <p>In this section we will show several forms of expression expansion and we will be using the dataframe that you can see below for that effect:</p>  Python Rust <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {  # As of 14th October 2024, ~3pm UTC\n        \"ticker\": [\"AAPL\", \"NVDA\", \"MSFT\", \"GOOG\", \"AMZN\"],\n        \"company_name\": [\"Apple\", \"NVIDIA\", \"Microsoft\", \"Alphabet (Google)\", \"Amazon\"],\n        \"price\": [229.9, 138.93, 420.56, 166.41, 188.4],\n        \"day_high\": [231.31, 139.6, 424.04, 167.62, 189.83],\n        \"day_low\": [228.6, 136.3, 417.52, 164.78, 188.44],\n        \"year_high\": [237.23, 140.76, 468.35, 193.31, 201.2],\n        \"year_low\": [164.08, 39.23, 324.39, 121.46, 118.35],\n    }\n)\n\nprint(df)\n</code></pre> <pre><code>use polars::prelude::*;\n\n// Data as of 14th October 2024, ~3pm UTC\nlet df = df!(\n    \"ticker\" =&gt; [\"AAPL\", \"NVDA\", \"MSFT\", \"GOOG\", \"AMZN\"],\n    \"company_name\" =&gt; [\"Apple\", \"NVIDIA\", \"Microsoft\", \"Alphabet (Google)\", \"Amazon\"],\n    \"price\" =&gt; [229.9, 138.93, 420.56, 166.41, 188.4],\n    \"day_high\" =&gt; [231.31, 139.6, 424.04, 167.62, 189.83],\n    \"day_low\" =&gt; [228.6, 136.3, 417.52, 164.78, 188.44],\n    \"year_high\" =&gt; [237.23, 140.76, 468.35, 193.31, 201.2],\n    \"year_low\" =&gt; [164.08, 39.23, 324.39, 121.46, 118.35],\n)?;\n\nprintln!(\"{df}\");\n</code></pre> <pre><code>shape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name      \u2506 price  \u2506 day_high \u2506 day_low \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---               \u2506 ---    \u2506 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 str               \u2506 f64    \u2506 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 229.9  \u2506 231.31   \u2506 228.6   \u2506 237.23    \u2506 164.08   \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 138.93 \u2506 139.6    \u2506 136.3   \u2506 140.76    \u2506 39.23    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 420.56 \u2506 424.04   \u2506 417.52  \u2506 468.35    \u2506 324.39   \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 166.41 \u2506 167.62   \u2506 164.78  \u2506 193.31    \u2506 121.46   \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 188.4  \u2506 189.83   \u2506 188.44  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#function-col","title":"Function <code>col</code>","text":"<p>The function <code>col</code> is the most common way of making use of expression expansion features in Polars. Typically used to refer to one column of a dataframe, in this section we explore other ways in which you can use <code>col</code> (or its variants, when in Rust).</p>"},{"location":"user-guide/expressions/expression-expansion/#explicit-expansion-by-column-name","title":"Explicit expansion by column name","text":"<p>The simplest form of expression expansion happens when you provide multiple column names to the function <code>col</code>.</p> <p>The example below uses a single function <code>col</code> with multiple column names to convert the values in USD to EUR:</p>  Python Rust <p> <code>col</code> <pre><code>eur_usd_rate = 1.09  # As of 14th October 2024\n\nresult = df.with_columns(\n    (\n        pl.col(\n            \"price\",\n            \"day_high\",\n            \"day_low\",\n            \"year_high\",\n            \"year_low\",\n        )\n        / eur_usd_rate\n    ).round(2)\n)\nprint(result)\n</code></pre></p> <p> <code>col</code> <pre><code>let eur_usd_rate = 1.09; // As of 14th October 2024\n\nlet result = df\n    .clone()\n    .lazy()\n    .with_column(\n        (cols([\"price\", \"day_high\", \"day_low\", \"year_high\", \"year_low\"]).as_expr()\n            / lit(eur_usd_rate))\n        .round(2, RoundMode::default()),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name      \u2506 price  \u2506 day_high \u2506 day_low \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---               \u2506 ---    \u2506 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 str               \u2506 f64    \u2506 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 210.92 \u2506 212.21   \u2506 209.72  \u2506 217.64    \u2506 150.53   \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 127.46 \u2506 128.07   \u2506 125.05  \u2506 129.14    \u2506 35.99    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 385.83 \u2506 389.03   \u2506 383.05  \u2506 429.68    \u2506 297.61   \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 152.67 \u2506 153.78   \u2506 151.17  \u2506 177.35    \u2506 111.43   \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 172.84 \u2506 174.16   \u2506 172.88  \u2506 184.59    \u2506 108.58   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When you list the column names you want the expression to expand to, you can predict what the expression will expand to. In this case, the expression that does the currency conversion is expanded to a list of five expressions:</p>  Python Rust <p> <code>col</code> <pre><code>exprs = [\n    (pl.col(\"price\") / eur_usd_rate).round(2),\n    (pl.col(\"day_high\") / eur_usd_rate).round(2),\n    (pl.col(\"day_low\") / eur_usd_rate).round(2),\n    (pl.col(\"year_high\") / eur_usd_rate).round(2),\n    (pl.col(\"year_low\") / eur_usd_rate).round(2),\n]\n\nresult2 = df.with_columns(exprs)\nprint(result.equals(result2))\n</code></pre></p> <p> <code>col</code> <pre><code>let exprs = [\n    (col(\"price\") / lit(eur_usd_rate)).round(2, RoundMode::default()),\n    (col(\"day_high\") / lit(eur_usd_rate)).round(2, RoundMode::default()),\n    (col(\"day_low\") / lit(eur_usd_rate)).round(2, RoundMode::default()),\n    (col(\"year_high\") / lit(eur_usd_rate)).round(2, RoundMode::default()),\n    (col(\"year_low\") / lit(eur_usd_rate)).round(2, RoundMode::default()),\n];\n\nlet result2 = df.clone().lazy().with_columns(exprs).collect()?;\nprintln!(\"{}\", result.equals(&amp;result2));\n</code></pre></p> <pre><code>True\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#expansion-by-data-type","title":"Expansion by data type","text":"<p>We had to type five column names in the previous example but the function <code>col</code> can also conveniently accept one or more data types. If you provide data types instead of column names, the expression is expanded to all columns that match one of the data types provided.</p> <p>The example below performs the exact same computation as before:</p>  Python Rust <p> <code>col</code> <pre><code>result = df.with_columns((pl.col(pl.Float64) / eur_usd_rate).round(2))\nprint(result)\n</code></pre></p> <p> <code>dtype_col</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .with_column(\n        (dtype_col(&amp;DataType::Float64).as_selector().as_expr() / lit(eur_usd_rate))\n            .round(2, RoundMode::default()),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name      \u2506 price  \u2506 day_high \u2506 day_low \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---               \u2506 ---    \u2506 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 str               \u2506 f64    \u2506 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 210.92 \u2506 212.21   \u2506 209.72  \u2506 217.64    \u2506 150.53   \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 127.46 \u2506 128.07   \u2506 125.05  \u2506 129.14    \u2506 35.99    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 385.83 \u2506 389.03   \u2506 383.05  \u2506 429.68    \u2506 297.61   \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 152.67 \u2506 153.78   \u2506 151.17  \u2506 177.35    \u2506 111.43   \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 172.84 \u2506 174.16   \u2506 172.88  \u2506 184.59    \u2506 108.58   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When we use a data type with expression expansion we cannot know, beforehand, how many columns a single expression will expand to. We need the schema of the input dataframe if we want to determine what is the final list of expressions that is to be applied.</p> <p>If we weren't sure about whether the price columns where of the type <code>Float64</code> or <code>Float32</code>, we could specify both data types:</p>  Python Rust <p> <code>col</code> <pre><code>result2 = df.with_columns(\n    (\n        pl.col(\n            pl.Float32,\n            pl.Float64,\n        )\n        / eur_usd_rate\n    ).round(2)\n)\nprint(result.equals(result2))\n</code></pre></p> <p> <code>dtype_cols</code> <pre><code>let result2 = df\n    .clone()\n    .lazy()\n    .with_column(\n        (dtype_cols([DataType::Float32, DataType::Float64])\n            .as_selector()\n            .as_expr()\n            / lit(eur_usd_rate))\n        .round(2, RoundMode::default()),\n    )\n    .collect()?;\nprintln!(\"{}\", result.equals(&amp;result2));\n</code></pre></p> <pre><code>True\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#expansion-by-pattern-matching","title":"Expansion by pattern matching","text":"<p>You can also use regular expressions to specify patterns that are used to match the column names. To distinguish between a regular column name and expansion by pattern matching, regular expressions start and end with <code>^</code> and <code>$</code>, respectively. This also means that the pattern must match against the whole column name string.</p> <p>Regular expressions can be mixed with regular column names:</p>  Python Rust <p> <code>col</code> <pre><code>result = df.select(pl.col(\"ticker\", \"^.*_high$\", \"^.*_low$\"))\nprint(result)\n</code></pre></p> <p> <code>col</code> <pre><code>// NOTE: Using regex inside `col`/`cols` requires the feature flag `regex`.\nlet result = df\n    .clone()\n    .lazy()\n    .select([cols([\"ticker\", \"^.*_high$\", \"^.*_low$\"]).as_expr()])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 day_high \u2506 day_low \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 231.31   \u2506 228.6   \u2506 237.23    \u2506 164.08   \u2502\n\u2502 NVDA   \u2506 139.6    \u2506 136.3   \u2506 140.76    \u2506 39.23    \u2502\n\u2502 MSFT   \u2506 424.04   \u2506 417.52  \u2506 468.35    \u2506 324.39   \u2502\n\u2502 GOOG   \u2506 167.62   \u2506 164.78  \u2506 193.31    \u2506 121.46   \u2502\n\u2502 AMZN   \u2506 189.83   \u2506 188.44  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#arguments-cannot-be-of-mixed-types","title":"Arguments cannot be of mixed types","text":"<p>In Python, the function <code>col</code> accepts an arbitrary number of strings (as column names or as regular expressions) or an arbitrary number of data types, but you cannot mix both in the same function call:</p> <pre><code>try:\n    df.select(pl.col(\"ticker\", pl.Float64))\nexcept TypeError as err:\n    print(\"TypeError:\", err)\n</code></pre> <pre><code>TypeError: argument 'names': 'DataTypeClass' object cannot be cast as 'str'\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#selecting-all-columns","title":"Selecting all columns","text":"<p>Polars provides the function <code>all</code> as shorthand notation to refer to all columns of a dataframe:</p>  Python Rust <p> <code>all</code> <pre><code>result = df.select(pl.all())\nprint(result.equals(df))\n</code></pre></p> <p> <code>all</code> <pre><code>let result = df.clone().lazy().select([all().as_expr()]).collect()?;\nprintln!(\"{}\", result.equals(&amp;df));\n</code></pre></p> <pre><code>True\n</code></pre> <p>Note</p> <p>The function <code>all</code> is syntactic sugar for <code>col(\"*\")</code>, but since the argument <code>\"*\"</code> is a special case and <code>all</code> reads more like English, the usage of <code>all</code> is preferred.</p>"},{"location":"user-guide/expressions/expression-expansion/#excluding-columns","title":"Excluding columns","text":"<p>Polars also provides a mechanism to exclude certain columns from expression expansion. For that, you use the function <code>exclude</code>, which accepts exactly the same types of arguments as <code>col</code>:</p>  Python Rust <p> <code>exclude</code> <pre><code>result = df.select(pl.all().exclude(\"^day_.*$\"))\nprint(result)\n</code></pre></p> <p> <code>exclude</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([all().exclude_cols([\"^day_.*$\"]).as_expr()])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name      \u2506 price  \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---               \u2506 ---    \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 str               \u2506 f64    \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 229.9  \u2506 237.23    \u2506 164.08   \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 138.93 \u2506 140.76    \u2506 39.23    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 420.56 \u2506 468.35    \u2506 324.39   \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 166.41 \u2506 193.31    \u2506 121.46   \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 188.4  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Naturally, the function <code>exclude</code> can also be used after the function <code>col</code>:</p>  Python Rust <p> <code>exclude</code> <pre><code>result = df.select(pl.col(pl.Float64).exclude(\"^day_.*$\"))\nprint(result)\n</code></pre></p> <p> <code>exclude</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([dtype_col(&amp;DataType::Float64)\n        .as_selector()\n        .exclude_cols([\"^day_.*$\"])\n        .as_expr()])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 price  \u2506 year_high \u2506 year_low \u2502\n\u2502 ---    \u2506 ---       \u2506 ---      \u2502\n\u2502 f64    \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 229.9  \u2506 237.23    \u2506 164.08   \u2502\n\u2502 138.93 \u2506 140.76    \u2506 39.23    \u2502\n\u2502 420.56 \u2506 468.35    \u2506 324.39   \u2502\n\u2502 166.41 \u2506 193.31    \u2506 121.46   \u2502\n\u2502 188.4  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#column-renaming","title":"Column renaming","text":"<p>By default, when you apply an expression to a column, the result keeps the same name as the original column.</p> <p>Preserving the column name can be semantically wrong and in certain cases Polars may even raise an error if duplicate names occur:</p>  Python Rust <pre><code>from polars.exceptions import DuplicateError\n\ngbp_usd_rate = 1.31  # As of 14th October 2024\n\ntry:\n    df.select(\n        pl.col(\"price\") / gbp_usd_rate,  # This would be named \"price\"...\n        pl.col(\"price\") / eur_usd_rate,  # And so would this.\n    )\nexcept DuplicateError as err:\n    print(\"DuplicateError:\", err)\n</code></pre> <pre><code>let gbp_usd_rate = 1.31; // As of 14th October 2024\n\nlet result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"price\") / lit(gbp_usd_rate),\n        col(\"price\") / lit(eur_usd_rate),\n    ])\n    .collect();\nmatch result {\n    Ok(df) =&gt; println!(\"{df}\"),\n    Err(e) =&gt; println!(\"{e}\"),\n};\n</code></pre> <pre><code>DuplicateError: projections contained duplicate output name 'price'. It's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.\n</code></pre> <p>To prevent errors like this, and to allow users to rename their columns when appropriate, Polars provides a series of functions that let you change the name of a column or a group of columns.</p>"},{"location":"user-guide/expressions/expression-expansion/#renaming-a-single-column-with-alias","title":"Renaming a single column with <code>alias</code>","text":"<p>The function <code>alias</code> has been used thoroughly in the documentation already and it lets you rename a single column:</p>  Python Rust <p> <code>alias</code> <pre><code>result = df.select(\n    (pl.col(\"price\") / gbp_usd_rate).alias(\"price (GBP)\"),\n    (pl.col(\"price\") / eur_usd_rate).alias(\"price (EUR)\"),\n)\n</code></pre></p> <p> <code>alias</code> <pre><code>let _result = df\n    .clone()\n    .lazy()\n    .select([\n        (col(\"price\") / lit(gbp_usd_rate)).alias(\"price (GBP)\"),\n        (col(\"price\") / lit(eur_usd_rate)).alias(\"price (EUR)\"),\n    ])\n    .collect()?;\n</code></pre></p> <p></p>"},{"location":"user-guide/expressions/expression-expansion/#prefixing-and-suffixing-column-names","title":"Prefixing and suffixing column names","text":"<p>When using expression expansion you cannot use the function <code>alias</code> because the function <code>alias</code> is designed specifically to rename a single column.</p> <p>When it suffices to add a static prefix or a static suffix to the existing names, we can use the functions <code>prefix</code> and <code>suffix</code> from the namespace <code>name</code>:</p>  Python Rust <p> <code>name namespace</code> \u00b7 <code>prefix</code> \u00b7 <code>suffix</code> <pre><code>result = df.select(\n    (pl.col(\"^year_.*$\") / eur_usd_rate).name.prefix(\"in_eur_\"),\n    (pl.col(\"day_high\", \"day_low\") / gbp_usd_rate).name.suffix(\"_gbp\"),\n)\nprint(result)\n</code></pre></p> <p> <code>name namespace</code> \u00b7 <code>prefix</code> \u00b7 <code>suffix</code> \u00b7  Available on feature lazy <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        (col(\"^year_.*$\") / lit(eur_usd_rate))\n            .name()\n            .prefix(\"in_eur_\"),\n        (cols([\"day_high\", \"day_low\"]).as_expr() / lit(gbp_usd_rate))\n            .name()\n            .suffix(\"_gbp\"),\n    ])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 in_eur_year_high \u2506 in_eur_year_low \u2506 day_high_gbp \u2506 day_low_gbp \u2502\n\u2502 ---              \u2506 ---             \u2506 ---          \u2506 ---         \u2502\n\u2502 f64              \u2506 f64             \u2506 f64          \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 217.642202       \u2506 150.53211       \u2506 176.572519   \u2506 174.503817  \u2502\n\u2502 129.137615       \u2506 35.990826       \u2506 106.564885   \u2506 104.045802  \u2502\n\u2502 429.678899       \u2506 297.605505      \u2506 323.694656   \u2506 318.717557  \u2502\n\u2502 177.348624       \u2506 111.431193      \u2506 127.954198   \u2506 125.78626   \u2502\n\u2502 184.587156       \u2506 108.577982      \u2506 144.908397   \u2506 143.847328  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#dynamic-name-replacement","title":"Dynamic name replacement","text":"<p>If a static prefix/suffix is not enough, the namespace <code>name</code> also provides the function <code>map</code> that accepts a callable that accepts the old column names and produces the new ones:</p>  Python Rust <p> <code>name namespace</code> \u00b7 <code>map</code> <pre><code># There is also `.name.to_uppercase`, so this usage of `.map` is moot.\nresult = df.select(pl.all().name.map(str.upper))\nprint(result)\n</code></pre></p> <p> <code>name namespace</code> \u00b7 <code>map</code> \u00b7  Available on feature lazy <pre><code>// There is also `name().to_uppercase()`, so this usage of `map` is moot.\nlet result = df\n    .clone()\n    .lazy()\n    .select([all()\n        .as_expr()\n        .name()\n        .map(PlanCallback::new(|name: PlSmallStr| {\n            Ok(PlSmallStr::from_string(name.to_ascii_uppercase()))\n        }))])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TICKER \u2506 COMPANY_NAME      \u2506 PRICE  \u2506 DAY_HIGH \u2506 DAY_LOW \u2506 YEAR_HIGH \u2506 YEAR_LOW \u2502\n\u2502 ---    \u2506 ---               \u2506 ---    \u2506 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 str    \u2506 str               \u2506 f64    \u2506 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 229.9  \u2506 231.31   \u2506 228.6   \u2506 237.23    \u2506 164.08   \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 138.93 \u2506 139.6    \u2506 136.3   \u2506 140.76    \u2506 39.23    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 420.56 \u2506 424.04   \u2506 417.52  \u2506 468.35    \u2506 324.39   \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 166.41 \u2506 167.62   \u2506 164.78  \u2506 193.31    \u2506 121.46   \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 188.4  \u2506 189.83   \u2506 188.44  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See the API reference for the full contents of the namespace <code>name</code>.</p>"},{"location":"user-guide/expressions/expression-expansion/#programmatically-generating-expressions","title":"Programmatically generating expressions","text":"<p>Expression expansion is a very useful feature but it does not solve all of your problems. For example, if we want to compute the day and year amplitude of the prices of the stocks in our dataframe, expression expansion won't help us.</p> <p>At first, you may think about using a <code>for</code> loop:</p>  Python Rust <pre><code>result = df\nfor tp in [\"day\", \"year\"]:\n    result = result.with_columns(\n        (pl.col(f\"{tp}_high\") - pl.col(f\"{tp}_low\")).alias(f\"{tp}_amplitude\")\n    )\nprint(result)\n</code></pre> <pre><code>let mut result = df.clone().lazy();\nfor tp in [\"day\", \"year\"] {\n    let high = format!(\"{tp}_high\");\n    let low = format!(\"{tp}_low\");\n    let aliased = format!(\"{tp}_amplitude\");\n    result = result.with_column((col(high) - col(low)).alias(aliased))\n}\nlet result = result.collect()?;\nprintln!(\"{result}\");\n</code></pre> <pre><code>shape: (5, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name \u2506 price  \u2506 day_high \u2506 \u2026 \u2506 year_high \u2506 year_low \u2506 day_amplitu \u2506 year_amplit \u2502\n\u2502 ---    \u2506 ---          \u2506 ---    \u2506 ---      \u2506   \u2506 ---       \u2506 ---      \u2506 de          \u2506 ude         \u2502\n\u2502 str    \u2506 str          \u2506 f64    \u2506 f64      \u2506   \u2506 f64       \u2506 f64      \u2506 ---         \u2506 ---         \u2502\n\u2502        \u2506              \u2506        \u2506          \u2506   \u2506           \u2506          \u2506 f64         \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple        \u2506 229.9  \u2506 231.31   \u2506 \u2026 \u2506 237.23    \u2506 164.08   \u2506 2.71        \u2506 73.15       \u2502\n\u2502 NVDA   \u2506 NVIDIA       \u2506 138.93 \u2506 139.6    \u2506 \u2026 \u2506 140.76    \u2506 39.23    \u2506 3.3         \u2506 101.53      \u2502\n\u2502 MSFT   \u2506 Microsoft    \u2506 420.56 \u2506 424.04   \u2506 \u2026 \u2506 468.35    \u2506 324.39   \u2506 6.52        \u2506 143.96      \u2502\n\u2502 GOOG   \u2506 Alphabet     \u2506 166.41 \u2506 167.62   \u2506 \u2026 \u2506 193.31    \u2506 121.46   \u2506 2.84        \u2506 71.85       \u2502\n\u2502        \u2506 (Google)     \u2506        \u2506          \u2506   \u2506           \u2506          \u2506             \u2506             \u2502\n\u2502 AMZN   \u2506 Amazon       \u2506 188.4  \u2506 189.83   \u2506 \u2026 \u2506 201.2     \u2506 118.35   \u2506 1.39        \u2506 82.85       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Do not do this. Instead, generate all of the expressions you want to compute programmatically and use them only once in a context. Loosely speaking, you want to swap the <code>for</code> loop with the context <code>with_columns</code>. In practice, you could do something like the following:</p>  Python Rust <pre><code>def amplitude_expressions(time_periods):\n    for tp in time_periods:\n        yield (pl.col(f\"{tp}_high\") - pl.col(f\"{tp}_low\")).alias(f\"{tp}_amplitude\")\n\n\nresult = df.with_columns(amplitude_expressions([\"day\", \"year\"]))\nprint(result)\n</code></pre> <pre><code>let mut exprs: Vec&lt;Expr&gt; = vec![];\nfor tp in [\"day\", \"year\"] {\n    let high = format!(\"{tp}_high\");\n    let low = format!(\"{tp}_low\");\n    let aliased = format!(\"{tp}_amplitude\");\n    exprs.push((col(high) - col(low)).alias(aliased))\n}\nlet result = df.lazy().with_columns(exprs).collect()?;\nprintln!(\"{result}\");\n</code></pre> <pre><code>shape: (5, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name \u2506 price  \u2506 day_high \u2506 \u2026 \u2506 year_high \u2506 year_low \u2506 day_amplitu \u2506 year_amplit \u2502\n\u2502 ---    \u2506 ---          \u2506 ---    \u2506 ---      \u2506   \u2506 ---       \u2506 ---      \u2506 de          \u2506 ude         \u2502\n\u2502 str    \u2506 str          \u2506 f64    \u2506 f64      \u2506   \u2506 f64       \u2506 f64      \u2506 ---         \u2506 ---         \u2502\n\u2502        \u2506              \u2506        \u2506          \u2506   \u2506           \u2506          \u2506 f64         \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple        \u2506 229.9  \u2506 231.31   \u2506 \u2026 \u2506 237.23    \u2506 164.08   \u2506 2.71        \u2506 73.15       \u2502\n\u2502 NVDA   \u2506 NVIDIA       \u2506 138.93 \u2506 139.6    \u2506 \u2026 \u2506 140.76    \u2506 39.23    \u2506 3.3         \u2506 101.53      \u2502\n\u2502 MSFT   \u2506 Microsoft    \u2506 420.56 \u2506 424.04   \u2506 \u2026 \u2506 468.35    \u2506 324.39   \u2506 6.52        \u2506 143.96      \u2502\n\u2502 GOOG   \u2506 Alphabet     \u2506 166.41 \u2506 167.62   \u2506 \u2026 \u2506 193.31    \u2506 121.46   \u2506 2.84        \u2506 71.85       \u2502\n\u2502        \u2506 (Google)     \u2506        \u2506          \u2506   \u2506           \u2506          \u2506             \u2506             \u2502\n\u2502 AMZN   \u2506 Amazon       \u2506 188.4  \u2506 189.83   \u2506 \u2026 \u2506 201.2     \u2506 118.35   \u2506 1.39        \u2506 82.85       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This produces the same final result and by specifying all of the expressions in one go we give Polars the opportunity to:</p> <ol> <li>do a better job at optimising the query; and</li> <li>parallelise the execution of the actual computations.</li> </ol>"},{"location":"user-guide/expressions/expression-expansion/#more-flexible-column-selections","title":"More flexible column selections","text":"<p>Polars comes with the submodule <code>selectors</code> that provides a number of functions that allow you to write more flexible column selections for expression expansion.</p> <p>Warning</p> <p>This functionality is not available in Rust yet. Refer to Polars issue #10594.</p> <p>As a first example, here is how we can use the functions <code>string</code> and <code>ends_with</code>, and the set operations that the functions from <code>selectors</code> support, to select all string columns and the columns whose names end with <code>\"_high\"</code>:</p>  Python Rust <p> <code>selectors</code> <pre><code>import polars.selectors as cs\n\nresult = df.select(cs.string() | cs.ends_with(\"_high\"))\nprint(result)\n</code></pre></p> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticker \u2506 company_name      \u2506 day_high \u2506 year_high \u2502\n\u2502 ---    \u2506 ---               \u2506 ---      \u2506 ---       \u2502\n\u2502 str    \u2506 str               \u2506 f64      \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AAPL   \u2506 Apple             \u2506 231.31   \u2506 237.23    \u2502\n\u2502 NVDA   \u2506 NVIDIA            \u2506 139.6    \u2506 140.76    \u2502\n\u2502 MSFT   \u2506 Microsoft         \u2506 424.04   \u2506 468.35    \u2502\n\u2502 GOOG   \u2506 Alphabet (Google) \u2506 167.62   \u2506 193.31    \u2502\n\u2502 AMZN   \u2506 Amazon            \u2506 189.83   \u2506 201.2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The submodule <code>selectors</code> provides a number of selectors that match based on the data type of the columns, of which the most useful are the functions that match a whole category of types, like <code>cs.numeric</code> for all numeric data types or <code>cs.temporal</code> for all temporal data types.</p> <p>The submodule <code>selectors</code> also provides a number of selectors that match based on patterns in the column names which make it more convenient to specify common patterns you may want to check for, like the function <code>cs.ends_with</code> that was shown above.</p>"},{"location":"user-guide/expressions/expression-expansion/#combining-selectors-with-set-operations","title":"Combining selectors with set operations","text":"<p>We can combine multiple selectors using set operations and the usual Python operators:</p> Operator Operation <code>A | B</code> Union <code>A &amp; B</code> Intersection <code>A - B</code> Difference <code>A ^ B</code> Symmetric difference <code>~A</code> Complement <p>The next example matches all non-string columns that contain an underscore in the name:</p>  Python Rust <p> <code>selectors</code> <pre><code>result = df.select(cs.contains(\"_\") - cs.string())\nprint(result)\n</code></pre></p> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 day_high \u2506 day_low \u2506 year_high \u2506 year_low \u2502\n\u2502 ---      \u2506 ---     \u2506 ---       \u2506 ---      \u2502\n\u2502 f64      \u2506 f64     \u2506 f64       \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 231.31   \u2506 228.6   \u2506 237.23    \u2506 164.08   \u2502\n\u2502 139.6    \u2506 136.3   \u2506 140.76    \u2506 39.23    \u2502\n\u2502 424.04   \u2506 417.52  \u2506 468.35    \u2506 324.39   \u2502\n\u2502 167.62   \u2506 164.78  \u2506 193.31    \u2506 121.46   \u2502\n\u2502 189.83   \u2506 188.44  \u2506 201.2     \u2506 118.35   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#resolving-operator-ambiguity","title":"Resolving operator ambiguity","text":"<p>Expression functions can be chained on top of selectors:</p>  Python Rust <p> <code>selectors</code> <pre><code>result = df.select((cs.contains(\"_\") - cs.string()) / eur_usd_rate)\nprint(result)\n</code></pre></p> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 day_high   \u2506 day_low    \u2506 year_high  \u2506 year_low   \u2502\n\u2502 ---        \u2506 ---        \u2506 ---        \u2506 ---        \u2502\n\u2502 f64        \u2506 f64        \u2506 f64        \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 212.211009 \u2506 209.724771 \u2506 217.642202 \u2506 150.53211  \u2502\n\u2502 128.073394 \u2506 125.045872 \u2506 129.137615 \u2506 35.990826  \u2502\n\u2502 389.027523 \u2506 383.045872 \u2506 429.678899 \u2506 297.605505 \u2502\n\u2502 153.779817 \u2506 151.174312 \u2506 177.348624 \u2506 111.431193 \u2502\n\u2502 174.155963 \u2506 172.880734 \u2506 184.587156 \u2506 108.577982 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, some operators have been overloaded to operate both on Polars selectors and on expressions. For example, the operator <code>~</code> on a selector represents the set operation \u201ccomplement\u201d and on an expression represents the Boolean operation of negation.</p> <p>When you use a selector and then want to use, in the context of an expression, one of the operators that act as set operators for selectors, you can use the function <code>as_expr</code>.</p> <p>Below, we want to negate the Boolean values in the columns \u201chas_partner\u201d, \u201chas_kids\u201d, and \u201chas_tattoos\u201d. If we are not careful, the combination of the operator <code>~</code> and the selector <code>cs.starts_with(\"has_\")</code> will actually select the columns that we do not care about:</p>  Python Rust <pre><code>people = pl.DataFrame(\n    {\n        \"name\": [\"Anna\", \"Bob\"],\n        \"has_partner\": [True, False],\n        \"has_kids\": [False, False],\n        \"has_tattoos\": [True, False],\n        \"is_alive\": [True, True],\n    }\n)\n\nwrong_result = people.select((~cs.starts_with(\"has_\")).name.prefix(\"not_\"))\nprint(wrong_result)\n</code></pre> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 not_name \u2506 not_is_alive \u2502\n\u2502 ---      \u2506 ---          \u2502\n\u2502 str      \u2506 bool         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Anna     \u2506 true         \u2502\n\u2502 Bob      \u2506 true         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The correct solution uses <code>as_expr</code>:</p>  Python Rust <pre><code>result = people.select((~cs.starts_with(\"has_\").as_expr()).name.prefix(\"not_\"))\nprint(result)\n</code></pre> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 not_has_partner \u2506 not_has_kids \u2506 not_has_tattoos \u2502\n\u2502 ---             \u2506 ---          \u2506 ---             \u2502\n\u2502 bool            \u2506 bool         \u2506 bool            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false           \u2506 true         \u2506 false           \u2502\n\u2502 true            \u2506 true         \u2506 true            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#debugging-selectors","title":"Debugging selectors","text":"<p>When you are not sure whether you have a Polars selector at hand or not, you can use the function <code>cs.is_selector</code> to check:</p>  Python Rust <p> <code>is_selector</code> <pre><code>print(cs.is_selector(~cs.starts_with(\"has_\").as_expr()))\n</code></pre></p> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>False\n</code></pre> <p>This should help you avoid any ambiguous situations where you think you are operating with expressions but are in fact operating with selectors.</p> <p>Another helpful debugging utility is the function <code>expand_selector</code>. Given a target frame or schema, you can check what columns a given selector will expand to:</p>  Python Rust <p> <code>expand_selector</code> <pre><code>print(\n    cs.expand_selector(\n        people,\n        cs.starts_with(\"has_\"),\n    )\n)\n</code></pre></p> <pre><code>// Selectors are not available in Rust yet.\n// Refer to https://github.com/pola-rs/polars/issues/10594\n</code></pre> <pre><code>('has_partner', 'has_kids', 'has_tattoos')\n</code></pre>"},{"location":"user-guide/expressions/expression-expansion/#complete-reference","title":"Complete reference","text":"<p>The tables below group the functions available in the submodule <code>selectors</code> by their type of behaviour.</p>"},{"location":"user-guide/expressions/expression-expansion/#selectors-for-data-types","title":"Selectors for data types","text":"<p>Selectors that match based on the data type of the column:</p> Selector function Data type(s) matched <code>binary</code> <code>Binary</code> <code>boolean</code> <code>Boolean</code> <code>by_dtype</code> Data types specified as arguments <code>categorical</code> <code>Categorical</code> <code>date</code> <code>Date</code> <code>datetime</code> <code>Datetime</code>, optionally filtering by time unit/zone <code>decimal</code> <code>Decimal</code> <code>duration</code> <code>Duration</code>, optionally filtering by time unit <code>float</code> All float types, regardless of precision <code>integer</code> All integer types, signed and unsigned, regardless of precision <code>numeric</code> All numeric types, namely integers, floats, and <code>Decimal</code> <code>signed_integer</code> All signed integer types, regardless of precision <code>string</code> <code>String</code> <code>temporal</code> All temporal data types, namely <code>Date</code>, <code>Datetime</code>, and <code>Duration</code> <code>time</code> <code>Time</code> <code>unsigned_integer</code> All unsigned integer types, regardless of precision"},{"location":"user-guide/expressions/expression-expansion/#selectors-for-column-name-patterns","title":"Selectors for column name patterns","text":"<p>Selectors that match based on column name patterns:</p> Selector function Columns selected <code>alpha</code> Columns with alphabetical names <code>alphanumeric</code> Columns with alphanumeric names (letters and the digits 0-9) <code>by_name</code> Columns with the names specified as arguments <code>contains</code> Columns whose names contain the substring specified <code>digit</code> Columns with numeric names (only the digits 0-9) <code>ends_with</code> Columns whose names end with the given substring <code>matches</code> Columns whose names match the given regex pattern <code>starts_with</code> Columns whose names start with the given substring"},{"location":"user-guide/expressions/expression-expansion/#positional-selectors","title":"Positional selectors","text":"<p>Selectors that match based on the position of the columns:</p> Selector function Columns selected <code>all</code> All columns <code>by_index</code> The columns at the specified indices <code>first</code> The first column in the context <code>last</code> The last column in the context"},{"location":"user-guide/expressions/expression-expansion/#miscellaneous-functions","title":"Miscellaneous functions","text":"<p>The submodule <code>selectors</code> also provides the following functions:</p> Function Behaviour <code>as_expr</code>* Convert a selector to an expression <code>exclude</code> Selects all columns except those matching the given names, data types, or selectors <code>expand_selector</code> Expand selector to matching columns with respect to a specific frame or target schema <code>is_selector</code> Check whether the given object/expression is a selector <p>*<code>as_expr</code> isn't a function defined on the submodule <code>selectors</code>, but rather a method defined on selectors.</p>"},{"location":"user-guide/expressions/folds/","title":"Folds","text":"<p>Polars provides many expressions to perform computations across columns, like <code>sum_horizontal</code>, <code>mean_horizontal</code>, and <code>min_horizontal</code>. However, these are just special cases of a general algorithm called a fold, and Polars provides a general mechanism for you to compute custom folds for when the specialised versions of Polars are not enough.</p> <p>Folds computed with the function <code>fold</code> operate on the full columns for maximum speed. They utilize the data layout very efficiently and often have vectorized execution.</p>"},{"location":"user-guide/expressions/folds/#basic-example","title":"Basic example","text":"<p>As a first example, we will reimplement <code>sum_horizontal</code> with the function <code>fold</code>:</p>  Python Rust <p> <code>fold</code> <pre><code>import operator\nimport polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"label\": [\"foo\", \"bar\", \"spam\"],\n        \"a\": [1, 2, 3],\n        \"b\": [10, 20, 30],\n    }\n)\n\nresult = df.select(\n    pl.fold(\n        acc=pl.lit(0),\n        function=operator.add,\n        exprs=pl.col(\"a\", \"b\"),\n    ).alias(\"sum_fold\"),\n    pl.sum_horizontal(pl.col(\"a\", \"b\")).alias(\"sum_horz\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>use polars::lazy::dsl::sum_horizontal;\nuse polars::prelude::*;\n\nlet df = df!(\n    \"label\" =&gt; [\"foo\", \"bar\", \"spam\"],\n    \"a\" =&gt; [1, 2, 3],\n    \"b\" =&gt; [10, 20, 30],\n)?;\n\nlet result = df\n    .clone()\n    .lazy()\n    .select([\n        fold_exprs(\n            lit(0),\n            PlanCallback::new(|(acc, val)| &amp;acc + &amp;val),\n            [col(\"a\"), col(\"b\")],\n            false,\n            None,\n        )\n        .alias(\"sum_fold\"),\n        sum_horizontal([col(\"a\"), col(\"b\")], true)?.alias(\"sum_horz\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum_fold \u2506 sum_horz \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 i32      \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11       \u2506 11       \u2502\n\u2502 22       \u2506 22       \u2502\n\u2502 33       \u2506 33       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The function <code>fold</code> expects a function <code>f</code> as the parameter <code>function</code> and <code>f</code> should accept two arguments. The first argument is the accumulated result, which we initialise as zero, and the second argument takes the successive values of the expressions listed in the parameter <code>exprs</code>. In our case, they're the two columns \u201ca\u201d and \u201cb\u201d.</p> <p>The snippet below includes a third explicit expression that represents what the function <code>fold</code> is doing above:</p>  Python Rust <p> <code>fold</code> <pre><code>acc = pl.lit(0)\nf = operator.add\n\nresult = df.select(\n    f(f(acc, pl.col(\"a\")), pl.col(\"b\")),\n    pl.fold(acc=acc, function=f, exprs=pl.col(\"a\", \"b\")).alias(\"sum_fold\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let acc = lit(0);\nlet f = |acc: Expr, val: Expr| acc + val;\n\nlet result = df\n    .clone()\n    .lazy()\n    .select([\n        f(f(acc, col(\"a\")), col(\"b\")),\n        fold_exprs(\n            lit(0),\n            PlanCallback::new(|(acc, val)| &amp;acc + &amp;val),\n            [col(\"a\"), col(\"b\")],\n            false,\n            None,\n        )\n        .alias(\"sum_fold\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 literal \u2506 sum_fold \u2502\n\u2502 ---     \u2506 ---      \u2502\n\u2502 i64     \u2506 i32      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11      \u2506 11       \u2502\n\u2502 22      \u2506 22       \u2502\n\u2502 33      \u2506 33       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <code>fold</code> in Python <p>Most programming languages include a higher-order function that implements the algorithm that the function <code>fold</code> in Polars implements. The Polars <code>fold</code> is very similar to Python's <code>functools.reduce</code>. You can learn more about the power of <code>functools.reduce</code> in this article.</p>"},{"location":"user-guide/expressions/folds/#the-initial-value-acc","title":"The initial value <code>acc</code>","text":"<p>The initial value chosen for the accumulator <code>acc</code> is typically, but not always, the identity element of the operation you want to apply. For example, if we wanted to multiply across the columns, we would not get the correct result if our accumulator was set to zero:</p>  Python Rust <p> <code>fold</code> <pre><code>result = df.select(\n    pl.fold(\n        acc=pl.lit(0),\n        function=operator.mul,\n        exprs=pl.col(\"a\", \"b\"),\n    ).alias(\"prod\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([fold_exprs(\n        lit(0),\n        PlanCallback::new(|(acc, val)| &amp;acc * &amp;val),\n        [col(\"a\"), col(\"b\")],\n        false,\n        None,\n    )\n    .alias(\"prod\")])\n    .collect()?;\n\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 prod \u2502\n\u2502 ---  \u2502\n\u2502 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0    \u2502\n\u2502 0    \u2502\n\u2502 0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To fix this, the accumulator <code>acc</code> should be set to <code>1</code>:</p>  Python Rust <p> <code>fold</code> <pre><code>result = df.select(\n    pl.fold(\n        acc=pl.lit(1),\n        function=operator.mul,\n        exprs=pl.col(\"a\", \"b\"),\n    ).alias(\"prod\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let result = df\n    .lazy()\n    .select([fold_exprs(\n        lit(1),\n        PlanCallback::new(|(acc, val)| &amp;acc * &amp;val),\n        [col(\"a\"), col(\"b\")],\n        false,\n        None,\n    )\n    .alias(\"prod\")])\n    .collect()?;\n\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 prod \u2502\n\u2502 ---  \u2502\n\u2502 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 10   \u2502\n\u2502 40   \u2502\n\u2502 90   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/folds/#conditional","title":"Conditional","text":"<p>In the case where you'd want to apply a condition/predicate across all columns in a dataframe, a fold can be a very concise way to express this.</p>  Python Rust <p> <code>fold</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [0, 1, 2],\n    }\n)\n\nresult = df.filter(\n    pl.fold(\n        acc=pl.lit(True),\n        function=lambda acc, x: acc &amp; x,\n        exprs=pl.all() &gt; 1,\n    )\n)\nprint(result)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let df = df!(\n    \"a\" =&gt; [1, 2, 3],\n    \"b\" =&gt; [0, 1, 2],\n)?;\n\nlet result = df\n    .lazy()\n    .filter(fold_exprs(\n        lit(true),\n        PlanCallback::new(|(acc, val)| &amp;acc &amp; &amp;val),\n        [col(\"*\").gt(1)],\n        false,\n        None,\n    ))\n    .collect()?;\n\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The snippet above filters all rows where all columns are greater than 1.</p>"},{"location":"user-guide/expressions/folds/#folds-and-string-data","title":"Folds and string data","text":"<p>Folds could be used to concatenate string data. However, due to the materialization of intermediate columns, this operation will have squared complexity.</p> <p>Therefore, we recommend using the function <code>concat_str</code> for this:</p>  Python Rust <p> <code>concat_str</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [\"a\", \"b\", \"c\"],\n        \"b\": [1, 2, 3],\n    }\n)\n\nresult = df.select(pl.concat_str([\"a\", \"b\"]))\nprint(result)\n</code></pre></p> <p> <code>concat_str</code> \u00b7  Available on feature concat_str <pre><code>let df = df!(\n    \"a\" =&gt; [\"a\", \"b\", \"c\"],\n    \"b\" =&gt; [1, 2, 3],\n)?;\n\nlet result = df\n    .lazy()\n    .select([concat_str([col(\"a\"), col(\"b\")], \"\", false)])\n    .collect()?;\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a1  \u2502\n\u2502 b2  \u2502\n\u2502 c3  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/","title":"Lists and arrays","text":"<p>Polars has first-class support for two homogeneous container data types: <code>List</code> and <code>Array</code>. Polars supports many operations with the two data types and their APIs overlap, so this section of the user guide has the objective of clarifying when one data type should be chosen in favour of the other.</p>"},{"location":"user-guide/expressions/lists-and-arrays/#lists-vs-arrays","title":"Lists vs arrays","text":""},{"location":"user-guide/expressions/lists-and-arrays/#the-data-type-list","title":"The data type <code>List</code>","text":"<p>The data type list is suitable for columns whose values are homogeneous 1D containers of varying lengths.</p> <p>The dataframe below contains three examples of columns with the data type <code>List</code>:</p>  Python Rust <p> <code>List</code> <pre><code>from datetime import datetime\nimport polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"names\": [\n            [\"Anne\", \"Averill\", \"Adams\"],\n            [\"Brandon\", \"Brooke\", \"Borden\", \"Branson\"],\n            [\"Camila\", \"Campbell\"],\n            [\"Dennis\", \"Doyle\"],\n        ],\n        \"children_ages\": [\n            [5, 7],\n            [],\n            [],\n            [8, 11, 18],\n        ],\n        \"medical_appointments\": [\n            [],\n            [],\n            [],\n            [datetime(2022, 5, 22, 16, 30)],\n        ],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>List</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 names                           \u2506 children_ages \u2506 medical_appointments  \u2502\n\u2502 ---                             \u2506 ---           \u2506 ---                   \u2502\n\u2502 list[str]                       \u2506 list[i64]     \u2506 list[datetime[\u03bcs]]    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"Anne\", \"Averill\", \"Adams\"]    \u2506 [5, 7]        \u2506 []                    \u2502\n\u2502 [\"Brandon\", \"Brooke\", \u2026 \"Brans\u2026 \u2506 []            \u2506 []                    \u2502\n\u2502 [\"Camila\", \"Campbell\"]          \u2506 []            \u2506 []                    \u2502\n\u2502 [\"Dennis\", \"Doyle\"]             \u2506 [8, 11, 18]   \u2506 [2022-05-22 16:30:00] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that the data type <code>List</code> is different from Python's type <code>list</code>, where elements can be of any type. If you want to store true Python lists in a column, you can do so with the data type <code>Object</code> and your column will not have the list manipulation features that we're about to discuss.</p>"},{"location":"user-guide/expressions/lists-and-arrays/#the-data-type-array","title":"The data type <code>Array</code>","text":"<p>The data type <code>Array</code> is suitable for columns whose values are homogeneous containers of an arbitrary dimension with a known and fixed shape.</p> <p>The dataframe below contains two examples of columns with the data type <code>Array</code>.</p>  Python Rust <p> <code>Array</code> <pre><code>df = pl.DataFrame(\n    {\n        \"bit_flags\": [\n            [True, True, True, True, False],\n            [False, True, True, True, True],\n        ],\n        \"tic_tac_toe\": [\n            [\n                [\" \", \"x\", \"o\"],\n                [\" \", \"x\", \" \"],\n                [\"o\", \"x\", \" \"],\n            ],\n            [\n                [\"o\", \"x\", \"x\"],\n                [\" \", \"o\", \"x\"],\n                [\" \", \" \", \"o\"],\n            ],\n        ],\n    },\n    schema={\n        \"bit_flags\": pl.Array(pl.Boolean, 5),\n        \"tic_tac_toe\": pl.Array(pl.String, (3, 3)),\n    },\n)\n\nprint(df)\n</code></pre></p> <p> <code>Array</code> \u00b7  Available on feature dtype-array <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bit_flags             \u2506 tic_tac_toe                     \u2502\n\u2502 ---                   \u2506 ---                             \u2502\n\u2502 array[bool, 5]        \u2506 array[str, (3, 3)]              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [true, true, \u2026 false] \u2506 [[\" \", \"x\", \"o\"], [\" \", \"x\", \"\u2026 \u2502\n\u2502 [false, true, \u2026 true] \u2506 [[\"o\", \"x\", \"x\"], [\" \", \"o\", \"\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The example above shows how to specify that the columns \u201cbit_flags\u201d and \u201ctic_tac_toe\u201d have the data type <code>Array</code>, parametrised by the data type of the elements contained within and by the shape of each array.</p> <p>In general, Polars does not infer that a column has the data type <code>Array</code> for performance reasons, and defaults to the appropriate variant of the data type <code>List</code>. In Python, an exception to this rule is when you provide a NumPy array to build a column. In that case, Polars has the guarantee from NumPy that all subarrays have the same shape, so an array of \\(n + 1\\) dimensions will generate a column of \\(n\\) dimensional arrays:</p>  Python Rust <p> <code>Array</code> <pre><code>import numpy as np\n\narray = np.arange(0, 120).reshape((5, 2, 3, 4))  # 4D array\n\nprint(pl.Series(array).dtype)  # Column with the 3D subarrays\n</code></pre></p> <p> <code>Array</code> \u00b7  Available on feature dtype-array <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>Array(Int64, shape=(2, 3, 4))\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/#when-to-use-each","title":"When to use each","text":"<p>In short, prefer the data type <code>Array</code> over <code>List</code> because it is more memory efficient and more performant. If you cannot use <code>Array</code>, then use <code>List</code>:</p> <ul> <li>when the values within a column do not have a fixed shape; or</li> <li>when you need functions that are only available in the list API.</li> </ul>"},{"location":"user-guide/expressions/lists-and-arrays/#working-with-lists","title":"Working with lists","text":""},{"location":"user-guide/expressions/lists-and-arrays/#the-namespace-list","title":"The namespace <code>list</code>","text":"<p>Polars provides many functions to work with values of the data type <code>List</code> and these are grouped inside the namespace <code>list</code>. We will explore this namespace a bit now.</p> <p><code>arr</code> then, <code>list</code> now</p> <p>In previous versions of Polars, the namespace for list operations used to be <code>arr</code>. <code>arr</code> is now the namespace for the data type <code>Array</code>. If you find references to the namespace <code>arr</code> on StackOverflow or other sources, note that those sources may be outdated.</p> <p>The dataframe <code>weather</code> defined below contains data from different weather stations across a region. When the weather station is unable to get a result, an error code is recorded instead of the actual temperature at that time.</p>  Python Rust <pre><code>weather = pl.DataFrame(\n    {\n        \"station\": [f\"Station {idx}\" for idx in range(1, 6)],\n        \"temperatures\": [\n            \"20 5 5 E1 7 13 19 9 6 20\",\n            \"18 8 16 11 23 E2 8 E2 E2 E2 90 70 40\",\n            \"19 24 E9 16 6 12 10 22\",\n            \"E2 E0 15 7 8 10 E1 24 17 13 6\",\n            \"14 8 E0 16 22 24 E1\",\n        ],\n    }\n)\n\nprint(weather)\n</code></pre> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures                    \u2502\n\u2502 ---       \u2506 ---                             \u2502\n\u2502 str       \u2506 str                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20 5 5 E1 7 13 19 9 6 20        \u2502\n\u2502 Station 2 \u2506 18 8 16 11 23 E2 8 E2 E2 E2 90\u2026 \u2502\n\u2502 Station 3 \u2506 19 24 E9 16 6 12 10 22          \u2502\n\u2502 Station 4 \u2506 E2 E0 15 7 8 10 E1 24 17 13 6   \u2502\n\u2502 Station 5 \u2506 14 8 E0 16 22 24 E1             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/#programmatically-creating-lists","title":"Programmatically creating lists","text":"<p>Given the dataframe <code>weather</code> defined previously, it is very likely we need to run some analysis on the temperatures that are captured by each station. To make this happen, we need to first be able to get individual temperature measurements. We can use the namespace <code>str</code> for this:</p>  Python Rust <p> <code>str.split</code> <pre><code>weather = weather.with_columns(\n    pl.col(\"temperatures\").str.split(\" \"),\n)\nprint(weather)\n</code></pre></p> <p> <code>str.split</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures         \u2502\n\u2502 ---       \u2506 ---                  \u2502\n\u2502 str       \u2506 list[str]            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 [\"20\", \"5\", \u2026 \"20\"]  \u2502\n\u2502 Station 2 \u2506 [\"18\", \"8\", \u2026 \"40\"]  \u2502\n\u2502 Station 3 \u2506 [\"19\", \"24\", \u2026 \"22\"] \u2502\n\u2502 Station 4 \u2506 [\"E2\", \"E0\", \u2026 \"6\"]  \u2502\n\u2502 Station 5 \u2506 [\"14\", \"8\", \u2026 \"E1\"]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>A natural follow-up would be to explode the list of temperatures so that each measurement is in its own row:</p>  Python Rust <p> <code>explode</code> <pre><code>result = weather.explode(\"temperatures\")\nprint(result)\n</code></pre></p> <p> <code>explode</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (49, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures \u2502\n\u2502 ---       \u2506 ---          \u2502\n\u2502 str       \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20           \u2502\n\u2502 Station 1 \u2506 5            \u2502\n\u2502 Station 1 \u2506 5            \u2502\n\u2502 Station 1 \u2506 E1           \u2502\n\u2502 Station 1 \u2506 7            \u2502\n\u2502 \u2026         \u2506 \u2026            \u2502\n\u2502 Station 5 \u2506 E0           \u2502\n\u2502 Station 5 \u2506 16           \u2502\n\u2502 Station 5 \u2506 22           \u2502\n\u2502 Station 5 \u2506 24           \u2502\n\u2502 Station 5 \u2506 E1           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, in Polars we often do not need to do this to operate on the list elements.</p>"},{"location":"user-guide/expressions/lists-and-arrays/#operating-on-lists","title":"Operating on lists","text":"<p>Polars provides several standard operations on columns with the <code>List</code> data type. Similar to what you can do with strings, lists can be sliced with the functions <code>head</code>, <code>tail</code>, and <code>slice</code>:</p>  Python Rust <p> <code>list namespace</code> <pre><code>result = weather.with_columns(\n    pl.col(\"temperatures\").list.head(3).alias(\"head\"),\n    pl.col(\"temperatures\").list.tail(3).alias(\"tail\"),\n    pl.col(\"temperatures\").list.slice(-3, 2).alias(\"two_next_to_last\"),\n)\nprint(result)\n</code></pre></p> <p> <code>list namespace</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures         \u2506 head               \u2506 tail               \u2506 two_next_to_last \u2502\n\u2502 ---       \u2506 ---                  \u2506 ---                \u2506 ---                \u2506 ---              \u2502\n\u2502 str       \u2506 list[str]            \u2506 list[str]          \u2506 list[str]          \u2506 list[str]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 [\"20\", \"5\", \u2026 \"20\"]  \u2506 [\"20\", \"5\", \"5\"]   \u2506 [\"9\", \"6\", \"20\"]   \u2506 [\"9\", \"6\"]       \u2502\n\u2502 Station 2 \u2506 [\"18\", \"8\", \u2026 \"40\"]  \u2506 [\"18\", \"8\", \"16\"]  \u2506 [\"90\", \"70\", \"40\"] \u2506 [\"90\", \"70\"]     \u2502\n\u2502 Station 3 \u2506 [\"19\", \"24\", \u2026 \"22\"] \u2506 [\"19\", \"24\", \"E9\"] \u2506 [\"12\", \"10\", \"22\"] \u2506 [\"12\", \"10\"]     \u2502\n\u2502 Station 4 \u2506 [\"E2\", \"E0\", \u2026 \"6\"]  \u2506 [\"E2\", \"E0\", \"15\"] \u2506 [\"17\", \"13\", \"6\"]  \u2506 [\"17\", \"13\"]     \u2502\n\u2502 Station 5 \u2506 [\"14\", \"8\", \u2026 \"E1\"]  \u2506 [\"14\", \"8\", \"E0\"]  \u2506 [\"22\", \"24\", \"E1\"] \u2506 [\"22\", \"24\"]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/#element-wise-computation-within-lists","title":"Element-wise computation within lists","text":"<p>If we need to identify the stations that are giving the most number of errors we need to</p> <ol> <li>try to convert the measurements into numbers;</li> <li>count the number of non-numeric values (i.e., <code>null</code> values) in the list, by row; and</li> <li>rename this output column as \u201cerrors\u201d so that we can easily identify the stations.</li> </ol> <p>To perform these steps, we need to perform a casting operation on each measurement within the list values. The function <code>eval</code> is used as the entry point to perform operations on the elements of the list. Within it, you can use the context <code>element</code> to refer to each single element of the list individually, and then you can use any Polars expression on the element:</p>  Python Rust <p> <code>element</code> <pre><code>result = weather.with_columns(\n    pl.col(\"temperatures\")\n    .list.eval(pl.element().cast(pl.Int64, strict=False).is_null())\n    .list.sum()\n    .alias(\"errors\"),\n)\nprint(result)\n</code></pre></p> <p> <code>element</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures         \u2506 errors \u2502\n\u2502 ---       \u2506 ---                  \u2506 ---    \u2502\n\u2502 str       \u2506 list[str]            \u2506 u32    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 [\"20\", \"5\", \u2026 \"20\"]  \u2506 1      \u2502\n\u2502 Station 2 \u2506 [\"18\", \"8\", \u2026 \"40\"]  \u2506 4      \u2502\n\u2502 Station 3 \u2506 [\"19\", \"24\", \u2026 \"22\"] \u2506 1      \u2502\n\u2502 Station 4 \u2506 [\"E2\", \"E0\", \u2026 \"6\"]  \u2506 3      \u2502\n\u2502 Station 5 \u2506 [\"14\", \"8\", \u2026 \"E1\"]  \u2506 2      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Another alternative would be to use a regular expression to check if a measurement starts with a letter:</p>  Python Rust <p> <code>element</code> <pre><code>result2 = weather.with_columns(\n    pl.col(\"temperatures\")\n    .list.eval(pl.element().str.contains(\"(?i)[a-z]\"))\n    .list.sum()\n    .alias(\"errors\"),\n)\nprint(result.equals(result2))\n</code></pre></p> <p> <code>element</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>True\n</code></pre> <p>If you are unfamiliar with the namespace <code>str</code> or the notation <code>(?i)</code> in the regex, now is a good time to look at how to work with strings and regular expressions in Polars.</p>"},{"location":"user-guide/expressions/lists-and-arrays/#aggregation-sorting","title":"Aggregation &amp; sorting","text":"<p>Like <code>select</code> on data frames, the two related functions <code>eval</code> and <code>agg</code> can also be used to aggregate over or sort the list elements.</p> <p>We'll reuse a slightly modified version of the example data from the very beginning:</p>  Python Rust <p> <code>List</code> <pre><code>df = pl.DataFrame(\n    {\n        \"children\": [\n            [\n                {\"name\": \"Anne\", \"age\": 5},\n                {\"name\": \"Averill\", \"age\": 7},\n            ],\n            [\n                {\"name\": \"Brandon\", \"age\": 12},\n                {\"name\": \"Brooke\", \"age\": 9},\n                {\"name\": \"Branson\", \"age\": 11},\n            ],\n            [{\"name\": \"Camila\", \"age\": 19}],\n            [\n                {\"name\": \"Dennis\", \"age\": 8},\n                {\"name\": \"Doyle\", \"age\": 11},\n                {\"name\": \"Dina\", \"age\": 18},\n            ],\n        ],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>List</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 children                        \u2502\n\u2502 ---                             \u2502\n\u2502 list[struct[2]]                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [{\"Anne\",5}, {\"Averill\",7}]     \u2502\n\u2502 [{\"Brandon\",12}, {\"Brooke\",9},\u2026 \u2502\n\u2502 [{\"Camila\",19}]                 \u2502\n\u2502 [{\"Dennis\",8}, {\"Doyle\",11}, {\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Using <code>eval</code>, we can sort the list elements or compute some aggregations:</p>  Python Rust <p> <code>list.eval</code> <pre><code>result = df.select(\n    pl.col(\"children\")\n    .list.eval(\n        pl.element()\n        .sort_by(pl.element().struct.field(\"age\"), descending=True)\n        .struct.field(\"name\")\n    )\n    .alias(\"names_by_age\"),\n    pl.col(\"children\")\n    .list.eval(pl.element().struct.field(\"age\").min())\n    .alias(\"min_age\"),\n    pl.col(\"children\")\n    .list.eval(pl.element().struct.field(\"age\").max())\n    .alias(\"max_age\"),\n)\nprint(result)\n</code></pre></p> <p> <code>list.eval</code> \u00b7  Available on feature list_eval <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 names_by_age                    \u2506 min_age   \u2506 max_age   \u2502\n\u2502 ---                             \u2506 ---       \u2506 ---       \u2502\n\u2502 list[str]                       \u2506 list[i64] \u2506 list[i64] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"Averill\", \"Anne\"]             \u2506 [5]       \u2506 [7]       \u2502\n\u2502 [\"Brandon\", \"Branson\", \"Brooke\u2026 \u2506 [9]       \u2506 [12]      \u2502\n\u2502 [\"Camila\"]                      \u2506 [19]      \u2506 [19]      \u2502\n\u2502 [\"Dina\", \"Doyle\", \"Dennis\"]     \u2506 [8]       \u2506 [18]      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p><code>eval</code> will always return a list. Use <code>agg</code> to get <code>min_age</code> and <code>max_age</code> as scalar values instead of single-element lists:</p>  Python Rust <pre><code>result = df.select(\n    pl.col(\"children\")\n    .list.eval(\n        pl.element()\n        .sort_by(pl.element().struct.field(\"age\"), descending=True)\n        .struct.field(\"name\")\n    )\n    .alias(\"names_by_age\"),\n    pl.col(\"children\")\n    .list.agg(pl.element().struct.field(\"age\").min())\n    .alias(\"min_age\"),\n    pl.col(\"children\")\n    .list.agg(pl.element().struct.field(\"age\").max())\n    .alias(\"max_age\"),\n)\nprint(result)\n</code></pre> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 names_by_age                    \u2506 min_age \u2506 max_age \u2502\n\u2502 ---                             \u2506 ---     \u2506 ---     \u2502\n\u2502 list[str]                       \u2506 i64     \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"Averill\", \"Anne\"]             \u2506 5       \u2506 7       \u2502\n\u2502 [\"Brandon\", \"Branson\", \"Brooke\u2026 \u2506 9       \u2506 12      \u2502\n\u2502 [\"Camila\"]                      \u2506 19      \u2506 19      \u2502\n\u2502 [\"Dina\", \"Doyle\", \"Dennis\"]     \u2506 8       \u2506 18      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If the evaluated expression is statically determined to return only one value, <code>agg</code> will automatically explode the resulting list into the inner values. This matches what <code>df.group_by(...).agg(...)</code> does, hence the name. This is in contrast with <code>eval</code>, which will not perform such unwrapping.</p> <p>While some aggregation functions like <code>.list.sum()</code> are directly available in the <code>list</code> namespace, you can access more exotic aggregations like <code>entropy</code> via <code>agg</code>/<code>eval</code> only:</p>  Python Rust <pre><code>result = df.with_columns(\n    pl.col(\"children\")\n    .list.agg(pl.element().struct.field(\"age\").entropy())\n    .alias(\"age_entropy\"),\n)\nprint(result)\n</code></pre> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 children                        \u2506 age_entropy \u2502\n\u2502 ---                             \u2506 ---         \u2502\n\u2502 list[struct[2]]                 \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [{\"Anne\",5}, {\"Averill\",7}]     \u2506 0.679193    \u2502\n\u2502 [{\"Brandon\",12}, {\"Brooke\",9},\u2026 \u2506 1.09165     \u2502\n\u2502 [{\"Camila\",19}]                 \u2506 0.0         \u2502\n\u2502 [{\"Dennis\",8}, {\"Doyle\",11}, {\u2026 \u2506 1.042294    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/#row-wise-computations","title":"Row-wise computations","text":"<p><code>pl.all()</code> can be combined with <code>pl.concat_list(...)</code> to perform row-wise aggregations over a subset of columns.</p> <p>To show this in action, we will start by creating another dataframe with some more weather data:</p>  Python Rust <pre><code>weather_by_day = pl.DataFrame(\n    {\n        \"station\": [f\"Station {idx}\" for idx in range(1, 11)],\n        \"day_1\": [17, 11, 8, 22, 9, 21, 20, 8, 8, 17],\n        \"day_2\": [15, 11, 10, 8, 7, 14, 18, 21, 15, 13],\n        \"day_3\": [16, 15, 24, 24, 8, 23, 19, 23, 16, 10],\n    }\n)\nprint(weather_by_day)\n</code></pre> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station    \u2506 day_1 \u2506 day_2 \u2506 day_3 \u2502\n\u2502 ---        \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 str        \u2506 i64   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1  \u2506 17    \u2506 15    \u2506 16    \u2502\n\u2502 Station 2  \u2506 11    \u2506 11    \u2506 15    \u2502\n\u2502 Station 3  \u2506 8     \u2506 10    \u2506 24    \u2502\n\u2502 Station 4  \u2506 22    \u2506 8     \u2506 24    \u2502\n\u2502 Station 5  \u2506 9     \u2506 7     \u2506 8     \u2502\n\u2502 Station 6  \u2506 21    \u2506 14    \u2506 23    \u2502\n\u2502 Station 7  \u2506 20    \u2506 18    \u2506 19    \u2502\n\u2502 Station 8  \u2506 8     \u2506 21    \u2506 23    \u2502\n\u2502 Station 9  \u2506 8     \u2506 15    \u2506 16    \u2502\n\u2502 Station 10 \u2506 17    \u2506 13    \u2506 10    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Now, we will calculate the percentage rank of the temperatures by day, measured across stations. Polars does not provide a function to do this directly, but because expressions are so versatile we can create our own percentage rank expression for highest temperature. Let's try that:</p>  Python Rust <p> <code>element</code> \u00b7 <code>rank</code> <pre><code>rank_pct = (pl.element().rank(descending=True) / pl.element().count()).round(2)\n\nresult = weather_by_day.with_columns(\n    # create the list of homogeneous data\n    pl.concat_list(pl.all().exclude(\"station\")).alias(\"all_temps\")\n).select(\n    # select all columns except the intermediate list\n    pl.all().exclude(\"all_temps\"),\n    # compute the rank by calling `list.eval`\n    pl.col(\"all_temps\").list.eval(rank_pct, parallel=True).alias(\"temps_rank\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>element</code> \u00b7 <code>rank</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (10, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station    \u2506 day_1 \u2506 day_2 \u2506 day_3 \u2506 temps_rank         \u2502\n\u2502 ---        \u2506 ---   \u2506 ---   \u2506 ---   \u2506 ---                \u2502\n\u2502 str        \u2506 i64   \u2506 i64   \u2506 i64   \u2506 list[f64]          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1  \u2506 17    \u2506 15    \u2506 16    \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 2  \u2506 11    \u2506 11    \u2506 15    \u2506 [0.83, 0.83, 0.33] \u2502\n\u2502 Station 3  \u2506 8     \u2506 10    \u2506 24    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 4  \u2506 22    \u2506 8     \u2506 24    \u2506 [0.67, 1.0, 0.33]  \u2502\n\u2502 Station 5  \u2506 9     \u2506 7     \u2506 8     \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 6  \u2506 21    \u2506 14    \u2506 23    \u2506 [0.67, 1.0, 0.33]  \u2502\n\u2502 Station 7  \u2506 20    \u2506 18    \u2506 19    \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 8  \u2506 8     \u2506 21    \u2506 23    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 9  \u2506 8     \u2506 15    \u2506 16    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 10 \u2506 17    \u2506 13    \u2506 10    \u2506 [0.33, 0.67, 1.0]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists-and-arrays/#working-with-arrays","title":"Working with arrays","text":""},{"location":"user-guide/expressions/lists-and-arrays/#creating-an-array-column","title":"Creating an array column","text":"<p>As we have seen above, Polars usually does not infer the data type <code>Array</code> automatically. You have to specify the data type <code>Array</code> when creating a series/dataframe or cast a column explicitly unless you create the column out of a NumPy array.</p>"},{"location":"user-guide/expressions/lists-and-arrays/#the-namespace-arr","title":"The namespace <code>arr</code>","text":"<p>The data type <code>Array</code> was recently introduced and is still pretty nascent in features that it offers. Even so, the namespace <code>arr</code> aggregates several functions that you can use to work with arrays.</p> <p><code>arr</code> then, <code>list</code> now</p> <p>In previous versions of Polars, the namespace for list operations used to be <code>arr</code>. <code>arr</code> is now the namespace for the data type <code>Array</code>. If you find references to the namespace <code>arr</code> on StackOverflow or other sources, note that those sources may be outdated.</p> <p>The API documentation should give you a good overview of the functions in the namespace <code>arr</code>, of which we present a couple:</p>  Python Rust <p> <code>arr namespace</code> <pre><code>df = pl.DataFrame(\n    {\n        \"first_last\": [\n            [\"Anne\", \"Adams\"],\n            [\"Brandon\", \"Branson\"],\n            [\"Camila\", \"Campbell\"],\n            [\"Dennis\", \"Doyle\"],\n        ],\n        \"fav_numbers\": [\n            [42, 0, 1],\n            [2, 3, 5],\n            [13, 21, 34],\n            [73, 3, 7],\n        ],\n    },\n    schema={\n        \"first_last\": pl.Array(pl.String, 2),\n        \"fav_numbers\": pl.Array(pl.Int32, 3),\n    },\n)\n\nresult = df.select(\n    pl.col(\"first_last\").arr.join(\" \").alias(\"name\"),\n    pl.col(\"fav_numbers\").arr.sort(),\n    pl.col(\"fav_numbers\").arr.max().alias(\"largest_fav\"),\n    pl.col(\"fav_numbers\").arr.sum().alias(\"summed\"),\n    pl.col(\"fav_numbers\").arr.contains(3).alias(\"likes_3\"),\n)\nprint(result)\n</code></pre></p> <p> <code>`arr</code> namespace` \u00b7  Available on feature dtype-array <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name            \u2506 fav_numbers   \u2506 largest_fav \u2506 summed \u2506 likes_3 \u2502\n\u2502 ---             \u2506 ---           \u2506 ---         \u2506 ---    \u2506 ---     \u2502\n\u2502 str             \u2506 array[i32, 3] \u2506 i32         \u2506 i32    \u2506 bool    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Anne Adams      \u2506 [0, 1, 42]    \u2506 42          \u2506 43     \u2506 false   \u2502\n\u2502 Brandon Branson \u2506 [2, 3, 5]     \u2506 5           \u2506 10     \u2506 true    \u2502\n\u2502 Camila Campbell \u2506 [13, 21, 34]  \u2506 34          \u2506 68     \u2506 false   \u2502\n\u2502 Dennis Doyle    \u2506 [3, 7, 73]    \u2506 73          \u2506 83     \u2506 true    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/","title":"Missing data","text":"<p>This section of the user guide teaches how to work with missing data in Polars.</p>"},{"location":"user-guide/expressions/missing-data/#null-and-nan-values","title":"<code>null</code> and <code>NaN</code> values","text":"<p>In Polars, missing data is represented by the value <code>null</code>. This missing value <code>null</code> is used for all data types, including numerical types.</p> <p>Polars also supports the value <code>NaN</code> (\u201cNot a Number\u201d) for columns with floating point numbers. The value <code>NaN</code> is considered to be a valid floating point value, which is different from missing data. We discuss the value <code>NaN</code> separately below.</p> <p>When creating a series or a dataframe, you can set a value to <code>null</code> by using the appropriate construct for your language:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"value\": [1, None],\n    },\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use polars::prelude::*;\nlet df = df! (\n    \"value\" =&gt; &amp;[Some(1), None],\n)?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2502 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Difference from pandas</p> <p>In pandas, the value used to represent missing data depends on the data type of the column. In Polars, missing data is always represented by the value <code>null</code>.</p>"},{"location":"user-guide/expressions/missing-data/#missing-data-metadata","title":"Missing data metadata","text":"<p>Polars keeps track of some metadata regarding the missing data of each series. This metadata allows Polars to answer some basic queries about missing values in a very efficient way, namely how many values are missing and which ones are missing.</p> <p>To determine how many values are missing from a column you can use the function <code>null_count</code>:</p>  Python Rust <p> <code>null_count</code> <pre><code>null_count_df = df.null_count()\nprint(null_count_df)\n</code></pre></p> <p> <code>null_count</code> <pre><code>let null_count_df = df.null_count();\nprintln!(\"{null_count_df}\");\n</code></pre></p> <pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The function <code>null_count</code> can be called on a dataframe, a column from a dataframe, or on a series directly. The function <code>null_count</code> is a cheap operation because the result is already known.</p> <p>Polars uses something called a \u201cvalidity bitmap\u201d to know which values are missing in a series. The validity bitmap is memory efficient as it is bit encoded. If a series has length \\(n\\), then its validity bitmap will cost \\(n / 8\\) bytes. The function <code>is_null</code> uses the validity bitmap to efficiently report which values are <code>null</code> and which are not:</p>  Python Rust <p> <code>is_null</code> <pre><code>is_null_series = df.select(\n    pl.col(\"value\").is_null(),\n)\nprint(is_null_series)\n</code></pre></p> <p> <code>is_null</code> <pre><code>let is_null_series = df.lazy().select([col(\"value\").is_null()]).collect()?;\nprintln!(\"{is_null_series}\");\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 bool  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false \u2502\n\u2502 true  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The function <code>is_null</code> can be used on a column of a dataframe or on a series directly. Again, this is a cheap operation because the result is already known by Polars.</p> Why does Polars waste memory on a validity bitmap? <p>It all comes down to a tradeoff. By using a bit more memory per column, Polars can be much more efficient when performing most operations on your columns. If the validity bitmap wasn't known, every time you wanted to compute something you would have to check each position of the series to see if a legal value was present or not. With the validity bitmap, Polars knows automatically the positions where your operations can be applied.</p>"},{"location":"user-guide/expressions/missing-data/#filling-missing-data","title":"Filling missing data","text":"<p>Missing data in a series can be filled with the function <code>fill_null</code>. You can specify how missing data is effectively filled in a couple of different ways:</p> <ul> <li>a literal of the correct data type;</li> <li>a Polars expression, such as replacing with values computed from another column;</li> <li>a strategy based on neighbouring values, such as filling forwards or backwards; and</li> <li>interpolation.</li> </ul> <p>To illustrate how each of these methods work we start by defining a simple dataframe with two missing values in the second column:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"col1\": [0.5, 1, 1.5, 2, 2.5],\n        \"col2\": [1, None, 3, None, 5],\n    },\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df! (\n    \"col1\" =&gt; [0.5, 1.0, 1.5, 2.0, 2.5],\n    \"col2\" =&gt; [Some(1), None, Some(3), None, Some(5)],\n)?;\n\nprintln!(\"{df}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 f64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5  \u2506 1    \u2502\n\u2502 1.0  \u2506 null \u2502\n\u2502 1.5  \u2506 3    \u2502\n\u2502 2.0  \u2506 null \u2502\n\u2502 2.5  \u2506 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/#fill-with-a-specified-literal-value","title":"Fill with a specified literal value","text":"<p>You can fill the missing data with a specified literal value. This literal value will replace all of the occurrences of the value <code>null</code>:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_literal_df = df.with_columns(\n    pl.col(\"col2\").fill_null(3),\n)\nprint(fill_literal_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_literal_df = df\n    .clone()\n    .lazy()\n    .with_column(col(\"col2\").fill_null(3))\n    .collect()?;\n\nprintln!(\"{fill_literal_df}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 f64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5  \u2506 1    \u2502\n\u2502 1.0  \u2506 3    \u2502\n\u2502 1.5  \u2506 3    \u2502\n\u2502 2.0  \u2506 3    \u2502\n\u2502 2.5  \u2506 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, this is actually just a special case of the general case where the function <code>fill_null</code> replaces missing values with the corresponding values from the result of a Polars expression, as seen next.</p>"},{"location":"user-guide/expressions/missing-data/#fill-with-an-expression","title":"Fill with an expression","text":"<p>In the general case, the missing data can be filled by extracting the corresponding values from the result of a general Polars expression. For example, we can fill the second column with values taken from the double of the first column:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_expression_df = df.with_columns(\n    pl.col(\"col2\").fill_null((2 * pl.col(\"col1\")).cast(pl.Int64)),\n)\nprint(fill_expression_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_expression_df = df\n    .clone()\n    .lazy()\n    .with_column(col(\"col2\").fill_null((lit(2) * col(\"col1\")).cast(DataType::Int64)))\n    .collect()?;\n\nprintln!(\"{fill_expression_df}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 f64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5  \u2506 1    \u2502\n\u2502 1.0  \u2506 2    \u2502\n\u2502 1.5  \u2506 3    \u2502\n\u2502 2.0  \u2506 4    \u2502\n\u2502 2.5  \u2506 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/#fill-with-a-strategy-based-on-neighbouring-values","title":"Fill with a strategy based on neighbouring values","text":"<p>You can also fill the missing data by following a fill strategy based on the neighbouring values. The two simpler strategies look for the first non-<code>null</code> value that comes immediately before or immediately after the value <code>null</code> that is being filled:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_forward_df = df.with_columns(\n    pl.col(\"col2\").fill_null(strategy=\"forward\").alias(\"forward\"),\n    pl.col(\"col2\").fill_null(strategy=\"backward\").alias(\"backward\"),\n)\nprint(fill_forward_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_literal_df = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"col2\")\n            .fill_null_with_strategy(FillNullStrategy::Forward(None))\n            .alias(\"forward\"),\n        col(\"col2\")\n            .fill_null_with_strategy(FillNullStrategy::Backward(None))\n            .alias(\"backward\"),\n    ])\n    .collect()?;\n\nprintln!(\"{fill_literal_df}\");\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2506 forward \u2506 backward \u2502\n\u2502 ---  \u2506 ---  \u2506 ---     \u2506 ---      \u2502\n\u2502 f64  \u2506 i64  \u2506 i64     \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5  \u2506 1    \u2506 1       \u2506 1        \u2502\n\u2502 1.0  \u2506 null \u2506 1       \u2506 3        \u2502\n\u2502 1.5  \u2506 3    \u2506 3       \u2506 3        \u2502\n\u2502 2.0  \u2506 null \u2506 3       \u2506 5        \u2502\n\u2502 2.5  \u2506 5    \u2506 5       \u2506 5        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can find other fill strategies in the API docs.</p>"},{"location":"user-guide/expressions/missing-data/#fill-with-interpolation","title":"Fill with interpolation","text":"<p>Additionally, you can fill intermediate missing data with interpolation by using the function <code>interpolate</code> instead of the function <code>fill_null</code>:</p>  Python Rust <p> <code>interpolate</code> <pre><code>fill_interpolation_df = df.with_columns(\n    pl.col(\"col2\").interpolate(),\n)\nprint(fill_interpolation_df)\n</code></pre></p> <p> <code>interpolate</code> <pre><code>let fill_interpolation_df = df\n    .lazy()\n    .with_column(col(\"col2\").interpolate(InterpolationMethod::Linear))\n    .collect()?;\n\nprintln!(\"{fill_interpolation_df}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 f64  \u2506 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5  \u2506 1.0  \u2502\n\u2502 1.0  \u2506 2.0  \u2502\n\u2502 1.5  \u2506 3.0  \u2502\n\u2502 2.0  \u2506 4.0  \u2502\n\u2502 2.5  \u2506 5.0  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note: With interpolate, nulls at the beginning and end of the series remain null.</p>"},{"location":"user-guide/expressions/missing-data/#not-a-number-or-nan-values","title":"Not a Number, or <code>NaN</code> values","text":"<p>Missing data in a series is only ever represented by the value <code>null</code>, regardless of the data type of the series. Columns with a floating point data type can sometimes have the value <code>NaN</code>, which might be confused with <code>null</code>.</p> <p>The special value <code>NaN</code> can be created directly:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import numpy as np\n\nnan_df = pl.DataFrame(\n    {\n        \"value\": [1.0, np.nan, float(\"nan\"), 3.0],\n    },\n)\nprint(nan_df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let nan_df = df!(\n    \"value\" =&gt; [1.0, f64::NAN, f64::NAN, 3.0],\n)?;\nprintln!(\"{nan_df}\");\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0   \u2502\n\u2502 NaN   \u2502\n\u2502 NaN   \u2502\n\u2502 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>And it might also arise as the result of a computation:</p>  Python Rust <pre><code>df = pl.DataFrame(\n    {\n        \"dividend\": [1, 0, -1],\n        \"divisor\": [1, 0, -1],\n    }\n)\nresult = df.select(pl.col(\"dividend\") / pl.col(\"divisor\"))\nprint(result)\n</code></pre> <pre><code>let df = df!(\n    \"dividend\" =&gt; [1.0, 0.0, -1.0],\n    \"divisor\" =&gt; [1.0, 0.0, -1.0],\n)?;\n\nlet result = df\n    .lazy()\n    .select([col(\"dividend\") / col(\"divisor\")])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dividend \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0      \u2502\n\u2502 NaN      \u2502\n\u2502 1.0      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>By default, a <code>NaN</code> value in an integer column causes the column to be cast to a float data type in pandas. This does not happen in Polars; instead, an exception is raised.</p> <p><code>NaN</code> values are considered to be a type of floating point data and are not considered to be missing data in Polars. This means:</p> <ul> <li><code>NaN</code> values are not counted with the function <code>null_count</code>; and</li> <li><code>NaN</code> values are filled when you use the specialised function <code>fill_nan</code> method but are not   filled with the function <code>fill_null</code>.</li> </ul> <p>Polars has the functions <code>is_nan</code> and <code>fill_nan</code>, which work in a similar way to the functions <code>is_null</code> and <code>fill_null</code>. Unlike with missing data, Polars does not hold any metadata regarding the <code>NaN</code> values, so the function <code>is_nan</code> entails actual computation.</p> <p>One further difference between the values <code>null</code> and <code>NaN</code> is that numerical aggregating functions, like <code>mean</code> and <code>sum</code>, skip the missing values when computing the result, whereas the value <code>NaN</code> is considered for the computation and typically propagates into the result. If desirable, this behavior can be avoided by replacing the occurrences of the value <code>NaN</code> with the value <code>null</code>:</p>  Python Rust <p> <code>fill_nan</code> <pre><code>mean_nan_df = nan_df.with_columns(\n    pl.col(\"value\").fill_nan(None).alias(\"replaced\"),\n).select(\n    pl.all().mean().name.suffix(\"_mean\"),\n    pl.all().sum().name.suffix(\"_sum\"),\n)\nprint(mean_nan_df)\n</code></pre></p> <p> <code>fill_nan</code> <pre><code>let mean_nan_df = nan_df\n    .lazy()\n    .with_column(col(\"value\").fill_nan(Null {}.lit()).alias(\"replaced\"))\n    .select([\n        col(\"*\").mean().name().suffix(\"_mean\"),\n        col(\"*\").sum().name().suffix(\"_sum\"),\n    ])\n    .collect()?;\n\nprintln!(\"{mean_nan_df}\");\n</code></pre></p> <pre><code>shape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value_mean \u2506 replaced_mean \u2506 value_sum \u2506 replaced_sum \u2502\n\u2502 ---        \u2506 ---           \u2506 ---       \u2506 ---          \u2502\n\u2502 f64        \u2506 f64           \u2506 f64       \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NaN        \u2506 2.0           \u2506 NaN       \u2506 4.0          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can learn more about the value <code>NaN</code> in the section about floating point number data types.</p>"},{"location":"user-guide/expressions/numpy-functions/","title":"Numpy functions","text":"<p>Polars expressions support NumPy ufuncs. See the NumPy documentation for a list of all supported NumPy functions.</p> <p>This means that if a function is not provided by Polars, we can use NumPy and we still have fast columnar operations through the NumPy API.</p>"},{"location":"user-guide/expressions/numpy-functions/#example","title":"Example","text":"Python <p> <code>DataFrame</code> \u00b7 <code>log</code> \u00b7  Available on feature numpy <pre><code>import polars as pl\nimport numpy as np\n\ndf = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n\nout = df.select(np.log(pl.all()).name.suffix(\"_log\"))\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a_log    \u2506 b_log    \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.0      \u2506 1.386294 \u2502\n\u2502 0.693147 \u2506 1.609438 \u2502\n\u2502 1.098612 \u2506 1.791759 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/numpy-functions/#interoperability","title":"Interoperability","text":"<p>Polars' series have support for NumPy universal functions (ufuncs) and generalized ufuncs. Element-wise functions such as <code>np.exp</code>, <code>np.cos</code>, <code>np.div</code>, etc, all work with almost zero overhead.</p> <p>However, bear in mind that Polars keeps track of missing values with a separate bitmask and NumPy does not receive this information. This can lead to a window function or a <code>np.convolve</code> giving flawed or incomplete results, so an error will be raised if you pass a series with missing data to a generalized ufunc. Convert a Polars series to a NumPy array with the function <code>to_numpy</code>. Missing values will be replaced by <code>np.nan</code> during the conversion.</p>"},{"location":"user-guide/expressions/strings/","title":"Strings","text":"<p>The following section discusses operations performed on string data, which is a frequently used data type when working with dataframes. String processing functions are available in the namespace <code>str</code>.</p> <p>Working with strings in other dataframe libraries can be highly inefficient due to the fact that strings have unpredictable lengths. Polars mitigates these inefficiencies by following the Arrow Columnar Format specification, so you can write performant data queries on string data too.</p>"},{"location":"user-guide/expressions/strings/#the-string-namespace","title":"The string namespace","text":"<p>When working with string data you will likely need to access the namespace <code>str</code>, which aggregates 40+ functions that let you work with strings. As an example of how to access functions from within that namespace, the snippet below shows how to compute the length of the strings in a column in terms of the number of bytes and the number of characters:</p>  Python Rust <p> <code>str.len_bytes</code> \u00b7 <code>str.len_chars</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"language\": [\"English\", \"Dutch\", \"Portuguese\", \"Finish\"],\n        \"fruit\": [\"pear\", \"peer\", \"p\u00eara\", \"p\u00e4\u00e4ryn\u00e4\"],\n    }\n)\n\nresult = df.with_columns(\n    pl.col(\"fruit\").str.len_bytes().alias(\"byte_count\"),\n    pl.col(\"fruit\").str.len_chars().alias(\"letter_count\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.len_bytes</code> \u00b7 <code>str.len_chars</code> <pre><code>use polars::prelude::*;\n\nlet df = df! (\n    \"language\" =&gt; [\"English\", \"Dutch\", \"Portuguese\", \"Finish\"],\n    \"fruit\" =&gt; [\"pear\", \"peer\", \"p\u00eara\", \"p\u00e4\u00e4ryn\u00e4\"],\n)?;\n\nlet result = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"fruit\").str().len_bytes().alias(\"byte_count\"),\n        col(\"fruit\").str().len_chars().alias(\"letter_count\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 language   \u2506 fruit   \u2506 byte_count \u2506 letter_count \u2502\n\u2502 ---        \u2506 ---     \u2506 ---        \u2506 ---          \u2502\n\u2502 str        \u2506 str     \u2506 u32        \u2506 u32          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 English    \u2506 pear    \u2506 4          \u2506 4            \u2502\n\u2502 Dutch      \u2506 peer    \u2506 4          \u2506 4            \u2502\n\u2502 Portuguese \u2506 p\u00eara    \u2506 5          \u2506 4            \u2502\n\u2502 Finish     \u2506 p\u00e4\u00e4ryn\u00e4 \u2506 10         \u2506 7            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note</p> <p>If you are working exclusively with ASCII text, then the results of the two computations will be the same and using <code>len_bytes</code> is recommended since it is faster.</p>"},{"location":"user-guide/expressions/strings/#parsing-strings","title":"Parsing strings","text":"<p>Polars offers multiple methods for checking and parsing elements of a string column, namely checking for the existence of given substrings or patterns, and counting, extracting, or replacing, them. We will demonstrate some of these operations in the upcoming examples.</p>"},{"location":"user-guide/expressions/strings/#check-for-the-existence-of-a-pattern","title":"Check for the existence of a pattern","text":"<p>We can use the function <code>contains</code> to check for the presence of a pattern within a string. By default, the argument to the function <code>contains</code> is interpreted as a regular expression. If you want to specify a literal substring, set the parameter <code>literal</code> to <code>True</code>.</p> <p>For the special cases where you want to check if the strings start or end with a fixed substring, you can use the functions <code>starts_with</code> or <code>ends_with</code>, respectively.</p>  Python Rust <p> <code>str.contains</code> \u00b7 <code>str.starts_with</code> \u00b7 <code>str.ends_with</code> <pre><code>result = df.select(\n    pl.col(\"fruit\"),\n    pl.col(\"fruit\").str.starts_with(\"p\").alias(\"starts_with_p\"),\n    pl.col(\"fruit\").str.contains(\"p..r\").alias(\"p..r\"),\n    pl.col(\"fruit\").str.contains(\"e+\").alias(\"e+\"),\n    pl.col(\"fruit\").str.ends_with(\"r\").alias(\"ends_with_r\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.contains</code> \u00b7 <code>str.starts_with</code> \u00b7 <code>str.ends_with</code> \u00b7  Available on feature regex <pre><code>let result = df\n    .lazy()\n    .select([\n        col(\"fruit\"),\n        col(\"fruit\")\n            .str()\n            .starts_with(lit(\"p\"))\n            .alias(\"starts_with_p\"),\n        col(\"fruit\").str().contains(lit(\"p..r\"), true).alias(\"p..r\"),\n        col(\"fruit\").str().contains(lit(\"e+\"), true).alias(\"e+\"),\n        col(\"fruit\").str().ends_with(lit(\"r\")).alias(\"ends_with_r\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruit   \u2506 starts_with_p \u2506 p..r  \u2506 e+    \u2506 ends_with_r \u2502\n\u2502 ---     \u2506 ---           \u2506 ---   \u2506 ---   \u2506 ---         \u2502\n\u2502 str     \u2506 bool          \u2506 bool  \u2506 bool  \u2506 bool        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 pear    \u2506 true          \u2506 true  \u2506 true  \u2506 true        \u2502\n\u2502 peer    \u2506 true          \u2506 true  \u2506 true  \u2506 true        \u2502\n\u2502 p\u00eara    \u2506 true          \u2506 false \u2506 false \u2506 false       \u2502\n\u2502 p\u00e4\u00e4ryn\u00e4 \u2506 true          \u2506 true  \u2506 false \u2506 false       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#regex-specification","title":"Regex specification","text":"<p>Polars relies on the Rust crate <code>regex</code> to work with regular expressions, so you may need to refer to the syntax documentation to see what features and flags are supported. In particular, note that the flavor of regex supported by Polars is different from Python's module <code>re</code>.</p>"},{"location":"user-guide/expressions/strings/#extract-a-pattern","title":"Extract a pattern","text":"<p>The function <code>extract</code> allows us to extract patterns from the string values in a column. The function <code>extract</code> accepts a regex pattern with one or more capture groups and extracts the capture group specified as the second argument.</p>  Python Rust <p> <code>str.extract</code> <pre><code>df = pl.DataFrame(\n    {\n        \"urls\": [\n            \"http://vote.com/ballon_dor?candidate=messi&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidat=jorginho&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidate=ronaldo&amp;ref=polars\",\n        ]\n    }\n)\nresult = df.select(\n    pl.col(\"urls\").str.extract(r\"candidate=(\\w+)\", group_index=1),\n)\nprint(result)\n</code></pre></p> <p> <code>str.extract</code> <pre><code>let df = df! (\n    \"urls\" =&gt; [\n        \"http://vote.com/ballon_dor?candidate=messi&amp;ref=polars\",\n        \"http://vote.com/ballon_dor?candidat=jorginho&amp;ref=polars\",\n        \"http://vote.com/ballon_dor?candidate=ronaldo&amp;ref=polars\",\n    ]\n)?;\n\nlet result = df\n    .lazy()\n    .select([col(\"urls\").str().extract(lit(r\"candidate=(\\w+)\"), 1)])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 urls    \u2502\n\u2502 ---     \u2502\n\u2502 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 messi   \u2502\n\u2502 null    \u2502\n\u2502 ronaldo \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To extract all occurrences of a pattern within a string, we can use the function <code>extract_all</code>. In the example below, we extract all numbers from a string using the regex pattern <code>(\\d+)</code>, which matches one or more digits. The resulting output of the function <code>extract_all</code> is a list containing all instances of the matched pattern within the string.</p>  Python Rust <p> <code>str.extract_all</code> <pre><code>df = pl.DataFrame({\"text\": [\"123 bla 45 asd\", \"xyz 678 910t\"]})\nresult = df.select(\n    pl.col(\"text\").str.extract_all(r\"(\\d+)\").alias(\"extracted_nrs\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.extract_all</code> <pre><code>let df = df! (\n    \"text\" =&gt; [\"123 bla 45 asd\", \"xyz 678 910t\"]\n)?;\n\nlet result = df\n    .lazy()\n    .select([col(\"text\")\n        .str()\n        .extract_all(lit(r\"(\\d+)\"))\n        .alias(\"extracted_nrs\")])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 extracted_nrs  \u2502\n\u2502 ---            \u2502\n\u2502 list[str]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"123\", \"45\"]  \u2502\n\u2502 [\"678\", \"910\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#replace-a-pattern","title":"Replace a pattern","text":"<p>Akin to the functions <code>extract</code> and <code>extract_all</code>, Polars provides the functions <code>replace</code> and <code>replace_all</code>. These accept a regex pattern or a literal substring (if the parameter <code>literal</code> is set to <code>True</code>) and perform the replacements specified. The function <code>replace</code> will make at most one replacement whereas the function <code>replace_all</code> will make all the non-overlapping replacements it finds.</p>  Python Rust <p> <code>str.replace</code> \u00b7 <code>str.replace_all</code> <pre><code>df = pl.DataFrame({\"text\": [\"123abc\", \"abc456\"]})\nresult = df.with_columns(\n    pl.col(\"text\").str.replace(r\"\\d\", \"-\"),\n    pl.col(\"text\").str.replace_all(r\"\\d\", \"-\").alias(\"text_replace_all\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.replace</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature regex <pre><code>let df = df! (\n    \"text\" =&gt; [\"123abc\", \"abc456\"]\n)?;\n\nlet result = df\n    .lazy()\n    .with_columns([\n        col(\"text\").str().replace(lit(r\"\\d\"), lit(\"-\"), false),\n        col(\"text\")\n            .str()\n            .replace_all(lit(r\"\\d\"), lit(\"-\"), false)\n            .alias(\"text_replace_all\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 text   \u2506 text_replace_all \u2502\n\u2502 ---    \u2506 ---              \u2502\n\u2502 str    \u2506 str              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -23abc \u2506 ---abc           \u2502\n\u2502 abc-56 \u2506 abc---           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#modifying-strings","title":"Modifying strings","text":""},{"location":"user-guide/expressions/strings/#case-conversion","title":"Case conversion","text":"<p>Converting the casing of a string is a common operation and Polars supports it out of the box with the functions <code>to_lowercase</code>, <code>to_titlecase</code>, and <code>to_uppercase</code>:</p>  Python Rust <p> <code>str.to_lowercase</code> \u00b7 <code>str.to_titlecase</code> \u00b7 <code>str.to_uppercase</code> <pre><code>addresses = pl.DataFrame(\n    {\n        \"addresses\": [\n            \"128 PERF st\",\n            \"Rust blVD, 158\",\n            \"PoLaRs Av, 12\",\n            \"1042 Query sq\",\n        ]\n    }\n)\n\naddresses = addresses.select(\n    pl.col(\"addresses\").alias(\"originals\"),\n    pl.col(\"addresses\").str.to_titlecase(),\n    pl.col(\"addresses\").str.to_lowercase().alias(\"lower\"),\n    pl.col(\"addresses\").str.to_uppercase().alias(\"upper\"),\n)\nprint(addresses)\n</code></pre></p> <p> <code>str.to_lowercase</code> \u00b7 <code>str.to_titlecase</code> \u00b7 <code>str.to_uppercase</code> \u00b7  Available on feature nightly <pre><code>let addresses = df! (\n    \"addresses\" =&gt; [\n        \"128 PERF st\",\n        \"Rust blVD, 158\",\n        \"PoLaRs Av, 12\",\n        \"1042 Query sq\",\n    ]\n)?;\n\nlet addresses = addresses\n    .lazy()\n    .select([\n        col(\"addresses\").alias(\"originals\"),\n        col(\"addresses\").str().to_titlecase(),\n        col(\"addresses\").str().to_lowercase().alias(\"lower\"),\n        col(\"addresses\").str().to_uppercase().alias(\"upper\"),\n    ])\n    .collect()?;\n\nprintln!(\"{addresses}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 originals      \u2506 addresses      \u2506 lower          \u2506 upper          \u2502\n\u2502 ---            \u2506 ---            \u2506 ---            \u2506 ---            \u2502\n\u2502 str            \u2506 str            \u2506 str            \u2506 str            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 128 PERF st    \u2506 128 Perf St    \u2506 128 perf st    \u2506 128 PERF ST    \u2502\n\u2502 Rust blVD, 158 \u2506 Rust Blvd, 158 \u2506 rust blvd, 158 \u2506 RUST BLVD, 158 \u2502\n\u2502 PoLaRs Av, 12  \u2506 Polars Av, 12  \u2506 polars av, 12  \u2506 POLARS AV, 12  \u2502\n\u2502 1042 Query sq  \u2506 1042 Query Sq  \u2506 1042 query sq  \u2506 1042 QUERY SQ  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#stripping-characters-from-the-ends","title":"Stripping characters from the ends","text":"<p>Polars provides five functions in the namespace <code>str</code> that let you strip characters from the ends of the string:</p> Function Behaviour <code>strip_chars</code> Removes leading and trailing occurrences of the characters specified. <code>strip_chars_end</code> Removes trailing occurrences of the characters specified. <code>strip_chars_start</code> Removes leading occurrences of the characters specified. <code>strip_prefix</code> Removes an exact substring prefix if present. <code>strip_suffix</code> Removes an exact substring suffix if present. Similarity to Python string methods <p><code>strip_chars</code> is similar to Python's string method <code>strip</code> and <code>strip_prefix</code>/<code>strip_suffix</code> are similar to Python's string methods <code>removeprefix</code> and <code>removesuffix</code>, respectively.</p> <p>It is important to understand that the first three functions interpret their string argument as a set of characters whereas the functions <code>strip_prefix</code> and <code>strip_suffix</code> do interpret their string argument as a literal string.</p>  Python Rust <p> <code>str.strip_chars</code> \u00b7 <code>str.strip_chars_end</code> \u00b7 <code>str.strip_chars_start</code> \u00b7 <code>str.strip_prefix</code> \u00b7 <code>str.strip_suffix</code> <pre><code>addr = pl.col(\"addresses\")\nchars = \", 0123456789\"\nresult = addresses.select(\n    addr.str.strip_chars(chars).alias(\"strip\"),\n    addr.str.strip_chars_end(chars).alias(\"end\"),\n    addr.str.strip_chars_start(chars).alias(\"start\"),\n    addr.str.strip_prefix(\"128 \").alias(\"prefix\"),\n    addr.str.strip_suffix(\", 158\").alias(\"suffix\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.strip_chars</code> \u00b7 <code>str.strip_chars_end</code> \u00b7 <code>str.strip_chars_start</code> \u00b7 <code>str.strip_prefix</code> \u00b7 <code>str.strip_suffix</code> <pre><code>let addr = col(\"addresses\");\nlet chars = lit(\", 0123456789\");\nlet result = addresses\n    .lazy()\n    .select([\n        addr.clone().str().strip_chars(chars.clone()).alias(\"strip\"),\n        addr.clone()\n            .str()\n            .strip_chars_end(chars.clone())\n            .alias(\"end\"),\n        addr.clone().str().strip_chars_start(chars).alias(\"start\"),\n        addr.clone().str().strip_prefix(lit(\"128 \")).alias(\"prefix\"),\n        addr.str().strip_suffix(lit(\", 158\")).alias(\"suffix\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 strip     \u2506 end           \u2506 start          \u2506 prefix         \u2506 suffix        \u2502\n\u2502 ---       \u2506 ---           \u2506 ---            \u2506 ---            \u2506 ---           \u2502\n\u2502 str       \u2506 str           \u2506 str            \u2506 str            \u2506 str           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Perf St   \u2506 128 Perf St   \u2506 Perf St        \u2506 Perf St        \u2506 128 Perf St   \u2502\n\u2502 Rust Blvd \u2506 Rust Blvd     \u2506 Rust Blvd, 158 \u2506 Rust Blvd, 158 \u2506 Rust Blvd     \u2502\n\u2502 Polars Av \u2506 Polars Av     \u2506 Polars Av, 12  \u2506 Polars Av, 12  \u2506 Polars Av, 12 \u2502\n\u2502 Query Sq  \u2506 1042 Query Sq \u2506 Query Sq       \u2506 1042 Query Sq  \u2506 1042 Query Sq \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If no argument is provided, the three functions <code>strip_chars</code>, <code>strip_chars_end</code>, and <code>strip_chars_start</code>, remove whitespace by default.</p>"},{"location":"user-guide/expressions/strings/#slicing","title":"Slicing","text":"<p>Besides extracting substrings as specified by patterns, you can also slice strings at specified offsets to produce substrings. The general-purpose function for slicing is <code>slice</code> and it takes the starting offset and the optional length of the slice. If the length of the slice is not specified or if it's past the end of the string, Polars slices the string all the way to the end.</p> <p>The functions <code>head</code> and <code>tail</code> are specialised versions used for slicing the beginning and end of a string, respectively.</p>  Python Rust <p> <code>str.slice</code> \u00b7 <code>str.head</code> \u00b7 <code>str.tail</code> <pre><code>df = pl.DataFrame(\n    {\n        \"fruits\": [\"pear\", \"mango\", \"dragonfruit\", \"passionfruit\"],\n        \"n\": [1, -1, 4, -4],\n    }\n)\n\nresult = df.with_columns(\n    pl.col(\"fruits\").str.slice(pl.col(\"n\")).alias(\"slice\"),\n    pl.col(\"fruits\").str.head(pl.col(\"n\")).alias(\"head\"),\n    pl.col(\"fruits\").str.tail(pl.col(\"n\")).alias(\"tail\"),\n)\nprint(result)\n</code></pre></p> <p> <code>str.str_slice</code> \u00b7 <code>str.str_head</code> \u00b7 <code>str.str_tail</code> <pre><code>let df = df! (\n    \"fruits\" =&gt; [\"pear\", \"mango\", \"dragonfruit\", \"passionfruit\"],\n    \"n\" =&gt; [1, -1, 4, -4],\n)?;\n\nlet result = df\n    .lazy()\n    .with_columns([\n        col(\"fruits\")\n            .str()\n            .slice(col(\"n\"), lit(NULL))\n            .alias(\"slice\"),\n        col(\"fruits\").str().head(col(\"n\")).alias(\"head\"),\n        col(\"fruits\").str().tail(col(\"n\")).alias(\"tail\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 fruits       \u2506 n   \u2506 slice   \u2506 head     \u2506 tail     \u2502\n\u2502 ---          \u2506 --- \u2506 ---     \u2506 ---      \u2506 ---      \u2502\n\u2502 str          \u2506 i64 \u2506 str     \u2506 str      \u2506 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 pear         \u2506 1   \u2506 ear     \u2506 p        \u2506 r        \u2502\n\u2502 mango        \u2506 -1  \u2506 o       \u2506 mang     \u2506 ango     \u2502\n\u2502 dragonfruit  \u2506 4   \u2506 onfruit \u2506 drag     \u2506 ruit     \u2502\n\u2502 passionfruit \u2506 -4  \u2506 ruit    \u2506 passionf \u2506 ionfruit \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#api-documentation","title":"API documentation","text":"<p>In addition to the examples covered above, Polars offers various other string manipulation functions. To explore these additional methods, you can go to the API documentation of your chosen programming language for Polars.</p>"},{"location":"user-guide/expressions/structs/","title":"Structs","text":"<p>The data type <code>Struct</code> is a composite data type that can store multiple fields in a single column.</p> <p>Python analogy</p> <p>For Python users, the data type <code>Struct</code> is kind of like a Python dictionary. Even better, if you are familiar with Python typing, you can think of the data type <code>Struct</code> as <code>typing.TypedDict</code>.</p> <p>In this page of the user guide we will see situations in which the data type <code>Struct</code> arises, we will understand why it does arise, and we will see how to work with <code>Struct</code> values.</p> <p>Let's start with a dataframe that captures the average rating of a few movies across some states in the US:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\n\nratings = pl.DataFrame(\n    {\n        \"Movie\": [\"Cars\", \"IT\", \"ET\", \"Cars\", \"Up\", \"IT\", \"Cars\", \"ET\", \"Up\", \"Cars\"],\n        \"Theatre\": [\"NE\", \"ME\", \"IL\", \"ND\", \"NE\", \"SD\", \"NE\", \"IL\", \"IL\", \"NE\"],\n        \"Avg_Rating\": [4.5, 4.4, 4.6, 4.3, 4.8, 4.7, 4.5, 4.9, 4.7, 4.6],\n        \"Count\": [30, 27, 26, 29, 31, 28, 28, 26, 33, 28],\n    }\n)\nprint(ratings)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use polars::prelude::*;\nlet ratings = df!(\n        \"Movie\"=&gt; [\"Cars\", \"IT\", \"ET\", \"Cars\", \"Up\", \"IT\", \"Cars\", \"ET\", \"Up\", \"Cars\"],\n        \"Theatre\"=&gt; [\"NE\", \"ME\", \"IL\", \"ND\", \"NE\", \"SD\", \"NE\", \"IL\", \"IL\", \"NE\"],\n        \"Avg_Rating\"=&gt; [4.5, 4.4, 4.6, 4.3, 4.8, 4.7, 4.5, 4.9, 4.7, 4.6],\n        \"Count\"=&gt; [30, 27, 26, 29, 31, 28, 28, 26, 33, 28],\n\n)?;\nprintln!(\"{}\", &amp;ratings);\n</code></pre></p> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2502\n\u2502 IT    \u2506 ME      \u2506 4.4        \u2506 27    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2502\n\u2502 Cars  \u2506 ND      \u2506 4.3        \u2506 29    \u2502\n\u2502 Up    \u2506 NE      \u2506 4.8        \u2506 31    \u2502\n\u2502 IT    \u2506 SD      \u2506 4.7        \u2506 28    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 28    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2502\n\u2502 Up    \u2506 IL      \u2506 4.7        \u2506 33    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.6        \u2506 28    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/structs/#encountering-the-data-type-struct","title":"Encountering the data type <code>Struct</code>","text":"<p>A common operation that will lead to a <code>Struct</code> column is the ever so popular <code>value_counts</code> function that is commonly used in exploratory data analysis. Checking the number of times a state appears in the data is done as so:</p>  Python Rust <p> <code>value_counts</code> <pre><code>result = ratings.select(pl.col(\"Theatre\").value_counts(sort=True))\nprint(result)\n</code></pre></p> <p> <code>value_counts</code> \u00b7  Available on feature dtype-struct <pre><code>let result = ratings\n    .clone()\n    .lazy()\n    .select([col(\"Theatre\").value_counts(true, true, \"count\", false)])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Theatre   \u2502\n\u2502 ---       \u2502\n\u2502 struct[2] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 {\"NE\",4}  \u2502\n\u2502 {\"IL\",3}  \u2502\n\u2502 {\"ME\",1}  \u2502\n\u2502 {\"ND\",1}  \u2502\n\u2502 {\"SD\",1}  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Quite unexpected an output, especially if coming from tools that do not have such a data type. We're not in peril, though. To get back to a more familiar output, all we need to do is use the function <code>unnest</code> on the <code>Struct</code> column:</p>  Python Rust <p> <code>unnest</code> <pre><code>result = ratings.select(pl.col(\"Theatre\").value_counts(sort=True)).unnest(\"Theatre\")\nprint(result)\n</code></pre></p> <p> <code>unnest</code> <pre><code>let result = ratings\n    .clone()\n    .lazy()\n    .select([col(\"Theatre\").value_counts(true, true, \"count\", false)])\n    .unnest(by_name([\"Theatre\"], true, false), None)\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Theatre \u2506 count \u2502\n\u2502 ---     \u2506 ---   \u2502\n\u2502 str     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NE      \u2506 4     \u2502\n\u2502 IL      \u2506 3     \u2502\n\u2502 ME      \u2506 1     \u2502\n\u2502 ND      \u2506 1     \u2502\n\u2502 SD      \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The function <code>unnest</code> will turn each field of the <code>Struct</code> into its own column.</p> <p>Why <code>value_counts</code> returns a <code>Struct</code></p> <p>Polars expressions always operate on a single series and return another series. <code>Struct</code> is the data type that allows us to provide multiple columns as input to an expression, or to output multiple columns from an expression. Thus, we can use the data type <code>Struct</code> to specify each value and its count when we use <code>value_counts</code>.</p>"},{"location":"user-guide/expressions/structs/#inferring-the-data-type-struct-from-dictionaries","title":"Inferring the data type <code>Struct</code> from dictionaries","text":"<p>When building series or dataframes, Polars will convert dictionaries to the data type <code>Struct</code>:</p>  Python Rust <p> <code>Series</code> <pre><code>rating_series = pl.Series(\n    \"ratings\",\n    [\n        {\"Movie\": \"Cars\", \"Theatre\": \"NE\", \"Avg_Rating\": 4.5},\n        {\"Movie\": \"Toy Story\", \"Theatre\": \"ME\", \"Avg_Rating\": 4.9},\n    ],\n)\nprint(rating_series)\n</code></pre></p> <p> <code>Series</code> <pre><code>// Don't think we can make it the same way in rust, but this works\nlet rating_series = df!(\n    \"Movie\" =&gt; &amp;[\"Cars\", \"Toy Story\"],\n    \"Theatre\" =&gt; &amp;[\"NE\", \"ME\"],\n    \"Avg_Rating\" =&gt; &amp;[4.5, 4.9],\n)?\n.into_struct(\"ratings\".into())\n.into_series();\nprintln!(\"{}\", &amp;rating_series);\n</code></pre></p> <pre><code>shape: (2,)\nSeries: 'ratings' [struct[3]]\n[\n    {\"Cars\",\"NE\",4.5}\n    {\"Toy Story\",\"ME\",4.9}\n]\n</code></pre> <p>The number of fields, their names, and their types, are inferred from the first dictionary seen. Subsequent incongruences can result in <code>null</code> values or in errors:</p>  Python Rust <p> <code>Series</code> <pre><code>null_rating_series = pl.Series(\n    \"ratings\",\n    [\n        {\"Movie\": \"Cars\", \"Theatre\": \"NE\", \"Avg_Rating\": 4.5},\n        {\"Mov\": \"Toy Story\", \"Theatre\": \"ME\", \"Avg_Rating\": 4.9},\n        {\"Movie\": \"Snow White\", \"Theatre\": \"IL\", \"Avg_Rating\": \"4.7\"},\n    ],\n    strict=False,  # To show the final structs with `null` values.\n)\nprint(null_rating_series)\n</code></pre></p> <p> <code>Series</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (3,)\nSeries: 'ratings' [struct[4]]\n[\n    {\"Cars\",\"NE\",\"4.5\",null}\n    {null,\"ME\",\"4.9\",\"Toy Story\"}\n    {\"Snow White\",\"IL\",\"4.7\",null}\n]\n</code></pre>"},{"location":"user-guide/expressions/structs/#extracting-individual-values-of-a-struct","title":"Extracting individual values of a <code>Struct</code>","text":"<p>Let's say that we needed to obtain just the field <code>\"Movie\"</code> from the <code>Struct</code> in the series that we created above. We can use the function <code>field</code> to do so:</p>  Python Rust <p> <code>struct.field</code> <pre><code>result = rating_series.struct.field(\"Movie\")\nprint(result)\n</code></pre></p> <p> <code>struct.field_by_name</code> <pre><code>let result = rating_series.struct_()?.field_by_name(\"Movie\")?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (2,)\nSeries: 'Movie' [str]\n[\n    \"Cars\"\n    \"Toy Story\"\n]\n</code></pre>"},{"location":"user-guide/expressions/structs/#renaming-individual-fields-of-a-struct","title":"Renaming individual fields of a <code>Struct</code>","text":"<p>What if we need to rename individual fields of a <code>Struct</code> column? We use the function <code>rename_fields</code>:</p>  Python Rust <p> <code>struct.rename_fields</code> <pre><code>result = rating_series.struct.rename_fields([\"Film\", \"State\", \"Value\"])\nprint(result)\n</code></pre></p> <p> <code>struct.rename_fields</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (2,)\nSeries: 'ratings' [struct[3]]\n[\n    {\"Cars\",\"NE\",4.5}\n    {\"Toy Story\",\"ME\",4.9}\n]\n</code></pre> <p>To be able to actually see that the field names were changed, we will create a dataframe where the only column is the result and then we use the function <code>unnest</code> so that each field becomes its own column. The column names will reflect the renaming operation we just did:</p>  Python Rust <p> <code>struct.rename_fields</code> <pre><code>print(\n    result.to_frame().unnest(\"ratings\"),\n)\n</code></pre></p> <p> <code>struct.rename_fields</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Film      \u2506 State \u2506 Value \u2502\n\u2502 ---       \u2506 ---   \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars      \u2506 NE    \u2506 4.5   \u2502\n\u2502 Toy Story \u2506 ME    \u2506 4.9   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/structs/#practical-use-cases-of-struct-columns","title":"Practical use-cases of <code>Struct</code> columns","text":""},{"location":"user-guide/expressions/structs/#identifying-duplicate-rows","title":"Identifying duplicate rows","text":"<p>Let's get back to the <code>ratings</code> data. We want to identify cases where there are duplicates at a \u201cMovie\u201d and \u201cTheatre\u201d level.</p> <p>This is where the data type <code>Struct</code> shines:</p>  Python Rust <p> <code>is_duplicated</code> \u00b7 <code>struct</code> <pre><code>result = ratings.filter(pl.struct(\"Movie\", \"Theatre\").is_duplicated())\nprint(result)\n</code></pre></p> <p> <code>is_duplicated</code> \u00b7 <code>Struct</code> \u00b7  Available on feature dtype-struct <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 28    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.6        \u2506 28    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can identify the unique cases at this level also with <code>is_unique</code>!</p>"},{"location":"user-guide/expressions/structs/#multi-column-ranking","title":"Multi-column ranking","text":"<p>Suppose, given that we know there are duplicates, we want to choose which rating gets a higher priority. We can say that the column \u201cCount\u201d is the most important, and if there is a tie in the column \u201cCount\u201d then we consider the column \u201cAvg_Rating\u201d.</p> <p>We can then do:</p>  Python Rust <p> <code>is_duplicated</code> \u00b7 <code>struct</code> <pre><code>result = ratings.with_columns(\n    pl.struct(\"Count\", \"Avg_Rating\")\n    .rank(\"dense\", descending=True)\n    .over(\"Movie\", \"Theatre\")\n    .alias(\"Rank\")\n).filter(pl.struct(\"Movie\", \"Theatre\").is_duplicated())\n\nprint(result)\n</code></pre></p> <p> <code>is_duplicated</code> \u00b7 <code>Struct</code> \u00b7  Available on feature dtype-struct <pre><code>let result = ratings\n    .lazy()\n    .with_columns([as_struct(vec![col(\"Count\"), col(\"Avg_Rating\")])\n        .rank(\n            RankOptions {\n                method: RankMethod::Dense,\n                descending: true,\n            },\n            None,\n        )\n        .over([col(\"Movie\"), col(\"Theatre\")])\n        .alias(\"Rank\")])\n    // .filter(as_struct(&amp;[col(\"Movie\"), col(\"Theatre\")]).is_duplicated())\n    // Error: .is_duplicated() not available if you try that\n    // https://github.com/pola-rs/polars/issues/3803\n    .filter(len().over([col(\"Movie\"), col(\"Theatre\")]).gt(lit(1)))\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2506 Rank \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2506 1    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2506 2    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 28    \u2506 3    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2506 1    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.6        \u2506 28    \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>That's a pretty complex set of requirements done very elegantly in Polars! To learn more about the function <code>over</code>, used above, see the user guide section on window functions.</p>"},{"location":"user-guide/expressions/structs/#using-multiple-columns-in-a-single-expression","title":"Using multiple columns in a single expression","text":"<p>As mentioned earlier, the data type <code>Struct</code> is also useful if you need to pass multiple columns as input to an expression. As an example, suppose we want to compute the Ackermann function on two columns of a dataframe. There is no way of composing Polars expressions to compute the Ackermann function<sup>1</sup>, so we define a custom function:</p>  Python Rust <pre><code>def ack(m, n):\n    if not m:\n        return n + 1\n    if not n:\n        return ack(m - 1, 1)\n    return ack(m - 1, ack(m, n - 1))\n</code></pre> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <p></p> <p>Now, to compute the values of the Ackermann function on those arguments, we start by creating a <code>Struct</code> with fields <code>m</code> and <code>n</code> and then use the function <code>map_elements</code> to apply the function <code>ack</code> to each value:</p>  Python Rust <p> <code>map_elements</code> <pre><code>values = pl.DataFrame(\n    {\n        \"m\": [0, 0, 0, 1, 1, 1, 2],\n        \"n\": [2, 3, 4, 1, 2, 3, 1],\n    }\n)\nresult = values.with_columns(\n    pl.struct([\"m\", \"n\"])\n    .map_elements(lambda s: ack(s[\"m\"], s[\"n\"]), return_dtype=pl.Int64)\n    .alias(\"ack\")\n)\n\nprint(result)\n</code></pre></p> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 m   \u2506 n   \u2506 ack \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 2   \u2506 3   \u2502\n\u2502 0   \u2506 3   \u2506 4   \u2502\n\u2502 0   \u2506 4   \u2506 5   \u2502\n\u2502 1   \u2506 1   \u2506 3   \u2502\n\u2502 1   \u2506 2   \u2506 4   \u2502\n\u2502 1   \u2506 3   \u2506 5   \u2502\n\u2502 2   \u2506 1   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Refer to this section of the user guide to learn more about applying user-defined Python functions to your data.</p> <ol> <li> <p>To say that something cannot be done is quite a bold claim. If you prove us wrong, please let us know!\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/expressions/user-defined-python-functions/","title":"User-defined Python functions","text":"<p>Note</p> <p>Before writing custom Python functions, consider using Polars plugins which offer better performance and integration. Check out the expression plugins documentation for custom expressions and I/O plugins for custom data sources.</p> <p>Polars expressions are quite powerful and flexible, so there is much less need for custom Python functions compared to other libraries. Still, you may need to pass an expression's state to a third party library or apply your black box function to data in Polars.</p> <p>In this part of the documentation we'll be using two APIs that allows you to do this:</p> <ul> <li> <code>map_elements</code>:   Call a function separately on each value in the <code>Series</code>.</li> <li> <code>map_batches</code>:   Always passes the full <code>Series</code> to the function.</li> </ul>"},{"location":"user-guide/expressions/user-defined-python-functions/#processing-individual-values-with-map_elements","title":"Processing individual values with <code>map_elements()</code>","text":"<p>Let's start with the simplest case: we want to process each value in a <code>Series</code> individually. Here is our data:</p>  Python Rust <pre><code>df = pl.DataFrame(\n    {\n        \"keys\": [\"a\", \"a\", \"b\", \"b\"],\n        \"values\": [10, 7, 1, 23],\n    }\n)\nprint(df)\n</code></pre> <pre><code>let df = df!(\n    \"keys\" =&gt; &amp;[\"a\", \"a\", \"b\", \"b\"],\n    \"values\" =&gt; &amp;[10, 7, 1, 23],\n)?;\nprintln!(\"{}\", df);\n</code></pre> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 values \u2502\n\u2502 ---  \u2506 ---    \u2502\n\u2502 str  \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 10     \u2502\n\u2502 a    \u2506 7      \u2502\n\u2502 b    \u2506 1      \u2502\n\u2502 b    \u2506 23     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We'll call <code>math.log()</code> on each individual value:</p>  Python Rust <pre><code>def my_log(value):\n    return math.log(value)\n\n\nout = df.select(pl.col(\"values\").map_elements(my_log, return_dtype=pl.Float64))\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 values   \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.302585 \u2502\n\u2502 1.94591  \u2502\n\u2502 0.0      \u2502\n\u2502 3.135494 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>While this works, <code>map_elements()</code> has two problems:</p> <ol> <li>Limited to individual items: Often you'll want to have a calculation that needs to operate on    the whole <code>Series</code>, rather than individual items one by one.</li> <li>Performance overhead: Even if you do want to process each item individually, calling a    function for each individual item is slow; all those extra function calls add a lot of overhead.</li> </ol> <p>Let's start by solving the first problem, and then we'll see how to solve the second problem.</p>"},{"location":"user-guide/expressions/user-defined-python-functions/#processing-a-whole-series-with-map_batches","title":"Processing a whole <code>Series</code> with <code>map_batches()</code>","text":"<p>We want to run a custom function on the contents of a whole <code>Series</code>. For demonstration purposes, let's say we want to calculate the difference between the mean of a <code>Series</code> and each value.</p> <p>We can use the <code>map_batches()</code> API to run this function on either the full <code>Series</code> or individual groups in a <code>group_by()</code>:</p>  Python Rust <pre><code>def diff_from_mean(series):\n    # This will be very slow for non-trivial Series, since it's all Python\n    # code:\n    total = 0\n    for value in series:\n        total += value\n    mean = total / len(series)\n    return pl.Series([value - mean for value in series])\n\n\n# Apply our custom function to a full Series with map_batches():\nout = df.select(pl.col(\"values\").map_batches(diff_from_mean, return_dtype=pl.Float64))\nprint(\"== select() with UDF ==\")\nprint(out)\n\n# Apply our custom function per group:\nprint(\"== group_by() with UDF ==\")\nout = df.group_by(\"keys\").agg(\n    pl.col(\"values\").map_batches(diff_from_mean, return_dtype=pl.Float64)\n)\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>== select() with UDF ==\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 values \u2502\n\u2502 ---    \u2502\n\u2502 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -0.25  \u2502\n\u2502 -3.25  \u2502\n\u2502 -9.25  \u2502\n\u2502 12.75  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n== group_by() with UDF ==\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 values        \u2502\n\u2502 ---  \u2506 ---           \u2502\n\u2502 str  \u2506 list[f64]     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [1.5, -1.5]   \u2502\n\u2502 b    \u2506 [-11.0, 11.0] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-python-functions/#fast-operations-with-user-defined-functions","title":"Fast operations with user-defined functions","text":"<p>The problem with a pure-Python implementation is that it's slow. In general, you want to minimize how much Python code you call if you want fast results.</p> <p>To maximize speed, you'll want to make sure that you're using a function written in a compiled language. For numeric calculations Polars supports a pair of interfaces defined by NumPy called \"ufuncs\" and \"generalized ufuncs\". The former runs on each item individually, and the latter accepts a whole NumPy array, which allows for more flexible operations.</p> <p>NumPy and other libraries like SciPy come with pre-written ufuncs you can use with Polars. For example:</p>  Python Rust <pre><code>out = df.select(pl.col(\"values\").map_batches(np.log, return_dtype=pl.Float64))\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 values   \u2502\n\u2502 ---      \u2502\n\u2502 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.302585 \u2502\n\u2502 1.94591  \u2502\n\u2502 0.0      \u2502\n\u2502 3.135494 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Notice that we can use <code>map_batches()</code>, because <code>numpy.log()</code> is able to run on both individual items and on whole NumPy arrays. This means it will run much faster than our original example, since we only have a single Python call and then all processing happens in a fast low-level language.</p>"},{"location":"user-guide/expressions/user-defined-python-functions/#example-a-fast-custom-function-using-numba","title":"Example: A fast custom function using Numba","text":"<p>The pre-written functions NumPy provides are helpful, but our goal is to write our own functions. For example, let's say we want a fast version of our <code>diff_from_mean()</code> example above. The easiest way to write this in Python is to use Numba, which allows you to write custom functions in (a subset) of Python while still getting the benefit of compiled code.</p> <p>In particular, Numba provides a decorator called <code>@guvectorize</code>. This creates a generalized ufunc by compiling a Python function to fast machine code, in a way that allows it to be used by Polars.</p> <p>In the following example the <code>diff_from_mean_numba()</code> will be compiled to fast machine code at import time, which will take a little time. After that all calls to the function will run quickly. The <code>Series</code> will be converted to a NumPy array before being passed to the function:</p>  Python Rust <pre><code># This will be compiled to machine code, so it will be fast. The Series is\n# converted to a NumPy array before being passed to the function. See the\n# Numba documentation for more details:\n# https://numba.readthedocs.io/en/stable/user/vectorize.html\n@guvectorize([(int64[:], float64[:])], \"(n)-&gt;(n)\")\ndef diff_from_mean_numba(arr, result):\n    total = 0\n    for value in arr:\n        total += value\n    mean = total / len(arr)\n    for i, value in enumerate(arr):\n        result[i] = value - mean\n\n\nout = df.select(\n    pl.col(\"values\").map_batches(diff_from_mean_numba, return_dtype=pl.Float64)\n)\nprint(\"== select() with UDF ==\")\nprint(out)\n\nout = df.group_by(\"keys\").agg(\n    pl.col(\"values\").map_batches(diff_from_mean_numba, return_dtype=pl.Float64)\n)\nprint(\"== group_by() with UDF ==\")\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>== select() with UDF ==\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 values \u2502\n\u2502 ---    \u2502\n\u2502 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -0.25  \u2502\n\u2502 -3.25  \u2502\n\u2502 -9.25  \u2502\n\u2502 12.75  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n== group_by() with UDF ==\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 values        \u2502\n\u2502 ---  \u2506 ---           \u2502\n\u2502 str  \u2506 list[f64]     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [1.5, -1.5]   \u2502\n\u2502 b    \u2506 [-11.0, 11.0] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-python-functions/#missing-data-is-not-allowed-when-calling-generalized-ufuncs","title":"Missing data is not allowed when calling generalized ufuncs","text":"<p>Before being passed to a user-defined function like <code>diff_from_mean_numba()</code>, a <code>Series</code> will be converted to a NumPy array. Unfortunately, NumPy arrays don't have a concept of missing data. If there is missing data in the original <code>Series</code>, this means the resulting array won't actually match the <code>Series</code>.</p> <p>If you're calculating results item by item, this doesn't matter. For example, <code>numpy.log()</code> gets called on each individual value separately, so those missing values don't change the calculation. But if the result of a user-defined function depend on multiple values in the <code>Series</code>, it's not clear what exactly should happen with the missing values.</p> <p>Therefore, when calling generalized ufuncs such as Numba functions decorated with <code>@guvectorize</code>, Polars will raise an error if you try to pass in a <code>Series</code> with missing data. How do you get rid of missing data? Either fill it in or drop it before calling your custom function.</p>"},{"location":"user-guide/expressions/user-defined-python-functions/#combining-multiple-column-values","title":"Combining multiple column values","text":"<p>If you want to pass multiple columns to a user-defined function, you can use <code>Struct</code>s, which are covered in detail in a different section. The basic idea is to combine multiple columns into a <code>Struct</code>, and then the function can extract the columns back out:</p>  Python Rust <pre><code># Add two arrays together:\n@guvectorize([(int64[:], int64[:], float64[:])], \"(n),(n)-&gt;(n)\")\ndef add(arr, arr2, result):\n    for i in range(len(arr)):\n        result[i] = arr[i] + arr2[i]\n\n\ndf3 = pl.DataFrame({\"values_1\": [1, 2, 3], \"values_2\": [10, 20, 30]})\n\nout = df3.select(\n    # Create a struct that has two columns in it:\n    pl.struct([\"values_1\", \"values_2\"])\n    # Pass the struct to a lambda that then passes the individual columns to\n    # the add() function:\n    .map_batches(\n        lambda combined: add(\n            combined.struct.field(\"values_1\"), combined.struct.field(\"values_2\")\n        ),\n        return_dtype=pl.Float64,\n    )\n    .alias(\"add_columns\")\n)\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 add_columns \u2502\n\u2502 ---         \u2502\n\u2502 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11.0        \u2502\n\u2502 22.0        \u2502\n\u2502 33.0        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-python-functions/#streaming-calculations","title":"Streaming calculations","text":"<p>Passing the full <code>Series</code> to the user-defined function has a cost: it may use a lot of memory, as its contents are copied into a NumPy array. You can use the <code>is_elementwise=True</code> argument to  <code>map_batches</code> to stream results into the function, which means it might not get all values at once.</p> <p>Note</p> <p>The <code>is_elementwise</code> argument can lead to incorrect results if set incorrectly. If you set <code>is_elementwise=True</code>, make sure that your function actually operates element-by-element (e.g. \"calculate the logarithm of each value\") - our example function <code>diff_from_mean()</code>, for instance, does not.</p>"},{"location":"user-guide/expressions/user-defined-python-functions/#return-types","title":"Return types","text":"<p>Custom Python functions are often black boxes; Polars doesn't know what your function is doing or what it will return. The return data type is therefore automatically inferred. We do that by waiting for the first non-null value. That value will then be used to determine the type of the resulting <code>Series</code>.</p> <p>The mapping of Python types to Polars data types is as follows:</p> <ul> <li><code>int</code> -&gt; <code>Int64</code></li> <li><code>float</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>str</code> -&gt; <code>String</code></li> <li><code>list[tp]</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> <li><code>dict[str, [tp]]</code> -&gt; <code>struct</code></li> <li><code>Any</code> -&gt; <code>object</code> (Prevent this at all times)</li> </ul> <p>Rust types map as follows:</p> <ul> <li><code>i32</code> or <code>i64</code> -&gt; <code>Int64</code></li> <li><code>f32</code> or <code>f64</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>String</code> or <code>str</code> -&gt; <code>String</code></li> <li><code>Vec&lt;tp&gt;</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> </ul> <p>You can pass a <code>return_dtype</code> argument to  <code>map_batches</code> if you want to override the inferred type.</p>"},{"location":"user-guide/expressions/window-functions/","title":"Window functions","text":"<p>Window functions are expressions with superpowers. They allow you to perform aggregations on groups within the context <code>select</code>. Let's get a feel for what that means.</p> <p>First, we load a Pok\u00e9mon dataset:</p>  Python Rust <p> <code>read_csv</code> <pre><code>import polars as pl\n\ntypes = (\n    \"Grass Water Fire Normal Ground Electric Psychic Fighting Bug Steel \"\n    \"Flying Dragon Dark Ghost Poison Rock Ice Fairy\".split()\n)\ntype_enum = pl.Enum(types)\n# then let's load some csv data with information about pokemon\npokemon = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\",\n).cast({\"Type 1\": type_enum, \"Type 2\": type_enum})\nprint(pokemon.head())\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\nuse reqwest::blocking::Client;\n\nlet data: Vec&lt;u8&gt; = Client::new()\n    .get(\"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\")\n    .send()?\n    .text()?\n    .bytes()\n    .collect();\n\nlet file = std::io::Cursor::new(data);\nlet df = CsvReadOptions::default()\n    .with_has_header(true)\n    .into_reader_with_file_handle(file)\n    .finish()?;\n\nprintln!(\"{}\", df.head(Some(5)));\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 enum   \u2506 enum   \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window-functions/#operations-per-group","title":"Operations per group","text":"<p>Window functions are ideal when we want to perform an operation within a group. For instance, suppose we want to rank our Pok\u00e9mon by the column \u201cSpeed\u201d. However, instead of a global ranking, we want to rank the speed within each group defined by the column \u201cType 1\u201d. We write the expression to rank the data by the column \u201cSpeed\u201d and then we add the function <code>over</code> to specify that this should happen over the unique values of the column \u201cType 1\u201d:</p>  Python Rust <p> <code>over</code> <pre><code>result = pokemon.select(\n    pl.col(\"Name\", \"Type 1\"),\n    pl.col(\"Speed\").rank(\"dense\", descending=True).over(\"Type 1\").alias(\"Speed rank\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"Name\"),\n        col(\"Type 1\"),\n        col(\"Speed\")\n            .rank(\n                RankOptions {\n                    method: RankMethod::Dense,\n                    descending: true,\n                },\n                None,\n            )\n            .over([\"Type 1\"])\n            .alias(\"Speed rank\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (163, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                  \u2506 Type 1  \u2506 Speed rank \u2502\n\u2502 ---                   \u2506 ---     \u2506 ---        \u2502\n\u2502 str                   \u2506 enum    \u2506 u32        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bulbasaur             \u2506 Grass   \u2506 6          \u2502\n\u2502 Ivysaur               \u2506 Grass   \u2506 3          \u2502\n\u2502 Venusaur              \u2506 Grass   \u2506 1          \u2502\n\u2502 VenusaurMega Venusaur \u2506 Grass   \u2506 1          \u2502\n\u2502 Charmander            \u2506 Fire    \u2506 7          \u2502\n\u2502 \u2026                     \u2506 \u2026       \u2506 \u2026          \u2502\n\u2502 Moltres               \u2506 Fire    \u2506 5          \u2502\n\u2502 Dratini               \u2506 Dragon  \u2506 3          \u2502\n\u2502 Dragonair             \u2506 Dragon  \u2506 2          \u2502\n\u2502 Dragonite             \u2506 Dragon  \u2506 1          \u2502\n\u2502 Mewtwo                \u2506 Psychic \u2506 2          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To help visualise this operation, you may imagine that Polars selects the subsets of the data that share the same value for the column \u201cType 1\u201d and then computes the ranking expression only for those values. Then, the results for that specific group are projected back to the original rows and Polars does this for all of the existing groups. The diagram below highlights the ranking computation for the Pok\u00e9mon with \u201cType 1\u201d equal to \u201cGrass\u201d.</p> Bulbasaur Ivysaur Venusaur VenusaurMega  Venusaur Charmander ... Oddish Gloom ... Grass Grass Grass Grass Fire ... Grass Grass ... 45 60 80 80 65 ... 30 40 ... 6 3 1 1 7 ... 8 7 ... Name Type 1 Speed Speed rank Golbat Poison 90 1 <p>Note how the row for the Pok\u00e9mon \u201cGolbat\u201d has a \u201cSpeed\u201d value of <code>90</code>, which is greater than the value <code>80</code> of the Pok\u00e9mon \u201cVenusaur\u201d, and yet the latter was ranked 1 because \u201cGolbat\u201d and \u201cVenusar\u201d do not share the same value for the column \u201cType 1\u201d.</p> <p>The function <code>over</code> accepts an arbitrary number of expressions to specify the groups over which to perform the computations. We can repeat the ranking above, but over the combination of the columns \u201cType 1\u201d and \u201cType 2\u201d for a more fine-grained ranking:</p>  Python Rust <p> <code>over</code> <pre><code>result = pokemon.select(\n    pl.col(\"Name\", \"Type 1\", \"Type 2\"),\n    pl.col(\"Speed\")\n    .rank(\"dense\", descending=True)\n    .over(\"Type 1\", \"Type 2\")\n    .alias(\"Speed rank\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (163, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                  \u2506 Type 1  \u2506 Type 2 \u2506 Speed rank \u2502\n\u2502 ---                   \u2506 ---     \u2506 ---    \u2506 ---        \u2502\n\u2502 str                   \u2506 enum    \u2506 enum   \u2506 u32        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bulbasaur             \u2506 Grass   \u2506 Poison \u2506 6          \u2502\n\u2502 Ivysaur               \u2506 Grass   \u2506 Poison \u2506 3          \u2502\n\u2502 Venusaur              \u2506 Grass   \u2506 Poison \u2506 1          \u2502\n\u2502 VenusaurMega Venusaur \u2506 Grass   \u2506 Poison \u2506 1          \u2502\n\u2502 Charmander            \u2506 Fire    \u2506 null   \u2506 7          \u2502\n\u2502 \u2026                     \u2506 \u2026       \u2506 \u2026      \u2506 \u2026          \u2502\n\u2502 Moltres               \u2506 Fire    \u2506 Flying \u2506 2          \u2502\n\u2502 Dratini               \u2506 Dragon  \u2506 null   \u2506 2          \u2502\n\u2502 Dragonair             \u2506 Dragon  \u2506 null   \u2506 1          \u2502\n\u2502 Dragonite             \u2506 Dragon  \u2506 Flying \u2506 1          \u2502\n\u2502 Mewtwo                \u2506 Psychic \u2506 null   \u2506 2          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In general, the results you get with the function <code>over</code> can also be achieved with an aggregation followed by a call to the function <code>explode</code>, although the rows would be in a different order:</p>  Python Rust <p> <code>explode</code> <pre><code>result = (\n    pokemon.group_by(\"Type 1\")\n    .agg(\n        pl.col(\"Name\"),\n        pl.col(\"Speed\").rank(\"dense\", descending=True).alias(\"Speed rank\"),\n    )\n    .select(pl.col(\"Name\"), pl.col(\"Type 1\"), pl.col(\"Speed rank\"))\n    .explode(\"Name\", \"Speed rank\")\n)\n\nprint(result)\n</code></pre></p> <p> <code>explode</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (163, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name      \u2506 Type 1 \u2506 Speed rank \u2502\n\u2502 ---       \u2506 ---    \u2506 ---        \u2502\n\u2502 str       \u2506 enum   \u2506 u32        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Jynx      \u2506 Ice    \u2506 1          \u2502\n\u2502 Articuno  \u2506 Ice    \u2506 2          \u2502\n\u2502 Sandshrew \u2506 Ground \u2506 5          \u2502\n\u2502 Sandslash \u2506 Ground \u2506 3          \u2502\n\u2502 Diglett   \u2506 Ground \u2506 2          \u2502\n\u2502 \u2026         \u2506 \u2026      \u2506 \u2026          \u2502\n\u2502 Tauros    \u2506 Normal \u2506 3          \u2502\n\u2502 Ditto     \u2506 Normal \u2506 16         \u2502\n\u2502 Eevee     \u2506 Normal \u2506 14         \u2502\n\u2502 Porygon   \u2506 Normal \u2506 18         \u2502\n\u2502 Snorlax   \u2506 Normal \u2506 19         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This shows that, usually, <code>group_by</code> and <code>over</code> produce results of different shapes:</p> <ul> <li><code>group_by</code> usually produces a resulting dataframe with as many rows as groups used for   aggregating; and</li> <li><code>over</code> usually produces a dataframe with the same number of rows as the original.</li> </ul> <p>The function <code>over</code> does not always produce results with the same number of rows as the original dataframe, and that is what we explore next.</p>"},{"location":"user-guide/expressions/window-functions/#mapping-results-to-dataframe-rows","title":"Mapping results to dataframe rows","text":"<p>The function <code>over</code> accepts a parameter <code>mapping_strategy</code> that determines how the results of the expression over the group are mapped back to the rows of the dataframe.</p>"},{"location":"user-guide/expressions/window-functions/#group_to_rows","title":"<code>group_to_rows</code>","text":"<p>The default behaviour is <code>\"group_to_rows\"</code>: the result of the expression over the group should be the same length as the group and the results are mapped back to the rows of that group.</p> <p>If the order of the rows is not relevant, the option <code>\"explode\"</code> is more performant. Instead of mapping the resulting values to the original rows, Polars creates a new dataframe where values from the same group are next to each other. To help understand the distinction, consider the following dataframe:</p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 athlete \u2506 country \u2506 rank \u2502\n\u2502 ---     \u2506 ---     \u2506 ---  \u2502\n\u2502 str     \u2506 str     \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A       \u2506 PT      \u2506 6    \u2502\n\u2502 B       \u2506 NL      \u2506 1    \u2502\n\u2502 C       \u2506 NL      \u2506 5    \u2502\n\u2502 D       \u2506 PT      \u2506 4    \u2502\n\u2502 E       \u2506 PT      \u2506 2    \u2502\n\u2502 F       \u2506 NL      \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can sort the athletes by rank within their own countries. If we do so, the Dutch athletes were in the second, third, and sixth, rows, and they will remain there. What will change is the order of the names of the athletes, which goes from \u201cB\u201d, \u201cC\u201d, and \u201cF\u201d, to \u201cB\u201d, \u201cF\u201d, and \u201cC\u201d:</p>  Python Rust <p> <code>over</code> <pre><code>result = athletes.select(\n    pl.col(\"athlete\", \"rank\").sort_by(pl.col(\"rank\")).over(pl.col(\"country\")),\n    pl.col(\"country\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 athlete \u2506 rank \u2506 country \u2502\n\u2502 ---     \u2506 ---  \u2506 ---     \u2502\n\u2502 str     \u2506 i64  \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 E       \u2506 2    \u2506 PT      \u2502\n\u2502 B       \u2506 1    \u2506 NL      \u2502\n\u2502 F       \u2506 3    \u2506 NL      \u2502\n\u2502 D       \u2506 4    \u2506 PT      \u2502\n\u2502 A       \u2506 6    \u2506 PT      \u2502\n\u2502 C       \u2506 5    \u2506 NL      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The diagram below represents this transformation:</p> A B C D E F PT NL NL PT PT NL 6 1 5 4 2 3 E B F D A C PT NL NL PT PT NL 2 1 3 4 6 5 NL NL"},{"location":"user-guide/expressions/window-functions/#explode","title":"<code>explode</code>","text":"<p>If we set the parameter <code>mapping_strategy</code> to <code>\"explode\"</code>, then athletes of the same country are grouped together, but the final order of the rows \u2013 with respect to the countries \u2013 will not be the same, as the diagram shows:</p> A B C D E F PT NL NL PT PT NL 6 1 5 4 2 3 E B F D A C PT NL NL PT PT NL 2 1 3 4 6 5 NL NL NL <p>Because Polars does not need to keep track of the positions of the rows of each group, using <code>\"explode\"</code> is typically faster than <code>\"group_to_rows\"</code>. However, using <code>\"explode\"</code> also requires more care because it implies reordering the other columns that we wish to keep. The code that produces this result follows</p>  Python Rust <p> <code>over</code> <pre><code>result = athletes.select(\n    pl.all()\n    .sort_by(pl.col(\"rank\"))\n    .over(pl.col(\"country\"), mapping_strategy=\"explode\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 athlete \u2506 country \u2506 rank \u2502\n\u2502 ---     \u2506 ---     \u2506 ---  \u2502\n\u2502 str     \u2506 str     \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 E       \u2506 PT      \u2506 2    \u2502\n\u2502 D       \u2506 PT      \u2506 4    \u2502\n\u2502 A       \u2506 PT      \u2506 6    \u2502\n\u2502 B       \u2506 NL      \u2506 1    \u2502\n\u2502 F       \u2506 NL      \u2506 3    \u2502\n\u2502 C       \u2506 NL      \u2506 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window-functions/#join","title":"<code>join</code>","text":"<p>Another possible value for the parameter <code>mapping_strategy</code> is <code>\"join\"</code>, which aggregates the resulting values in a list and repeats the list over all rows of the same group:</p>  Python Rust <p> <code>over</code> <pre><code>result = athletes.with_columns(\n    pl.col(\"rank\").sort().over(pl.col(\"country\"), mapping_strategy=\"join\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>// Contribute the Rust translation of the Python example by opening a PR.\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 athlete \u2506 country \u2506 rank      \u2502\n\u2502 ---     \u2506 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 str     \u2506 list[i64] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A       \u2506 PT      \u2506 [2, 4, 6] \u2502\n\u2502 B       \u2506 NL      \u2506 [1, 3, 5] \u2502\n\u2502 C       \u2506 NL      \u2506 [1, 3, 5] \u2502\n\u2502 D       \u2506 PT      \u2506 [2, 4, 6] \u2502\n\u2502 E       \u2506 PT      \u2506 [2, 4, 6] \u2502\n\u2502 F       \u2506 NL      \u2506 [1, 3, 5] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window-functions/#windowed-aggregation-expressions","title":"Windowed aggregation expressions","text":"<p>In case the expression applied to the values of a group produces a scalar value, the scalar is broadcast across the rows of the group:</p>  Python Rust <p> <code>over</code> <pre><code>result = pokemon.select(\n    pl.col(\"Name\", \"Type 1\", \"Speed\"),\n    pl.col(\"Speed\").mean().over(pl.col(\"Type 1\")).alias(\"Mean speed in group\"),\n)\n\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>let result = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"Name\"),\n        col(\"Type 1\"),\n        col(\"Speed\"),\n        col(\"Speed\")\n            .mean()\n            .over([\"Type 1\"])\n            .alias(\"Mean speed in group\"),\n    ])\n    .collect()?;\n\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (163, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                  \u2506 Type 1  \u2506 Speed \u2506 Mean speed in group \u2502\n\u2502 ---                   \u2506 ---     \u2506 ---   \u2506 ---                 \u2502\n\u2502 str                   \u2506 enum    \u2506 i64   \u2506 f64                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bulbasaur             \u2506 Grass   \u2506 45    \u2506 54.230769           \u2502\n\u2502 Ivysaur               \u2506 Grass   \u2506 60    \u2506 54.230769           \u2502\n\u2502 Venusaur              \u2506 Grass   \u2506 80    \u2506 54.230769           \u2502\n\u2502 VenusaurMega Venusaur \u2506 Grass   \u2506 80    \u2506 54.230769           \u2502\n\u2502 Charmander            \u2506 Fire    \u2506 65    \u2506 86.285714           \u2502\n\u2502 \u2026                     \u2506 \u2026       \u2506 \u2026     \u2506 \u2026                   \u2502\n\u2502 Moltres               \u2506 Fire    \u2506 90    \u2506 86.285714           \u2502\n\u2502 Dratini               \u2506 Dragon  \u2506 50    \u2506 66.666667           \u2502\n\u2502 Dragonair             \u2506 Dragon  \u2506 70    \u2506 66.666667           \u2502\n\u2502 Dragonite             \u2506 Dragon  \u2506 80    \u2506 66.666667           \u2502\n\u2502 Mewtwo                \u2506 Psychic \u2506 130   \u2506 99.25               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window-functions/#more-examples","title":"More examples","text":"<p>For more exercises, below are some window functions for us to compute:</p> <ul> <li>sort all Pok\u00e9mon by type;</li> <li>select the first <code>3</code> Pok\u00e9mon per type as <code>\"Type 1\"</code>;</li> <li>sort the Pok\u00e9mon within a type by speed in descending order and select the first <code>3</code> as   <code>\"fastest/group\"</code>;</li> <li>sort the Pok\u00e9mon within a type by attack in descending order and select the first <code>3</code> as   <code>\"strongest/group\"</code>; and</li> <li>sort the Pok\u00e9mon within a type by name and select the first <code>3</code> as <code>\"sorted_by_alphabet\"</code>.</li> </ul>  Python Rust <p> <code>over</code> <pre><code>result = pokemon.sort(\"Type 1\").select(\n    pl.col(\"Type 1\").head(3).over(\"Type 1\", mapping_strategy=\"explode\"),\n    pl.col(\"Name\")\n    .sort_by(pl.col(\"Speed\"), descending=True)\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"fastest/group\"),\n    pl.col(\"Name\")\n    .sort_by(pl.col(\"Attack\"), descending=True)\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"strongest/group\"),\n    pl.col(\"Name\")\n    .sort()\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"sorted_by_alphabet\"),\n)\nprint(result)\n</code></pre></p> <p> <code>over</code> <pre><code>let result = df\n    .lazy()\n    .select([\n        col(\"Type 1\")\n            .head(Some(3))\n            .over_with_options(Some([\"Type 1\"]), None, WindowMapping::Explode)?\n            .explode(ExplodeOptions {\n                empty_as_null: false,\n                keep_nulls: false,\n            }),\n        col(\"Name\")\n            .sort_by(\n                [\"Speed\"],\n                SortMultipleOptions::default().with_order_descending(true),\n            )\n            .head(Some(3))\n            .over_with_options(Some([\"Type 1\"]), None, WindowMapping::Explode)?\n            .explode(ExplodeOptions {\n                empty_as_null: false,\n                keep_nulls: false,\n            })\n            .alias(\"fastest/group\"),\n        col(\"Name\")\n            .sort_by(\n                [\"Attack\"],\n                SortMultipleOptions::default().with_order_descending(true),\n            )\n            .head(Some(3))\n            .over_with_options(Some([\"Type 1\"]), None, WindowMapping::Explode)?\n            .explode(ExplodeOptions {\n                empty_as_null: false,\n                keep_nulls: false,\n            })\n            .alias(\"strongest/group\"),\n        col(\"Name\")\n            .sort(Default::default())\n            .head(Some(3))\n            .over_with_options(Some([\"Type 1\"]), None, WindowMapping::Explode)?\n            .explode(ExplodeOptions {\n                empty_as_null: false,\n                keep_nulls: false,\n            })\n            .alias(\"sorted_by_alphabet\"),\n    ])\n    .collect()?;\nprintln!(\"{result:?}\");\n</code></pre></p> <pre><code>shape: (43, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type 1 \u2506 fastest/group         \u2506 strongest/group       \u2506 sorted_by_alphabet      \u2502\n\u2502 ---    \u2506 ---                   \u2506 ---                   \u2506 ---                     \u2502\n\u2502 enum   \u2506 str                   \u2506 str                   \u2506 str                     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Grass  \u2506 Venusaur              \u2506 Victreebel            \u2506 Bellsprout              \u2502\n\u2502 Grass  \u2506 VenusaurMega Venusaur \u2506 VenusaurMega Venusaur \u2506 Bulbasaur               \u2502\n\u2502 Grass  \u2506 Victreebel            \u2506 Exeggutor             \u2506 Exeggcute               \u2502\n\u2502 Water  \u2506 Starmie               \u2506 GyaradosMega Gyarados \u2506 Blastoise               \u2502\n\u2502 Water  \u2506 Tentacruel            \u2506 Kingler               \u2506 BlastoiseMega Blastoise \u2502\n\u2502 \u2026      \u2506 \u2026                     \u2506 \u2026                     \u2506 \u2026                       \u2502\n\u2502 Rock   \u2506 Kabutops              \u2506 Kabutops              \u2506 Geodude                 \u2502\n\u2502 Ice    \u2506 Jynx                  \u2506 Articuno              \u2506 Articuno                \u2502\n\u2502 Ice    \u2506 Articuno              \u2506 Jynx                  \u2506 Jynx                    \u2502\n\u2502 Fairy  \u2506 Clefable              \u2506 Clefable              \u2506 Clefable                \u2502\n\u2502 Fairy  \u2506 Clefairy              \u2506 Clefairy              \u2506 Clefairy                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/","title":"IO","text":"<p>Reading and writing your data is crucial for a DataFrame library. In this chapter you will learn more on how to read and write to different file formats that are supported by Polars.</p> <ul> <li>CSV</li> <li>Excel</li> <li>Parquet</li> <li>Json</li> <li>Multiple</li> <li>Hive</li> <li>Database</li> <li>Cloud storage</li> <li>Google Big Query</li> <li>Hugging Face</li> </ul>"},{"location":"user-guide/io/bigquery/","title":"Google BigQuery","text":"<p>To read or write from GBQ, additional dependencies are needed:</p>  Python <pre><code>$ pip install google-cloud-bigquery\n</code></pre>"},{"location":"user-guide/io/bigquery/#read","title":"Read","text":"<p>We can load a query into a <code>DataFrame</code> like this:</p>  Python <p> <code>from_arrow</code> \u00b7  Available on feature pyarrow \u00b7  Available on feature fsspec <pre><code>import polars as pl\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Perform a query.\nQUERY = (\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n    'WHERE state = \"TX\" '\n    'LIMIT 100')\nquery_job = client.query(QUERY)  # API request\nrows = query_job.result()  # Waits for query to finish\n\ndf = pl.from_arrow(rows.to_arrow())\n</code></pre></p>"},{"location":"user-guide/io/bigquery/#write","title":"Write","text":"Python <pre><code>from google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Write DataFrame to stream as parquet file; does not hit disk\nwith io.BytesIO() as stream:\n    df.write_parquet(stream)\n    stream.seek(0)\n    parquet_options = bigquery.ParquetOptions()\n    parquet_options.enable_list_inference = True\n    job = client.load_table_from_file(\n        stream,\n        destination='tablename',\n        project='projectname',\n        job_config=bigquery.LoadJobConfig(\n            source_format=bigquery.SourceFormat.PARQUET,\n            parquet_options=parquet_options,\n        ),\n    )\njob.result()  # Waits for the job to complete\n</code></pre>"},{"location":"user-guide/io/cloud-storage/","title":"Cloud storage","text":"<p>Polars can read and write to AWS S3, Azure Blob Storage and Google Cloud Storage. The API is the same for all three storage providers.</p> <p>To read from cloud storage, additional dependencies may be needed depending on the use case and cloud storage provider:</p>  Python Rust <pre><code>$ pip install fsspec s3fs adlfs gcsfs\n</code></pre> <pre><code>$ cargo add aws_sdk_s3 aws_config tokio --features tokio/full\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#reading-from-cloud-storage","title":"Reading from cloud storage","text":"<p>Polars supports reading Parquet, CSV, IPC and NDJSON files from cloud storage:</p>  Python Rust <p> <code>read_parquet</code> \u00b7 <code>read_csv</code> \u00b7 <code>read_ipc</code> <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\ndf = pl.read_parquet(source)\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7 <code>CsvReader</code> \u00b7 <code>IpcReader</code> \u00b7  Available on feature ipc \u00b7  Available on feature parquet \u00b7  Available on feature csv <pre><code>use aws_config::BehaviorVersion;\nuse polars::prelude::*;\n\n#[tokio::main]\nasync fn main() {\n    let bucket = \"&lt;YOUR_BUCKET&gt;\";\n    let path = \"&lt;YOUR_PATH&gt;\";\n\n    let config = aws_config::load_defaults(BehaviorVersion::latest()).await;\n    let client = aws_sdk_s3::Client::new(&amp;config);\n\n    let object = client\n        .get_object()\n        .bucket(bucket)\n        .key(path)\n        .send()\n        .await\n        .unwrap();\n\n    let bytes = object.body.collect().await.unwrap().into_bytes();\n\n    let cursor = std::io::Cursor::new(bytes);\n    let df = CsvReader::new(cursor).finish().unwrap();\n\n    println!(\"{df:?}\");\n}\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#scanning-from-cloud-storage-with-query-optimisation","title":"Scanning from cloud storage with query optimisation","text":"<p>Using <code>pl.scan_*</code> functions to read from cloud storage can benefit from predicate and projection pushdowns, where the query optimizer will apply them before the file is downloaded. This can significantly reduce the amount of data that needs to be downloaded. The query evaluation is triggered by calling <code>collect</code>.</p>  Python Rust <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\ndf = pl.scan_parquet(source).filter(pl.col(\"id\") &lt; 100).select(\"id\",\"value\").collect()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#cloud-authentication","title":"Cloud authentication","text":"<p>Polars is able to automatically load default credential configurations for some cloud providers. For cases when this does not happen, it is possible to manually configure the credentials for Polars to use for authentication. This can be done in a few ways:</p>"},{"location":"user-guide/io/cloud-storage/#using-storage_options","title":"Using <code>storage_options</code>:","text":"<ul> <li>Credentials can be passed as configuration keys in a dict with the <code>storage_options</code> parameter:</li> </ul>  Python Rust <p> <code>scan_parquet</code> <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\nstorage_options = {\n    \"aws_access_key_id\": \"&lt;secret&gt;\",\n    \"aws_secret_access_key\": \"&lt;secret&gt;\",\n    \"aws_region\": \"us-east-1\",\n}\ndf = pl.scan_parquet(source, storage_options=storage_options).collect()\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#using-one-of-the-available-credentialprovider-utility-classes","title":"Using one of the available <code>CredentialProvider*</code> utility classes","text":"<ul> <li>There may be a utility class <code>pl.CredentialProvider*</code> that provides the required authentication   functionality. For example, <code>pl.CredentialProviderAWS</code> supports selecting AWS profiles, as well as   assuming an IAM role:</li> </ul>  Python Rust <p> <code>scan_parquet</code> \u00b7 <code>CredentialProviderAWS</code> <pre><code>lf = pl.scan_parquet(\n    \"s3://.../...\",\n    credential_provider=pl.CredentialProviderAWS(\n        profile_name=\"...\",\n        assume_role={\n            \"RoleArn\": f\"...\",\n            \"RoleSessionName\": \"...\",\n        }\n    ),\n)\n\ndf = lf.collect()\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#using-a-custom-credential_provider-function","title":"Using a custom <code>credential_provider</code> function","text":"<ul> <li>Some environments may require custom authentication logic (e.g. AWS IAM role-chaining). For these   cases a Python function can be provided for Polars to use to retrieve credentials:</li> </ul>  Python Rust <p> <code>scan_parquet</code> <pre><code>def get_credentials() -&gt; pl.CredentialProviderFunctionReturn:\n    expiry = None\n\n    return {\n        \"aws_access_key_id\": \"...\",\n        \"aws_secret_access_key\": \"...\",\n        \"aws_session_token\": \"...\",\n    }, expiry\n\n\nlf = pl.scan_parquet(\n    \"s3://.../...\",\n    credential_provider=get_credentials,\n)\n\ndf = lf.collect()\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p> <ul> <li>Example for Azure:</li> </ul>  Python Rust <p> <code>scan_parquet</code> \u00b7 <code>CredentialProviderAzure</code> <pre><code>def credential_provider():\n    credential = DefaultAzureCredential(exclude_managed_identity_credential=True)\n    token = credential.get_token(\"https://storage.azure.com/.default\")\n\n    return {\"bearer_token\": token.token}, token.expires_on\n\n\npl.scan_parquet(\n    \"abfss://...@.../...\",\n    credential_provider=credential_provider,\n)\n\n# Note that for the above case, this shortcut is also available:\n\npl.scan_parquet(\n    \"abfss://...@.../...\",\n    credential_provider=pl.CredentialProviderAzure(\n        credential=DefaultAzureCredential(exclude_managed_identity_credential=True)\n    ),\n)\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#set-a-default-credential-provider-to-use","title":"Set a default credential provider to use","text":"<ul> <li>It is possible to globally configure a default credential provider, so that it does not need to be   passed to every I/O function call. This can be convenient in the case where there are many cloud   I/O operations that use the same credential provider.</li> </ul>  Python Rust <p> <code>scan_parquet</code> \u00b7 <code>CredentialProviderAWS</code> <pre><code>pl.Config.set_default_credential_provider(\n    pl.CredentialProviderAWS(\n        profile_name=\"...\",\n        assume_role={\n            \"RoleArn\": f\"...\",\n            \"RoleSessionName\": \"...\",\n        },\n    )\n)\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#cloud-retry-configuration","title":"Cloud retry configuration","text":"<ul> <li>Retry behavior such as maximum retries and backoff can be configured via <code>storage_options</code>:</li> </ul>  Python Rust <p> <code>scan_parquet</code> <pre><code>import polars as pl\n\npl.scan_parquet(\n    \"s3://bucket/*.parquet\",\n    storage_options={\n        \"max_retries\": 3,\n        \"retry_timeout_ms\": 9873,\n        \"retry_init_backoff_ms\": 9874,\n        \"retry_max_backoff_ms\": 9875,\n        \"retry_base_multiplier\": 3.14159,\n    },\n)\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#scanning-with-pyarrow","title":"Scanning with PyArrow","text":"<p>We can also scan from cloud storage using PyArrow. This is particularly useful for partitioned datasets such as Hive partitioning.</p> <p>We first create a PyArrow dataset and then create a <code>LazyFrame</code> from the dataset.</p>  Python Rust <p> <code>scan_pyarrow_dataset</code> <pre><code>import polars as pl\nimport pyarrow.dataset as ds\n\ndset = ds.dataset(\"s3://my-partitioned-folder/\", format=\"parquet\")\n(\n    pl.scan_pyarrow_dataset(dset)\n    .filter(pl.col(\"foo\") == \"a\")\n    .select([\"foo\", \"bar\"])\n    .collect()\n)\n</code></pre></p> <p> <code>scan_pyarrow_dataset</code> <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#writing-to-cloud-storage","title":"Writing to cloud storage","text":"<p><code>DataFrame</code>s can also be written to cloud storage by passing a cloud URL:</p>  Python Rust <p> <code>write_parquet</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n        \"bar\": [1, 2, 3, 4, 5],\n    }\n)\n\ndestination = \"s3://bucket/my_file.parquet\"\n\ndf.write_parquet(destination)\n</code></pre></p> <p> <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p> <p>Note that <code>DataFrame</code>s can also be written to any Python file object that supports writes. This can be helpful for performing operations that are not yet natively supported, e.g. writing a compressed CSV directly to cloud:</p>  Python Rust <p> <code>write_csv</code> <pre><code>import polars as pl\nimport s3fs\nimport gzip\n\ndf = pl.DataFrame(\n    {\n        \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n        \"bar\": [1, 2, 3, 4, 5],\n    }\n)\n\ndestination = \"s3://bucket/my_file.csv.gz\"\n\nfs = s3fs.S3FileSystem()\n\nwith fs.open(destination, \"wb\") as cloud_f:\n    with gzip.open(cloud_f, \"w\") as f:\n        df.write_csv(f)\n</code></pre></p> <p> <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/csv/","title":"CSV","text":""},{"location":"user-guide/io/csv/#read-write","title":"Read &amp; write","text":"<p>Reading a CSV file should look familiar:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/assets/data/path.csv\")\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\n\nlet mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/assets/data/path.csv\").unwrap();\nCsvWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n\nlet df = CsvReadOptions::default()\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/path.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap();\n</code></pre></p> <p>Writing a CSV file is similar with the <code>write_csv</code> function:</p>  Python Rust <p> <code>write_csv</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_csv(\"docs/assets/data/path.csv\")\n</code></pre></p> <p> <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/assets/data/path.csv\").unwrap();\nCsvWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p>"},{"location":"user-guide/io/csv/#scan","title":"Scan","text":"<p>Polars allows you to scan a CSV input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_csv</code> <pre><code>df = pl.scan_csv(\"docs/assets/data/path.csv\")\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let lf = LazyCsvReader::new(PlRefPath::new(\"docs/assets/data/path.csv\"))\n    .finish()\n    .unwrap();\n</code></pre></p> <p>If you want to know why this is desirable, you can read more about these Polars optimizations here.</p>"},{"location":"user-guide/io/database/","title":"Databases","text":""},{"location":"user-guide/io/database/#read-from-a-database","title":"Read from a database","text":"<p>Polars can read from a database using the <code>pl.read_database_uri</code> and <code>pl.read_database</code> functions.</p>"},{"location":"user-guide/io/database/#difference-between-read_database_uri-and-read_database","title":"Difference between <code>read_database_uri</code> and <code>read_database</code>","text":"<p>Use <code>pl.read_database_uri</code> if you want to specify the database connection with a connection string called a <code>uri</code>. For example, the following snippet shows a query to read all columns from the <code>foo</code> table in a Postgres database where we use the <code>uri</code> to connect:</p>  Python <p> <code>read_database_uri</code> <pre><code>import polars as pl\n\nuri = \"postgresql://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database_uri(query=query, uri=uri)\n</code></pre></p> <p>On the other hand, use <code>pl.read_database</code> if you want to connect via a connection engine created with a library like SQLAlchemy.</p>  Python <p> <code>read_database</code> <pre><code>import polars as pl\nfrom sqlalchemy import create_engine\n\nconn = create_engine(f\"sqlite:///test.db\")\n\nquery = \"SELECT * FROM foo\"\n\npl.read_database(query=query, connection=conn.connect())\n</code></pre></p> <p>Note that <code>pl.read_database_uri</code> is likely to be faster than <code>pl.read_database</code> if you are using a SQLAlchemy or DBAPI2 connection as these connections may load the data row-wise into Python before copying the data again to the column-wise Apache Arrow format.</p>"},{"location":"user-guide/io/database/#engines","title":"Engines","text":"<p>Polars doesn't manage connections and data transfer from databases by itself. Instead, external libraries (known as engines) handle this.</p> <p>When using <code>pl.read_database</code>, you specify the engine when you create the connection object. When using <code>pl.read_database_uri</code>, you can specify one of two engines to read from the database:</p> <ul> <li>ConnectorX and</li> <li>ADBC</li> </ul> <p>Both engines have native support for Apache Arrow and so can read data directly into a Polars <code>DataFrame</code> without copying the data.</p>"},{"location":"user-guide/io/database/#connectorx","title":"ConnectorX","text":"<p>ConnectorX is the default engine and supports numerous databases including Postgres, Mysql, SQL Server and Redshift. ConnectorX is written in Rust and stores data in Arrow format to allow for zero-copy to Polars.</p> <p>To read from one of the supported databases with <code>ConnectorX</code> you need to activate the additional dependency <code>ConnectorX</code> when installing Polars or install it manually with</p> <pre><code>$ pip install connectorx\n</code></pre>"},{"location":"user-guide/io/database/#adbc","title":"ADBC","text":"<p>ADBC (Arrow Database Connectivity) is an engine supported by the Apache Arrow project. ADBC aims to be both an API standard for connecting to databases and libraries implementing this standard in a range of languages.</p> <p>It is still early days for ADBC so support for different databases is limited. At present, drivers for ADBC are only available for Postgres, SQLite and Snowflake. To install ADBC, you need to install the driver for your database. For example, to install the driver for SQLite, you run:</p> <pre><code>$ pip install adbc-driver-sqlite\n</code></pre> <p>As ADBC is not the default engine, you must specify the engine as an argument to <code>pl.read_database_uri</code>.</p>  Python <p> <code>read_database_uri</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database_uri(query=query, uri=uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/database/#write-to-a-database","title":"Write to a database","text":"<p>We can write to a database with Polars using the <code>pl.write_database</code> function.</p>"},{"location":"user-guide/io/database/#engines_1","title":"Engines","text":"<p>As with reading from a database above, Polars uses an engine to write to a database. The currently supported engines are:</p> <ul> <li>SQLAlchemy and</li> <li>Arrow Database Connectivity (ADBC)</li> </ul>"},{"location":"user-guide/io/database/#sqlalchemy","title":"SQLAlchemy","text":"<p>With the default engine SQLAlchemy you can write to any database supported by SQLAlchemy. To use this engine you need to install SQLAlchemy and Pandas</p> <pre><code>$ pip install SQLAlchemy pandas\n</code></pre> <p>In this example, we write the <code>DataFrame</code> to a table called <code>records</code> in the database</p>  Python <p> <code>write_database</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\",  connection=uri)\n</code></pre></p> <p>In the SQLAlchemy approach, Polars converts the <code>DataFrame</code> to a Pandas <code>DataFrame</code> backed by PyArrow and then uses SQLAlchemy methods on a Pandas <code>DataFrame</code> to write to the database.</p>"},{"location":"user-guide/io/database/#adbc_1","title":"ADBC","text":"<p>ADBC can also be used to write to a database. Writing is supported for the same databases that support reading with ADBC. As shown above, you need to install the appropriate ADBC driver for your database.</p>  Python <p> <code>write_database</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\", connection=uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/excel/","title":"Excel","text":"<p>Polars can read and write to Excel files from Python. From a performance perspective, we recommend using other formats if possible, such as Parquet or CSV files.</p>"},{"location":"user-guide/io/excel/#read","title":"Read","text":"<p>Polars does not have a native Excel reader. Instead, it uses an external library called an \"engine\" to parse Excel files into a form that Polars can parse. The available engines are:</p> <ul> <li>fastexcel: This engine is based on the Rust calamine crate   and is (by far) the fastest reader.</li> <li>xlsx2csv: This reader parses the .xlsx file to an in-memory CSV that Polars then reads with its   own CSV reader.</li> <li>openpyxl: Typically slower than xls2csv, but can provide more flexibility for files that are   difficult to parse.</li> </ul> <p>We recommend working with the default fastexcel engine. The xlsx2csv and openpyxl engines are slower but may have more features for parsing tricky data. These engines may be helpful if the fastexcel reader does not work for a specific Excel file.</p> <p>To use one of these engines, the appropriate Python package must be installed as an additional dependency.</p>  Python <pre><code>$ pip install fastexcel xlsx2csv openpyxl \n</code></pre> <p>The default engine for reading .xlsx files is fastexcel. This engine uses the Rust calamine crate to read .xlsx files into an Apache Arrow in-memory representation that Polars can read without needing to copy the data.</p>  Python <p> <code>read_excel</code> <pre><code>df = pl.read_excel(\"docs/assets/data/path.xlsx\")\n</code></pre></p> <p>We can specify the sheet name to read with the <code>sheet_name</code> argument. If we do not specify a sheet name, the first sheet will be read.</p>  Python <p> <code>read_excel</code> <pre><code>df = pl.read_excel(\"docs/assets/data/path.xlsx\", sheet_name=\"Sales\")\n</code></pre></p>"},{"location":"user-guide/io/excel/#write","title":"Write","text":"<p>We need the xlswriter library installed as an additional dependency to write to Excel files.</p>  Python <pre><code>$ pip install xlsxwriter\n</code></pre> <p>Writing to Excel files is not currently available in Rust Polars, though it is possible to use this crate to write to Excel files from Rust.</p> <p>Writing a <code>DataFrame</code> to an Excel file is done with the <code>write_excel</code> method:</p>  Python <p> <code>write_excel</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_excel(\"docs/assets/data/path.xlsx\")\n</code></pre></p> <p>The name of the worksheet can be specified with the <code>worksheet</code> argument.</p>  Python <p> <code>write_excel</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_excel(\"docs/assets/data/path.xlsx\", worksheet=\"Sales\")\n</code></pre></p> <p>Polars can create rich Excel files with multiple sheets and formatting. For more details, see the API docs for <code>write_excel</code>.</p>"},{"location":"user-guide/io/hive/","title":"Hive","text":""},{"location":"user-guide/io/hive/#scanning-hive-partitioned-data","title":"Scanning hive partitioned data","text":"<p>Polars supports scanning hive partitioned parquet and IPC datasets, with planned support for other formats in the future.</p> <p>Hive partition parsing is enabled by default if <code>scan_parquet</code> receives a single directory path, otherwise it is disabled by default. This can be explicitly configured using the <code>hive_partitioning</code> parameter.</p>"},{"location":"user-guide/io/hive/#scanning-a-hive-directory","title":"Scanning a hive directory","text":"<p>For this example the following directory structure is used:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 File path                                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 docs/assets/data/hive/year=2023/month=11/data.parquet \u2502\n\u2502 docs/assets/data/hive/year=2023/month=12/data.parquet \u2502\n\u2502 docs/assets/data/hive/year=2024/month=01/data.parquet \u2502\n\u2502 docs/assets/data/hive/year=2024/month=02/data.parquet \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Simply pass the directory to <code>scan_parquet</code>, and all files will be loaded with the hive parts in the path included in the output:</p>  Python <p> <code>scan_parquet</code> <pre><code>import polars as pl\n\ndf = pl.scan_parquet(\"docs/assets/data/hive/\").collect()\n\nwith pl.Config(tbl_rows=99):\n    print(df)\n</code></pre></p> <pre><code>shape: (11, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2506 year \u2506 month \u2502\n\u2502 --- \u2506 ---  \u2506 ---   \u2502\n\u2502 i64 \u2506 i64  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2023 \u2506 11    \u2502\n\u2502 2   \u2506 2023 \u2506 11    \u2502\n\u2502 3   \u2506 2023 \u2506 12    \u2502\n\u2502 4   \u2506 2023 \u2506 12    \u2502\n\u2502 5   \u2506 2023 \u2506 12    \u2502\n\u2502 6   \u2506 2024 \u2506 1     \u2502\n\u2502 7   \u2506 2024 \u2506 1     \u2502\n\u2502 8   \u2506 2024 \u2506 2     \u2502\n\u2502 9   \u2506 2024 \u2506 2     \u2502\n\u2502 10  \u2506 2024 \u2506 2     \u2502\n\u2502 11  \u2506 2024 \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/hive/#handling-mixed-files","title":"Handling mixed files","text":"<p>Passing a directory to <code>scan_parquet</code> may not work if there are files with different extensions in the directory.</p> <p>For this example the following directory structure is used:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 File path                                                   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 docs/assets/data/hive_mixed/description.txt                 \u2502\n\u2502 docs/assets/data/hive_mixed/year=2023/month=11/data.parquet \u2502\n\u2502 docs/assets/data/hive_mixed/year=2023/month=12/data.parquet \u2502\n\u2502 docs/assets/data/hive_mixed/year=2024/month=01/data.parquet \u2502\n\u2502 docs/assets/data/hive_mixed/year=2024/month=02/data.parquet \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python <p> <code>scan_parquet</code> <pre><code>from pathlib import Path\n\ntry:\n    pl.scan_parquet(\"docs/assets/data/hive_mixed/\").collect()\nexcept Exception as e:\n    print(e)\n</code></pre></p> <p>The above fails as <code>description.txt</code> is not a valid parquet file:</p> <pre><code>directory contained paths with different file extensions: first path: docs/assets/data/hive_mixed/description.txt, second path: docs/assets/data/hive_mixed/year=2024/month=02/data.parquet. Please use a glob pattern to explicitly specify which files to read (e.g. 'dir/**/*', 'dir/**/*.parquet')\n</code></pre> <p>In this situation, a glob pattern can be used to be more specific about which files to load. Note that <code>hive_partitioning</code> must explicitly set to <code>True</code>:</p>  Python <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\n    # Glob to match all files ending in `.parquet`\n    \"docs/assets/data/hive_mixed/**/*.parquet\",\n    hive_partitioning=True,\n).collect()\n\nwith pl.Config(tbl_rows=99):\n    print(df)\n</code></pre></p> <pre><code>shape: (11, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2506 year \u2506 month \u2502\n\u2502 --- \u2506 ---  \u2506 ---   \u2502\n\u2502 i64 \u2506 i64  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2023 \u2506 11    \u2502\n\u2502 2   \u2506 2023 \u2506 11    \u2502\n\u2502 3   \u2506 2023 \u2506 12    \u2502\n\u2502 4   \u2506 2023 \u2506 12    \u2502\n\u2502 5   \u2506 2023 \u2506 12    \u2502\n\u2502 6   \u2506 2024 \u2506 1     \u2502\n\u2502 7   \u2506 2024 \u2506 1     \u2502\n\u2502 8   \u2506 2024 \u2506 2     \u2502\n\u2502 9   \u2506 2024 \u2506 2     \u2502\n\u2502 10  \u2506 2024 \u2506 2     \u2502\n\u2502 11  \u2506 2024 \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/hive/#scanning-file-paths-with-hive-parts","title":"Scanning file paths with hive parts","text":"<p><code>hive_partitioning</code> is not enabled by default for file paths:</p>  Python <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\n    [\n        \"docs/assets/data/hive/year=2024/month=01/data.parquet\",\n        \"docs/assets/data/hive/year=2024/month=02/data.parquet\",\n    ],\n).collect()\n\nprint(df)\n</code></pre></p> <pre><code>shape: (6, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6   \u2502\n\u2502 7   \u2502\n\u2502 8   \u2502\n\u2502 9   \u2502\n\u2502 10  \u2502\n\u2502 11  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Pass <code>hive_partitioning=True</code> to enable hive partition parsing:</p>  Python <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\n    [\n        \"docs/assets/data/hive/year=2024/month=01/data.parquet\",\n        \"docs/assets/data/hive/year=2024/month=02/data.parquet\",\n    ],\n    hive_partitioning=True,\n).collect()\n\nprint(df)\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2506 year \u2506 month \u2502\n\u2502 --- \u2506 ---  \u2506 ---   \u2502\n\u2502 i64 \u2506 i64  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6   \u2506 2024 \u2506 1     \u2502\n\u2502 7   \u2506 2024 \u2506 1     \u2502\n\u2502 8   \u2506 2024 \u2506 2     \u2502\n\u2502 9   \u2506 2024 \u2506 2     \u2502\n\u2502 10  \u2506 2024 \u2506 2     \u2502\n\u2502 11  \u2506 2024 \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/hive/#writing-hive-partitioned-data","title":"Writing hive partitioned data","text":"<p>Note: The following functionality is considered unstable, and is subject to change.</p> <p>Polars supports writing hive partitioned parquet datasets, with planned support for other formats.</p>"},{"location":"user-guide/io/hive/#example","title":"Example","text":"<p>For this example the following DataFrame is used:</p>  Python <pre><code>df = pl.DataFrame({\"a\": [1, 1, 2, 2, 3], \"b\": [1, 1, 1, 2, 2], \"c\": 1})\nprint(df)\n</code></pre> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 1   \u2506 1   \u2502\n\u2502 1   \u2506 1   \u2506 1   \u2502\n\u2502 2   \u2506 1   \u2506 1   \u2502\n\u2502 2   \u2506 2   \u2506 1   \u2502\n\u2502 3   \u2506 2   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We will write it to a hive-partitioned parquet dataset, partitioned by the columns <code>a</code> and <code>b</code>:</p>  Python <p> <code>write_parquet</code> <pre><code>df.write_parquet(\"docs/assets/data/hive_write/\", partition_by=[\"a\", \"b\"])\n</code></pre></p> <p></p> <p>The output is a hive partitioned parquet dataset with the following paths:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 File path                                            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 docs/assets/data/hive_write/a=1/b=1/00000000.parquet \u2502\n\u2502 docs/assets/data/hive_write/a=2/b=1/00000000.parquet \u2502\n\u2502 docs/assets/data/hive_write/a=2/b=2/00000000.parquet \u2502\n\u2502 docs/assets/data/hive_write/a=3/b=2/00000000.parquet \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/hugging-face/","title":"Hugging Face","text":""},{"location":"user-guide/io/hugging-face/#scanning-datasets-from-hugging-face","title":"Scanning datasets from Hugging Face","text":"<p>All cloud-enabled scan functions, and their <code>read_</code> counterparts transparently support scanning from Hugging Face:</p> Scan Read scan_parquet read_parquet scan_csv read_csv scan_ndjson read_ndjson scan_ipc read_ipc"},{"location":"user-guide/io/hugging-face/#path-format","title":"Path format","text":"<p>To scan from Hugging Face, a <code>hf://</code> path can be passed to the scan functions. The <code>hf://</code> path format is defined as <code>hf://BUCKET/REPOSITORY@REVISION/PATH</code>, where:</p> <ul> <li><code>BUCKET</code> is one of <code>datasets</code> or <code>spaces</code></li> <li><code>REPOSITORY</code> is the location of the repository, this is usually in the format of   <code>username/repo_name</code>. A branch can also be optionally specified by appending <code>@branch</code></li> <li><code>REVISION</code> is the name of the branch (or commit) to use. This is optional and defaults to <code>main</code>   if not given.</li> <li><code>PATH</code> is a file or directory path, or a glob pattern from the repository root.</li> </ul> <p>Example <code>hf://</code> paths:</p> Path Path components hf://datasets/nameexhaustion/polars-docs/iris.csv Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: mainPath: iris.csv Web URL hf://datasets/nameexhaustion/polars-docs@foods/*.csv Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: foodsPath: *.csv Web URL hf://datasets/nameexhaustion/polars-docs/hive_dates/ Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: mainPath: hive_dates/ Web URL hf://spaces/nameexhaustion/polars-docs/orders.feather Bucket: spacesRepository: nameexhaustion/polars-docsBranch: mainPath: orders.feather Web URL"},{"location":"user-guide/io/hugging-face/#authentication","title":"Authentication","text":"<p>A Hugging Face API key can be passed to Polars to access private locations using either of the following methods:</p> <ul> <li>Passing a <code>token</code> in <code>storage_options</code> to the scan function, e.g.   <code>scan_parquet(..., storage_options={'token': '&lt;your HF token&gt;'})</code></li> <li>Setting the <code>HF_TOKEN</code> environment variable, e.g. <code>export HF_TOKEN=&lt;your HF token&gt;</code></li> </ul>"},{"location":"user-guide/io/hugging-face/#examples","title":"Examples","text":""},{"location":"user-guide/io/hugging-face/#csv","title":"CSV","text":"Python <p> <code>scan_csv</code> <pre><code>print(pl.scan_csv(\"hf://datasets/nameexhaustion/polars-docs/iris.csv\").collect())\n</code></pre></p> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 5.0          \u2506 3.6         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.7          \u2506 3.0         \u2506 5.2          \u2506 2.3         \u2506 virginica \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See this file at https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.csv</p>"},{"location":"user-guide/io/hugging-face/#ndjson","title":"NDJSON","text":"Python <p> <code>scan_ndjson</code> <pre><code>print(pl.scan_ndjson(\"hf://datasets/nameexhaustion/polars-docs/iris.jsonl\").collect())\n</code></pre></p> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 5.0          \u2506 3.6         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.7          \u2506 3.0         \u2506 5.2          \u2506 2.3         \u2506 virginica \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See this file at https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.jsonl</p>"},{"location":"user-guide/io/hugging-face/#parquet","title":"Parquet","text":"Python <p> <code>scan_parquet</code> <pre><code>print(\n    \"\"\"\\\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date1      \u2506 date2                      \u2506 x   \u2502\n\u2502 ---        \u2506 ---                        \u2506 --- \u2502\n\u2502 date       \u2506 datetime[\u03bcs]               \u2506 i32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2024-01-01 \u2506 2023-01-01 00:00:00        \u2506 1   \u2502\n\u2502 2024-02-01 \u2506 2023-02-01 00:00:00        \u2506 2   \u2502\n\u2502 2024-03-01 \u2506 null                       \u2506 3   \u2502\n\u2502 null       \u2506 2023-03-01 01:01:01.000001 \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n)\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date1      \u2506 date2                      \u2506 x   \u2502\n\u2502 ---        \u2506 ---                        \u2506 --- \u2502\n\u2502 date       \u2506 datetime[\u03bcs]               \u2506 i32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2024-01-01 \u2506 2023-01-01 00:00:00        \u2506 1   \u2502\n\u2502 2024-02-01 \u2506 2023-02-01 00:00:00        \u2506 2   \u2502\n\u2502 2024-03-01 \u2506 null                       \u2506 3   \u2502\n\u2502 null       \u2506 2023-03-01 01:01:01.000001 \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See this folder at https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/main/hive_dates/</p>"},{"location":"user-guide/io/hugging-face/#ipc","title":"IPC","text":"Python <p> <code>scan_ipc</code> <pre><code>print(pl.scan_ipc(\"hf://spaces/nameexhaustion/polars-docs/orders.feather\").collect())\n</code></pre></p> <pre><code>shape: (10, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 o_orderkey \u2506 o_custkey \u2506 o_orderstatus \u2506 o_totalprice \u2506 \u2026 \u2506 o_orderpriority \u2506 o_clerk         \u2506 o_shippriority \u2506 o_comment               \u2502\n\u2502 ---        \u2506 ---       \u2506 ---           \u2506 ---          \u2506   \u2506 ---             \u2506 ---             \u2506 ---            \u2506 ---                     \u2502\n\u2502 i64        \u2506 i64       \u2506 str           \u2506 f64          \u2506   \u2506 str             \u2506 str             \u2506 i64            \u2506 str                     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1          \u2506 36901     \u2506 O             \u2506 173665.47    \u2506 \u2026 \u2506 5-LOW           \u2506 Clerk#000000951 \u2506 0              \u2506 nstructions sleep       \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 furiously am\u2026           \u2502\n\u2502 2          \u2506 78002     \u2506 O             \u2506 46929.18     \u2506 \u2026 \u2506 1-URGENT        \u2506 Clerk#000000880 \u2506 0              \u2506 foxes. pending accounts \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 at th\u2026                  \u2502\n\u2502 3          \u2506 123314    \u2506 F             \u2506 193846.25    \u2506 \u2026 \u2506 5-LOW           \u2506 Clerk#000000955 \u2506 0              \u2506 sly final accounts      \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 boost. care\u2026            \u2502\n\u2502 4          \u2506 136777    \u2506 O             \u2506 32151.78     \u2506 \u2026 \u2506 5-LOW           \u2506 Clerk#000000124 \u2506 0              \u2506 sits. slyly regular     \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 warthogs c\u2026             \u2502\n\u2502 5          \u2506 44485     \u2506 F             \u2506 144659.2     \u2506 \u2026 \u2506 5-LOW           \u2506 Clerk#000000925 \u2506 0              \u2506 quickly. bold deposits  \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 sleep s\u2026                \u2502\n\u2502 6          \u2506 55624     \u2506 F             \u2506 58749.59     \u2506 \u2026 \u2506 4-NOT SPECIFIED \u2506 Clerk#000000058 \u2506 0              \u2506 ggle. special, final    \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 requests \u2026              \u2502\n\u2502 7          \u2506 39136     \u2506 O             \u2506 252004.18    \u2506 \u2026 \u2506 2-HIGH          \u2506 Clerk#000000470 \u2506 0              \u2506 ly special requests     \u2502\n\u2502 32         \u2506 130057    \u2506 O             \u2506 208660.75    \u2506 \u2026 \u2506 2-HIGH          \u2506 Clerk#000000616 \u2506 0              \u2506 ise blithely bold,      \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 regular req\u2026            \u2502\n\u2502 33         \u2506 66958     \u2506 F             \u2506 163243.98    \u2506 \u2026 \u2506 3-MEDIUM        \u2506 Clerk#000000409 \u2506 0              \u2506 uriously. furiously     \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 final requ\u2026             \u2502\n\u2502 34         \u2506 61001     \u2506 O             \u2506 58949.67     \u2506 \u2026 \u2506 3-MEDIUM        \u2506 Clerk#000000223 \u2506 0              \u2506 ly final packages.      \u2502\n\u2502            \u2506           \u2506               \u2506              \u2506   \u2506                 \u2506                 \u2506                \u2506 fluffily fi\u2026            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See this file at https://huggingface.co/spaces/nameexhaustion/polars-docs/blob/main/orders.feather</p>"},{"location":"user-guide/io/json/","title":"JSON files","text":"<p>Polars can read and write both standard JSON and newline-delimited JSON (NDJSON).</p>"},{"location":"user-guide/io/json/#read","title":"Read","text":""},{"location":"user-guide/io/json/#json","title":"JSON","text":"<p>Reading a JSON file should look familiar:</p>  Python Rust <p> <code>read_json</code> <pre><code>df = pl.read_json(\"docs/assets/data/path.json\")\n</code></pre></p> <p> <code>JsonReader</code> \u00b7  Available on feature json <pre><code>use polars::prelude::*;\n\nlet mut file = std::fs::File::open(\"docs/assets/data/path.json\").unwrap();\nlet df = JsonReader::new(&amp;mut file).finish()?;\n</code></pre></p>"},{"location":"user-guide/io/json/#newline-delimited-json","title":"Newline Delimited JSON","text":"<p>JSON objects that are delimited by newlines can be read into Polars in a much more performant way than standard json.</p> <p>Polars can read an NDJSON file into a <code>DataFrame</code> using the <code>read_ndjson</code> function:</p>  Python Rust <p> <code>read_ndjson</code> <pre><code>df = pl.read_ndjson(\"docs/assets/data/path.json\")\n</code></pre></p> <pre><code>let df = LazyJsonLineReader::new(PlRefPath::new(\"docs/assets/data/path.json\"))\n    .finish()\n    .unwrap()\n    .collect()\n    .unwrap();\n</code></pre>"},{"location":"user-guide/io/json/#write","title":"Write","text":"Python Rust <p> <code>write_json</code> \u00b7 <code>write_ndjson</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_json(\"docs/assets/data/path.json\")\n</code></pre></p> <p> <code>JsonWriter</code> \u00b7 <code>JsonWriter</code> \u00b7  Available on feature json <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/assets/data/path.json\").unwrap();\n\n// json\nJsonWriter::new(&amp;mut file)\n    .with_json_format(JsonFormat::Json)\n    .finish(&amp;mut df)\n    .unwrap();\n\n// ndjson\nJsonWriter::new(&amp;mut file)\n    .with_json_format(JsonFormat::JsonLines)\n    .finish(&amp;mut df)\n    .unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json/#scan","title":"Scan","text":"<p>Polars allows you to scan a NDJSON input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_ndjson</code> <pre><code>df = pl.scan_ndjson(\"docs/assets/data/path.json\")\n</code></pre></p> <p> <code>LazyJsonLineReader</code> \u00b7  Available on feature json <pre><code>let lf = LazyJsonLineReader::new(PlRefPath::new(\"docs/assets/data/path.json\"))\n    .finish()\n    .unwrap();\n</code></pre></p>"},{"location":"user-guide/io/multiple/","title":"Multiple","text":""},{"location":"user-guide/io/multiple/#dealing-with-multiple-files","title":"Dealing with multiple files.","text":"<p>Polars can deal with multiple files differently depending on your needs and memory strain.</p> <p>Let's create some files to give us some context:</p>  Python <p> <code>write_csv</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"ham\", \"spam\"]})\n\nfor i in range(5):\n    df.write_csv(f\"docs/assets/data/my_many_files_{i}.csv\")\n</code></pre></p>"},{"location":"user-guide/io/multiple/#reading-into-a-single-dataframe","title":"Reading into a single <code>DataFrame</code>","text":"<p>To read multiple files into a single <code>DataFrame</code>, we can use globbing patterns:</p>  Python <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/assets/data/my_many_files_*.csv\")\nprint(df)\n</code></pre></p> <pre><code>shape: (15, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar  \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 \u2026   \u2506 \u2026    \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see how this works we can take a look at the query plan. Below we see that all files are read separately and concatenated into a single <code>DataFrame</code>. Polars will try to parallelize the reading.</p>  Python <p> <code>show_graph</code> <pre><code>pl.scan_csv(\"docs/assets/data/my_many_files_*.csv\").show_graph()\n</code></pre></p> <p></p>"},{"location":"user-guide/io/multiple/#reading-and-processing-in-parallel","title":"Reading and processing in parallel","text":"<p>If your files don't have to be in a single table you can also build a query plan for each file and execute them in parallel on the Polars thread pool.</p> <p>All query plan execution is embarrassingly parallel and doesn't require any communication.</p>  Python <p> <code>scan_csv</code> <pre><code>import glob\n\nimport polars as pl\n\nqueries = []\nfor file in glob.glob(\"docs/assets/data/my_many_files_*.csv\"):\n    q = pl.scan_csv(file).group_by(\"bar\").agg(pl.len(), pl.sum(\"foo\"))\n    queries.append(q)\n\ndataframes = pl.collect_all(queries)\nprint(dataframes)\n</code></pre></p> <pre><code>[shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518]\n</code></pre>"},{"location":"user-guide/io/parquet/","title":"Parquet","text":"<p>Loading or writing <code>Parquet</code> files is lightning fast as the layout of data in a Polars <code>DataFrame</code> in memory mirrors the layout of a Parquet file on disk in many respects.</p> <p>Unlike CSV, Parquet is a columnar format. This means that the data is stored in columns rather than rows. This is a more efficient way of storing data as it allows for better compression and faster access to data.</p>"},{"location":"user-guide/io/parquet/#read","title":"Read","text":"<p>We can read a <code>Parquet</code> file into a <code>DataFrame</code> using the <code>read_parquet</code> function:</p>  Python Rust <p> <code>read_parquet</code> <pre><code>df = pl.read_parquet(\"docs/assets/data/path.parquet\")\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7  Available on feature parquet <pre><code>let mut file = std::fs::File::open(\"docs/assets/data/path.parquet\").unwrap();\n\nlet df = ParquetReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/parquet/#write","title":"Write","text":"Python Rust <p> <code>write_parquet</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_parquet(\"docs/assets/data/path.parquet\")\n</code></pre></p> <p> <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/assets/data/path.parquet\").unwrap();\nParquetWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p>"},{"location":"user-guide/io/parquet/#scan","title":"Scan","text":"<p>Polars allows you to scan a <code>Parquet</code> input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\"docs/assets/data/path.parquet\")\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>let args = ScanArgsParquet::default();\nlet lf =\n    LazyFrame::scan_parquet(PlRefPath::new(\"docs/assets/data/path.parquet\"), args).unwrap();\n</code></pre></p> <p>If you want to know why this is desirable, you can read more about those Polars optimizations here.</p> <p>When we scan a <code>Parquet</code> file stored in the cloud, we can also apply predicate and projection pushdowns. This can significantly reduce the amount of data that needs to be downloaded. For scanning a Parquet file in the cloud, see Cloud storage.</p>"},{"location":"user-guide/io/sheets_colab/","title":"Google Sheets (via Colab)","text":"<p>Google Colab provides a utility class to read from and write to Google Sheets.</p>"},{"location":"user-guide/io/sheets_colab/#opening-and-reading-from-a-sheet","title":"Opening and reading from a sheet","text":"<p>We can open existing sheets by initializing <code>sheets.InteractiveSheet</code> with either:</p> <ul> <li>the <code>url</code> parameter, for example   https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/</li> <li>the <code>sheet_id</code> parameter for example 1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms</li> </ul> <p>By default the left-most worksheets will be used, we can change this by providing either <code>worksheet_id</code> or <code>worksheet_name</code>.</p> <p>The first time in each session that we use <code>InteractiveSheet</code> we will need to give Colab permission to edit our drive assets on our behalf.</p>  Python <pre><code>import polars as pl\nfrom google.colab import sheets\nurl = \"https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms\"\nsheet = sheets.InteractiveSheet(url=url, backend=\"polars\", display=False)\nsheet.as_df()\n</code></pre>"},{"location":"user-guide/io/sheets_colab/#creating-a-new-sheet","title":"Creating a new sheet","text":"<p>When you don't provide the source of the spreadsheet one will be created for you.</p>  Python <pre><code>sheet = sheets.InteractiveSheet(title=\"Colab &lt;3 Polars\", backend=\"polars\")\n</code></pre> <p>When you pass the <code>df</code> parameter the data will be written to the sheet immediately.</p>  Python <pre><code>df = pl.DataFrame({\"a\": [1,2,3], \"b\": [\"a\", \"b\", \"c\"]})\nsheet = sheets.InteractiveSheet(df=df, title=\"Colab &lt;3 Polars\", backend=\"polars\")\n</code></pre>"},{"location":"user-guide/io/sheets_colab/#writing-to-a-sheet","title":"Writing to a sheet","text":"<p>By default the <code>update</code> method will clear the worksheet and write the dataframe in the top left corner.</p>  Python <pre><code>sheet.update(df)\n</code></pre> <p>We can modify where the data is written with the <code>location</code> parameter and whether the worksheet is cleared before with <code>clear</code>.</p>  Python <pre><code>sheet.update(df, clear=False)\nsheet.update(df, location=\"D3\")\nsheet.update(df, location=(3, 4))\n</code></pre> <p>A good way to write multiple dataframes onto a worksheet in a loop is:</p>  Python <pre><code>for i, df in dfs:\n  df = pl.select(x=pl.arange(5)).with_columns(pow=pl.col(\"x\") ** i)\n  sheet.update(df, loc=(1, i * 3), clear=i == 0)\n</code></pre> <p>This clears the worksheet then writes the dataframes next to each other, one every five columns.</p>"},{"location":"user-guide/lazy/","title":"Lazy","text":"<p>The Lazy chapter is a guide for working with <code>LazyFrames</code>. It covers the functionalities like how to use it and how to optimise it. You can also find more information about the query plan or gain more insight in the streaming capabilities.</p> <ul> <li>Using lazy API</li> <li>Optimisations</li> <li>Schemas</li> <li>Query plan</li> <li>Execution</li> <li>Sources &amp; Sinks</li> <li>GPU Support</li> </ul>"},{"location":"user-guide/lazy/datatype_exprs/","title":"DataType Expressions","text":"<p>In your lazy queries, you may want to reason about the datatypes of columns or expressions used in your queries. DataType expressions allow for the inspection and manipulation of datatypes that are used in your query. The datatypes are resolved during query planning and behave the same as static datatypes during runtime.</p> <p>DataType expressions can be especially useful when you don't have full control over input data. This can occur when you try to compartmentalize code, write utility functions or are loading data from heterogeneous data sources. DataType expressions also allow you to express relations between the datatype of expressions or columns.</p>"},{"location":"user-guide/lazy/datatype_exprs/#basic-usage","title":"Basic Usage","text":"<p>DataType expressions often start with <code>pl.dtype_of</code>. This allows inspecting the datatype of a column or expression.</p>  Python <p> <code>dtype_of</code> <pre><code>dtype_expr = pl.dtype_of(\"UserID\")\n\n# For debugging you can collect the output datatype in a specific context.\nschema = pl.Schema({ 'UserID': pl.UInt64, 'Name': pl.String })\ndtype_expr.collect_dtype(schema)\n</code></pre></p> <p></p> <p>These expressions can be manipulated in various ways to transform them into the datatype that you need.</p>  Python <pre><code>dtype_expr.wrap_in_list().collect_dtype(schema)\n\ndtype_expr.to_signed_integer().collect_dtype(schema)\n</code></pre> <p></p> <p>You can also inspect information about the datatype to use at runtime.</p>  Python <pre><code>df = schema.to_frame()\ndf.select(\n    userid_dtype_name = pl.dtype_of('UserID').display(),\n    userid_is_signed  = pl.dtype_of('UserID').matches(cs.signed_integer()),\n)\n</code></pre> <p></p>"},{"location":"user-guide/lazy/datatype_exprs/#expressing-relations-between-datatypes","title":"Expressing relations between datatypes","text":"<p>Datatypes can help with utility functions by being able to express the relation between the output datatype of two expressions. The following example allows you to express that <code>map_batches</code> has the same output datatype as input datatype.</p>  Python <p> <code>map_batches</code> <pre><code>def inspect(expr: pl.Expr) -&gt; pl.Expr:\n    def print_and_return(s: pl.Series) -&gt; pl.Series:\n        print(s)\n        return s\n\n    return expr.map_batches(\n        print_and_return,\n\n        # Clarify that the expression returns the same datatype as the input\n        # datatype.\n        return_dtype=pl.dtype_of(expr),\n    )\n\ndf = pl.DataFrame({\n    'UserID': [1, 2, 3, 4, 5],\n    'Name': [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Ethan\"],\n})\ndf.select(inspect(pl.col('Name')))\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'Name' [str]\n[\n    \"Alice\"\n    \"Bob\"\n    \"Charlie\"\n    \"Diana\"\n    \"Ethan\"\n]\n</code></pre> <p>Similarly, you want to express that one column needs to be casted to the datatype of another column.</p>  Python <p> <code>cast</code> <pre><code>df = pl.DataFrame({\n    'UserID': [1, 2, 3, 4, 5],\n    'Name': [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Ethan\"],\n}).with_columns(\n    pl.col('UserID').cast(pl.dtype_of('Name'))\n)\n</code></pre></p> <p></p>"},{"location":"user-guide/lazy/execution/","title":"Query execution","text":"<p>Our example query on the Reddit dataset is:</p>  Python <p> <code>scan_csv</code> <pre><code>q1 = (\n    pl.scan_csv(\"docs/assets/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.</p> <p>When we execute the code Polars executes the optimized query graph by default.</p>"},{"location":"user-guide/lazy/execution/#execution-on-the-full-dataset","title":"Execution on the full dataset","text":"<p>We can execute our query on the full dataset by calling the <code>.collect</code> method on the query.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q4 = (\n    pl.scan_csv(f\"docs/assets/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect()\n)\n</code></pre></p> <pre><code>shape: (14_029, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id      \u2506 name                      \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 ---     \u2506 ---                       \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64     \u2506 str                       \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 TAOJIANLONG_JASONBROKEN   \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2502 17      \u2506 SSAIG_JASONBROKEN         \u2506 1397113544  \u2506 1536527864 \u2506 1             \u2506 0          \u2502\n\u2502 19      \u2506 FDBVFDSSDGFDS_JASONBROKEN \u2506 1397113552  \u2506 1536527864 \u2506 3             \u2506 0          \u2502\n\u2502 37      \u2506 IHATEWHOWEARE_JASONBROKEN \u2506 1397113636  \u2506 1536527864 \u2506 61            \u2506 0          \u2502\n\u2502 \u2026       \u2506 \u2026                         \u2506 \u2026           \u2506 \u2026          \u2506 \u2026             \u2506 \u2026          \u2502\n\u2502 1229384 \u2506 DSFOX                     \u2506 1163177415  \u2506 1536497412 \u2506 44411         \u2506 7917       \u2502\n\u2502 1229459 \u2506 NEOCARTY                  \u2506 1163177859  \u2506 1536533090 \u2506 40            \u2506 0          \u2502\n\u2502 1229587 \u2506 TEHSMA                    \u2506 1163178847  \u2506 1536497412 \u2506 14794         \u2506 5707       \u2502\n\u2502 1229621 \u2506 JEREMYLOW                 \u2506 1163179075  \u2506 1536497412 \u2506 411           \u2506 1063       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Above we see that from the 10 million rows there are 14,029 rows that match our predicate.</p> <p>With the default <code>collect</code> method Polars processes all of your data as one batch. This means that all the data has to fit into your available memory at the point of peak memory usage in your query.</p> <p>Reusing <code>LazyFrame</code> objects</p> <p>Remember that <code>LazyFrame</code>s are query plans i.e. a promise on computation and is not guaranteed to cache common subplans. This means that every time you reuse it in separate downstream queries after it is defined, it is computed all over again. If you define an operation on a <code>LazyFrame</code> that doesn't maintain row order (such as a <code>group_by</code>), then the order will also change every time it is run. To avoid this, use <code>maintain_order=True</code> arguments for such operations.</p>"},{"location":"user-guide/lazy/execution/#execution-on-larger-than-memory-data","title":"Execution on larger-than-memory data","text":"<p>If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the <code>engine=\"streaming\"</code> argument to <code>collect</code></p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q5 = (\n    pl.scan_csv(f\"docs/assets/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect(engine='streaming')\n)\n</code></pre></p>"},{"location":"user-guide/lazy/execution/#execution-on-a-partial-dataset","title":"Execution on a partial dataset","text":"<p>While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.</p> <p>Instead, you can scan a subset of your partitions or use <code>.head</code>/<code>.collect</code> at the beginning and end of your query, respectively. Keep in mind that the results of aggregations and filters on subsets of your data may not be representative of the result you would get on the full data.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> \u00b7 <code>head</code> <pre><code>q9 = (\n    pl.scan_csv(f\"docs/assets/data/reddit.csv\")\n    .head(10)\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect()\n)\n</code></pre></p> <pre><code>shape: (1, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 name                    \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 --- \u2506 ---                     \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64 \u2506 str                     \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6   \u2506 TAOJIANLONG_JASONBROKEN \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/execution/#diverging-queries","title":"Diverging queries","text":"<p>It is very common that a query diverges at one point. In these cases it is recommended to use <code>collect_all</code> as they will ensure that diverging queries execute only once.</p> <pre><code># Some expensive LazyFrame\nlf: LazyFrame\n\nlf_1 = lf.select(pl.all().sum())\n\nlf_2 = lf.some_other_computation()\n\npl.collect_all([lf_1, lf_2]) # this will execute lf only once!\n</code></pre>"},{"location":"user-guide/lazy/gpu/","title":"GPU Support","text":"<p>Polars provides an in-memory, GPU-accelerated execution engine for the Lazy API in Python using RAPIDS cuDF on NVIDIA GPUs. This functionality is available in Open Beta, is undergoing rapid development, and is currently a single GPU implementation.</p> <p>If you install Polars with the GPU feature flag, you can trigger GPU-based execution by running <code>.collect(engine=\"gpu\")</code> instead of <code>.collect()</code>.</p>  Python <pre><code>import polars as pl\n\ndf = pl.LazyFrame({\"a\": [1.242, 1.535]})\n\nq = df.select(pl.col(\"a\").round(1))\n\nresult = q.collect(engine=\"gpu\")\nprint(result)\n</code></pre> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.2 \u2502\n\u2502 1.5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Learn more in the GPU Support guide.</p>"},{"location":"user-guide/lazy/multiplexing/","title":"Multiplexing queries","text":"<p>In the Sources and Sinks page, we already discussed multiplexing as a way to split a query into multiple sinks. This page will go a bit deeper in this concept, as it is important to understand when combining <code>LazyFrame</code>s with procedural programming constructs.</p> <p>When dealing with eager dataframes, it is very common to keep state in a temporary variable. Let's look at the following example. Below we create a <code>DataFrame</code> with 10 unique elements in a random order (so that Polars doesn't hit any fast paths for sorted keys).</p>  Python <pre><code>np.random.seed(0)\na = np.arange(0, 10)\nnp.random.shuffle(a)\ndf = pl.DataFrame({\"n\": a})\nprint(df)\n</code></pre> <pre><code>shape: (10, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 n   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2502\n\u2502 8   \u2502\n\u2502 4   \u2502\n\u2502 9   \u2502\n\u2502 1   \u2502\n\u2502 6   \u2502\n\u2502 7   \u2502\n\u2502 3   \u2502\n\u2502 0   \u2502\n\u2502 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/multiplexing/#eager","title":"Eager","text":"<p>If you deal with the Polars eager API, making a variable and iterating over that temporary <code>DataFrame</code> gives the result you expect, as the result of the group-by is stored in <code>df1</code>. Even though the output order is unstable, it doesn't matter as it is eagerly evaluated. The follow snippet therefore doesn't raise and the assert passes.</p>  Python <pre><code># A group-by doesn't guarantee order\ndf1 = df.group_by(\"n\").len()\n\n# Take the lower half and the upper half in a list\nout = [df1.slice(offset=i * 5, length=5) for i in range(2)]\n\n# Assert df1 is equal to the sum of both halves\npl.testing.assert_frame_equal(df1, pl.concat(out))\n</code></pre>"},{"location":"user-guide/lazy/multiplexing/#lazy","title":"Lazy","text":"<p>Now if we tried this naively with <code>LazyFrame</code>s, this would fail.</p>  Python <pre><code>lf1 = df.lazy().group_by(\"n\").len()\n\nout = [lf1.slice(offset=i * 5, length=5).collect() for i in range(2)]\n\npl.testing.assert_frame_equal(lf1.collect(), pl.concat(out))\n</code></pre> <pre><code>AssertionError: DataFrames are different (value mismatch for column 'n')\n[left]:  [9, 2, 0, 5, 3, 1, 7, 8, 6, 4]\n[right]: [0, 9, 6, 8, 2, 5, 4, 3, 1, 7]\n</code></pre> <p>The reason this fails is that <code>lf1</code> doesn't contain the materialized result of <code>df.lazy().group_by(\"n\").len()</code>, it instead holds the query plan in that variable.</p> <p></p> <p>This means that every time we branch of this <code>LazyFrame</code> and call <code>collect</code> we re-evaluate the group-by. Besides being expensive, this also leads to unexpected results if you assume that the output is stable (which isn't the case here).</p> <p>In the example above you are actually evaluating 2 query plans:</p> <p>Plan 1</p> <p></p> <p>Plan 2</p> <p></p>"},{"location":"user-guide/lazy/multiplexing/#combine-the-query-plans","title":"Combine the query plans","text":"<p>To circumvent this, we must give Polars the opportunity to look at all the query plans in a single optimization and execution pass. This can be done by passing the diverging <code>LazyFrame</code>'s to the <code>collect_all</code> function.</p>  Python <pre><code>lf1 = df.lazy().group_by(\"n\").len()\n\nout = [lf1.slice(offset=i * 5, length=5) for i in range(2)]\nresults = pl.collect_all([lf1] + out)\n\npl.testing.assert_frame_equal(results[0], pl.concat(results[1:]))\n</code></pre> <p>If we explain the combined queries with <code>pl.explain_all</code>, we can also observe that they are shared under a single \"SINK_MULTIPLE\" evaluation and that the optimizer has recognized that parts of the query come from the same subplan, indicated by the inserted \"CACHE\" nodes.</p> <pre><code>SINK_MULTIPLE\n  PLAN 0:\n    CACHE[id: cdeb7ed0-310d-48ae-bf71-76471a7882bb]\n      AGGREGATE[maintain_order: false]\n        [len()] BY [col(\"n\")]\n        FROM\n        DF [\"n\"]; PROJECT[\"n\"] 1/1 COLUMNS\n  PLAN 1:\n    SLICE[offset: 0, len: 5]\n      CACHE[id: cdeb7ed0-310d-48ae-bf71-76471a7882bb]\n        AGGREGATE[maintain_order: false]\n          [len()] BY [col(\"n\")]\n          FROM\n          DF [\"n\"]; PROJECT[\"n\"] 1/1 COLUMNS\n  PLAN 2:\n    SLICE[offset: 5, len: 5]\n      CACHE[id: cdeb7ed0-310d-48ae-bf71-76471a7882bb]\n        AGGREGATE[maintain_order: false]\n          [len()] BY [col(\"n\")]\n          FROM\n          DF [\"n\"]; PROJECT[\"n\"] 1/1 COLUMNS\nEND SINK_MULTIPLE\n</code></pre> <p>Combining related subplans in a single execution unit with <code>pl.collect_all</code> can thus lead to large performance increases and allows diverging query plans, storing temporary tables, and a more procedural programming style.</p>"},{"location":"user-guide/lazy/optimizations/","title":"Optimizations","text":"<p>If you use Polars' lazy API, Polars will run several optimizations on your query. Some of them are executed up front, others are determined just in time as the materialized data comes in.</p> <p>Here is a non-complete overview of optimizations done by polars, what they do and how often they run.</p> Optimization Explanation runs Predicate pushdown Applies filters as early as possible/ at scan level. 1 time Projection pushdown Select only the columns that are needed at the scan level. 1 time Slice pushdown Only load the required slice from the scan level. Don't materialize sliced outputs (e.g. join.head(10)). 1 time Common subplan elimination Cache subtrees/file scans that are used by multiple subtrees in the query plan. 1 time Simplify expressions Various optimizations, such as constant folding and replacing expensive operations with faster alternatives. until fixed point Join ordering Estimates the branches of joins that should be executed first in order to reduce memory pressure. 1 time Type coercion Coerce types such that operations succeed and run on minimal required memory. until fixed point Cardinality estimation Estimates cardinality in order to determine optimal group by strategy. 0/n times; dependent on query"},{"location":"user-guide/lazy/query-plan/","title":"Query plan","text":"<p>For any lazy query Polars has both:</p> <ul> <li>a non-optimized plan with the set of steps code as we provided it and</li> <li>an optimized plan with changes made by the query optimizer</li> </ul> <p>We can understand both the non-optimized and optimized query plans with visualization and by printing them as text.</p> <p>Below we consider the following query:</p>  Python <pre><code>q1 = (\n    pl.scan_csv(\"docs/assets/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre> <p></p>"},{"location":"user-guide/lazy/query-plan/#non-optimized-query-plan","title":"Non-optimized query plan","text":""},{"location":"user-guide/lazy/query-plan/#graphviz-visualization","title":"Graphviz visualization","text":"<p>To create visualizations of the query plan, Graphviz should be installed and added to your PATH.</p> <p>First we visualize the non-optimized plan by setting <code>optimized=False</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1.show_graph(optimized=False)\n</code></pre></p> <p></p> <p>The query plan visualization should be read from bottom to top. In the visualization:</p> <ul> <li>each box corresponds to a stage in the query plan</li> <li>the <code>sigma</code> stands for <code>SELECTION</code> and indicates any filter conditions</li> <li>the <code>pi</code> stands for <code>PROJECTION</code> and indicates choosing a subset of columns</li> </ul>"},{"location":"user-guide/lazy/query-plan/#printed-query-plan","title":"Printed query plan","text":"<p>We can also print the non-optimized plan with <code>explain(optimized=False)</code></p>  Python <p> <code>explain</code> <pre><code>q1.explain(optimized=False)\n</code></pre></p> <p></p> <pre><code>FILTER [(col(\"comment_karma\")) &gt; (0)] FROM WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n</code></pre> <p>The printed plan should also be read from bottom to top. This non-optimized plan is roughly equal to:</p> <ul> <li>read from the <code>data/reddit.csv</code> file</li> <li>read all 6 columns (where the * wildcard in PROJECT */6 COLUMNS means take all columns)</li> <li>transform the <code>name</code> column to uppercase</li> <li>apply a filter on the <code>comment_karma</code> column</li> </ul>"},{"location":"user-guide/lazy/query-plan/#optimized-query-plan","title":"Optimized query plan","text":"<p>Now we visualize the optimized plan with <code>show_graph</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1.show_graph()\n</code></pre></p> <p></p> <p>We can also print the optimized plan with <code>explain</code></p>  Python <p> <code>explain</code> <pre><code>q1.explain()\n</code></pre></p> <pre><code> WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n    SELECTION: [(col(\"comment_karma\")) &gt; (0)]\n</code></pre> <p>The optimized plan is to:</p> <ul> <li>read the data from the Reddit CSV</li> <li>apply the filter on the <code>comment_karma</code> column while the CSV is being read line-by-line</li> <li>transform the <code>name</code> column to uppercase</li> </ul> <p>In this case the query optimizer has identified that the <code>filter</code> can be applied while the CSV is read from disk rather than reading the whole file into memory and then applying the filter. This optimization is called Predicate Pushdown.</p>"},{"location":"user-guide/lazy/schemas/","title":"Schema","text":"<p>The schema of a Polars <code>DataFrame</code> or <code>LazyFrame</code> sets out the names of the columns and their datatypes. You can see the schema with the <code>.collect_schema</code> method on a <code>DataFrame</code> or <code>LazyFrame</code></p>  Python <p> <code>LazyFrame</code> <pre><code>lf = pl.LazyFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]})\n\nprint(lf.collect_schema())\n</code></pre></p> <pre><code>Schema({'foo': String, 'bar': Int64})\n</code></pre> <p>The schema plays an important role in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#type-checking-in-the-lazy-api","title":"Type checking in the lazy API","text":"<p>One advantage of the lazy API is that Polars will check the schema before any data is processed. This check happens when you execute your lazy query.</p> <p>We see how this works in the following simple example where we call the <code>.round</code> expression on the string column <code>foo</code>.</p>  Python <p> <code>with_columns</code> <pre><code>lf = pl.LazyFrame({\"foo\": [\"a\", \"b\", \"c\"]}).with_columns(pl.col(\"foo\").round(2))\n</code></pre></p> <p>The <code>.round</code> expression is only valid for columns with a numeric data type. Calling <code>.round</code> on a string column means the operation will raise an <code>InvalidOperationError</code> when we evaluate the query with <code>collect</code>. This schema check happens before the data is processed when we call <code>collect</code>.</p>  Python <pre><code>try:\n    print(lf.collect())\nexcept Exception as e:\n    print(f\"{type(e).__name__}: {e}\")\n</code></pre> <pre><code>InvalidOperationError: round can only be used on numeric types\n</code></pre> <p>If we executed this query in eager mode the error would only be found once the data had been processed in all earlier steps.</p> <p>When we execute a lazy query Polars checks for any potential <code>InvalidOperationError</code> before the time-consuming step of actually processing the data in the pipeline.</p>"},{"location":"user-guide/lazy/schemas/#the-lazy-api-must-know-the-schema","title":"The lazy API must know the schema","text":"<p>In the lazy API the Polars query optimizer must be able to infer the schema at every step of a query plan. This means that operations where the schema is not knowable in advance cannot be used with the lazy API.</p> <p>The classic example of an operation where the schema is not knowable in advance is a <code>.pivot</code> operation. In a <code>.pivot</code> the new column names come from data in one of the columns. As these column names cannot be known in advance a <code>.pivot</code> is not available in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#dealing-with-operations-not-available-in-the-lazy-api","title":"Dealing with operations not available in the lazy API","text":"<p>If your pipeline includes an operation that is not available in the lazy API it is normally best to:</p> <ul> <li>run the pipeline in lazy mode up until that point</li> <li>execute the pipeline with <code>.collect</code> to materialize a <code>DataFrame</code></li> <li>do the non-lazy operation on the <code>DataFrame</code></li> <li>convert the output back to a <code>LazyFrame</code> with <code>.lazy</code> and continue in lazy mode</li> </ul> <p>We show how to deal with a non-lazy operation in this example where we:</p> <ul> <li>create a simple <code>DataFrame</code></li> <li>convert it to a <code>LazyFrame</code> with <code>.lazy</code></li> <li>do a transformation using <code>.with_columns</code></li> <li>execute the query before the pivot with <code>.collect</code> to get a <code>DataFrame</code></li> <li>do the <code>.pivot</code> on the <code>DataFrame</code></li> <li>convert back in lazy mode</li> <li>do a <code>.filter</code></li> <li>finish by executing the query with <code>.collect</code> to get a <code>DataFrame</code></li> </ul>  Python <p> <code>collect</code> \u00b7 <code>lazy</code> \u00b7 <code>pivot</code> \u00b7 <code>filter</code> <pre><code>lazy_eager_query = (\n    pl.LazyFrame(\n        {\n            \"id\": [\"a\", \"b\", \"c\"],\n            \"month\": [\"jan\", \"feb\", \"mar\"],\n            \"values\": [0, 1, 2],\n        }\n    )\n    .with_columns((2 * pl.col(\"values\")).alias(\"double_values\"))\n    .collect()\n    .pivot(index=\"id\", on=\"month\", values=\"double_values\", aggregate_function=\"first\")\n    .lazy()\n    .filter(pl.col(\"mar\").is_null())\n    .collect()\n)\nprint(lazy_eager_query)\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 jan  \u2506 feb  \u2506 mar  \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 0    \u2506 null \u2506 null \u2502\n\u2502 b   \u2506 null \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/sources_sinks/","title":"Sources and sinks","text":""},{"location":"user-guide/lazy/sources_sinks/#scan","title":"Scan","text":"<p>When using the <code>LazyFrame</code> API, it is important to favor <code>scan_*</code> (<code>scan_parquet</code>, <code>scan_csv</code>, etc.) over <code>read_*</code>. A Polars <code>scan</code> is lazy and will delay execution until the query is collected. The benefit of this, is that the Polars optimizer can push optimization into the readers. They can skip reading columns and rows that aren't required. Another benefit is that, during streaming execution, the engine already can process batches before the file is completely read.</p>"},{"location":"user-guide/lazy/sources_sinks/#sink","title":"Sink","text":"<p>Sinks can execute a query and stream the results to storage (being disk or cloud). The benefit of sinking data to storage is that you don't necessarily have to store all data in RAM, but can process data in batches.</p> <p>If we would want to convert many csv files to parquet, whilst dropping the missing data, we could do something like the query below. We use a partitioning strategy that defines how many rows may be in a single parquet file, before we generate a new file</p> <pre><code>lf = scan_csv(\"my_dataset/*.csv\").filter(pl.all().is_not_null())\nlf.sink_parquet(\n    pl.PartitionBy(\n        \"output_folder/\"\n        max_rows_per_file=512_000\n    )\n)\n</code></pre> <p>This will create the following files on disk:</p> <pre><code>output_folder/00000000.parquet\noutput_folder/00000001.parquet\n...\noutput_folder/0000000f.parquet\noutput_folder/00000010.parquet\n</code></pre>"},{"location":"user-guide/lazy/sources_sinks/#multiplexing-sinks","title":"Multiplexing sinks","text":"<p>Sinks can also multiplex. Meaning that we write to different sinks in a single query. In the code snippet below, we take a <code>LazyFrame</code> and sink it into 2 sinks at the same time.</p> <pre><code># Some expensive computation\nlf: LazyFrame\n\nq1 = lf.sink_parquet(.., lazy=True)\nq2 = lf.sink_ipc(.., lazy=True)\n\npl.collect_all([q1, q2])\n</code></pre>"},{"location":"user-guide/lazy/using/","title":"Usage","text":"<p>With the lazy API, Polars doesn't run each query line-by-line but instead processes the full query end-to-end. To get the most out of Polars it is important that you use the lazy API because:</p> <ul> <li>the lazy API allows Polars to apply automatic query optimization with the query optimizer</li> <li>the lazy API allows you to work with larger than memory datasets using streaming</li> <li>the lazy API can catch schema errors before processing the data</li> </ul> <p>Here we see how to use the lazy API starting from either a file or an existing <code>DataFrame</code>.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-file","title":"Using the lazy API from a file","text":"<p>In the ideal case we would use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.</p> <p>We create a lazy query from the Reddit CSV data and apply some transformations.</p> <p>By starting the query with <code>pl.scan_csv</code> we are using the lazy API.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>with_columns</code> \u00b7 <code>filter</code> \u00b7 <code>col</code> <pre><code>q1 = (\n    pl.scan_csv(f\"docs/assets/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>A <code>pl.scan_</code> function is available for a number of file types including CSV, IPC, Parquet and JSON.</p> <p>In this query we tell Polars that we want to:</p> <ul> <li>load data from the Reddit CSV file</li> <li>convert the <code>name</code> column to uppercase</li> <li>apply a filter to the <code>comment_karma</code> column</li> </ul> <p>The lazy query will not be executed at this point. See this page on executing lazy queries for more on running lazy queries.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-dataframe","title":"Using the lazy API from a <code>DataFrame</code>","text":"<p>An alternative way to access the lazy API is to call <code>.lazy</code> on a <code>DataFrame</code> that has already been created in memory.</p>  Python <p> <code>lazy</code> <pre><code>q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n</code></pre></p> <p>By calling <code>.lazy</code> we convert the <code>DataFrame</code> to a <code>LazyFrame</code>.</p>"},{"location":"user-guide/migration/pandas/","title":"Coming from Pandas","text":"<p>Here we set out the key points that anyone who has experience with pandas and wants to try Polars should know. We include both differences in the concepts the libraries are built on and differences in how you should write Polars code compared to pandas code.</p>"},{"location":"user-guide/migration/pandas/#differences-in-concepts-between-polars-and-pandas","title":"Differences in concepts between Polars and pandas","text":""},{"location":"user-guide/migration/pandas/#polars-does-not-have-a-multi-indexindex","title":"Polars does not have a multi-index/index","text":"<p>pandas gives a label to each row with an index. Polars does not use an index and each row is indexed by its integer position in the table.</p> <p>Polars aims to have predictable results and readable queries, as such we think an index does not help us reach that objective. We believe the semantics of a query should not change by the state of an index or a <code>reset_index</code> call.</p> <p>In Polars a DataFrame will always be a 2D table with heterogeneous data-types. The data-types may have nesting, but the table itself will not. Operations like resampling will be done by specialized functions or methods that act like 'verbs' on a table explicitly stating the columns that 'verb' operates on. As such, it is our conviction that not having indices make things simpler, more explicit, more readable and less error-prone.</p> <p>Note that an 'index' data structure as known in databases will be used by Polars as an optimization technique.</p>"},{"location":"user-guide/migration/pandas/#polars-adheres-to-the-apache-arrow-memory-format-to-represent-data-in-memory-while-pandas-uses-numpy-arrays","title":"Polars adheres to the Apache Arrow memory format to represent data in memory while pandas uses NumPy arrays","text":"<p>Polars represents data in memory according to the Arrow memory spec while pandas by default represents data in memory with NumPy arrays. Apache Arrow is an emerging standard for in-memory columnar analytics that can accelerate data load times, reduce memory usage and accelerate calculations.</p> <p>Polars can convert data to NumPy format with the <code>to_numpy</code> method.</p>"},{"location":"user-guide/migration/pandas/#polars-has-more-support-for-parallel-operations-than-pandas","title":"Polars has more support for parallel operations than pandas","text":"<p>Polars exploits the strong support for concurrency in Rust to run many operations in parallel. While some operations in pandas are multi-threaded the core of the library is single-threaded and an additional library such as <code>Dask</code> must be used to parallelize operations. Polars is faster than all open source solutions that parallelize pandas code.</p>"},{"location":"user-guide/migration/pandas/#polars-has-support-for-different-engines","title":"Polars has support for different engines","text":"<p>Polars has native support for an engine optimized for in-memory processing and a streaming engine optimized for large scale data processing. Furthermore Polars has native integration with a CuDF supported engine. All these engines benefit from Polars' query optimizer and Polars ensures semantic correctness between all those engines. In pandas the implementation can dispatch between numpy and Pyarrow, but because of pandas' loose strictness guarantees, the data-type outputs and semantics between those backends can differ. This can lead to subtle bugs.</p>"},{"location":"user-guide/migration/pandas/#polars-can-lazily-evaluate-queries-and-apply-query-optimization","title":"Polars can lazily evaluate queries and apply query optimization","text":"<p>Eager evaluation is when code is evaluated as soon as you run the code. Lazy evaluation is when running a line of code means that the underlying logic is added to a query plan rather than being evaluated.</p> <p>Polars supports eager evaluation and lazy evaluation whereas pandas only supports eager evaluation. The lazy evaluation mode is powerful because Polars carries out automatic query optimization when it examines the query plan and looks for ways to accelerate the query or reduce memory usage.</p> <p><code>Dask</code> also supports lazy evaluation when it generates a query plan.</p>"},{"location":"user-guide/migration/pandas/#polars-is-strict","title":"Polars is strict","text":"<p>Polars is strict about data types. Data type resolution in Polars is dependent on the operation graph, whereas pandas converts types loosely (e.g. new missing data can lead to integer columns being converted to floats). This strictness leads to fewer bugs and more predictable behavior.</p>"},{"location":"user-guide/migration/pandas/#polars-has-a-more-versatile-api","title":"Polars has a more versatile API","text":"<p>Polars is built on expressions and allows expression inputs in almost all operations. This means that when you understand how expressions work, your knowledge in Polars extrapolates. Pandas doesn't have an expression system and often requires Python <code>lambda</code>s to express the complexity you want. Polars sees the requirement of a Python <code>lambda</code> as a lack of expressiveness of its API, and tries to give you native support whenever possible.</p>"},{"location":"user-guide/migration/pandas/#key-syntax-differences","title":"Key syntax differences","text":"<p>Users coming from pandas generally need to know one thing...</p> <pre><code>polars != pandas\n</code></pre> <p>If your Polars code looks like it could be pandas code, it might run, but it likely runs slower than it should.</p> <p>Let's go through some typical pandas code and see how we might rewrite it in Polars.</p>"},{"location":"user-guide/migration/pandas/#selecting-data","title":"Selecting data","text":"<p>As there is no index in Polars there is no <code>.loc</code> or <code>iloc</code> method in Polars - and there is also no <code>SettingWithCopyWarning</code> in Polars.</p> <p>However, the best way to select data in Polars is to use the expression API. For example, if you want to select a column in pandas, you can do one of the following:</p> <pre><code>df[\"a\"]\ndf.loc[:,\"a\"]\n</code></pre> <p>but in Polars you would use the <code>.select</code> method:</p> <pre><code>df.select(\"a\")\n</code></pre> <p>If you want to select rows based on the values then in Polars you use the <code>.filter</code> method:</p> <pre><code>df.filter(pl.col(\"a\") &lt; 10)\n</code></pre> <p>As noted in the section on expressions below, Polars can run operations in <code>.select</code> and <code>filter</code> in parallel and Polars can carry out query optimization on the full set of data selection criteria.</p>"},{"location":"user-guide/migration/pandas/#be-lazy","title":"Be lazy","text":"<p>Working in lazy evaluation mode is straightforward and should be your default in Polars as the lazy mode allows Polars to do query optimization.</p> <p>We can run in lazy mode by either using an implicitly lazy function (such as <code>scan_csv</code>) or explicitly using the <code>lazy</code> method.</p> <p>Take the following simple example where we read a CSV file from disk and do a group by. The CSV file has numerous columns but we just want to do a group by on one of the id columns (<code>id1</code>) and then sum by a value column (<code>v1</code>). In pandas this would be:</p> <pre><code>df = pd.read_csv(csv_file, usecols=[\"id1\",\"v1\"])\ngrouped_df = df.loc[:,[\"id1\",\"v1\"]].groupby(\"id1\").sum()\n</code></pre> <p>In Polars you can build this query in lazy mode with query optimization and evaluate it by replacing the eager pandas function <code>read_csv</code> with the implicitly lazy Polars function <code>scan_csv</code>:</p> <pre><code>df = pl.scan_csv(csv_file)\ngrouped_df = df.group_by(\"id1\").agg(pl.col(\"v1\").sum()).collect()\n</code></pre> <p>Polars optimizes this query by identifying that only the <code>id1</code> and <code>v1</code> columns are relevant and so will only read these columns from the CSV. By calling the <code>.collect</code> method at the end of the second line we instruct Polars to eagerly evaluate the query.</p> <p>If you do want to run this query in eager mode you can just replace <code>scan_csv</code> with <code>read_csv</code> in the Polars code.</p> <p>Read more about working with lazy evaluation in the lazy API section.</p>"},{"location":"user-guide/migration/pandas/#express-yourself","title":"Express yourself","text":"<p>A typical pandas script consists of multiple data transformations that are executed sequentially. However, in Polars these transformations can be executed in parallel using expressions.</p>"},{"location":"user-guide/migration/pandas/#column-assignment","title":"Column assignment","text":"<p>We have a dataframe <code>df</code> with a column called <code>value</code>. We want to add two new columns, a column called <code>tenXValue</code> where the <code>value</code> column is multiplied by 10 and a column called <code>hundredXValue</code> where the <code>value</code> column is multiplied by 100.</p> <p>In pandas this would be:</p> <pre><code>df.assign(\n    tenXValue=lambda df_: df_.value * 10,\n    hundredXValue=lambda df_: df_.value * 100\n)\n</code></pre> <p>These column assignments are executed sequentially.</p> <p>In Polars we add columns to <code>df</code> using the <code>.with_columns</code> method:</p> <pre><code>df.with_columns(\n    tenXValue=pl.col(\"value\") * 10,\n    hundredXValue=pl.col(\"value\") * 100,\n)\n</code></pre> <p>These column assignments are executed in parallel.</p>"},{"location":"user-guide/migration/pandas/#column-assignment-based-on-predicate","title":"Column assignment based on predicate","text":"<p>In this case we have a dataframe <code>df</code> with columns <code>a</code>,<code>b</code> and <code>c</code>. We want to re-assign the values in column <code>a</code> based on a condition. When the value in column <code>c</code> is equal to 2 then we replace the value in <code>a</code> with the value in <code>b</code>.</p> <p>In pandas this would be:</p> <pre><code>df.assign(a=lambda df_: df_[\"a\"].mask(df_[\"c\"] == 2, df_[\"b\"]))\n</code></pre> <p>while in Polars this would be:</p> <pre><code>df.with_columns(\n    pl.when(pl.col(\"c\") == 2)\n    .then(pl.col(\"b\"))\n    .otherwise(pl.col(\"a\")).alias(\"a\")\n)\n</code></pre> <p>Polars can compute every branch of an <code>if -&gt; then -&gt; otherwise</code> in parallel. This is valuable, when the branches get more expensive to compute.</p>"},{"location":"user-guide/migration/pandas/#filtering","title":"Filtering","text":"<p>We want to filter the dataframe <code>df</code> with housing data based on some criteria.</p> <p>In pandas you filter the dataframe by passing Boolean expressions to the <code>query</code> method:</p> <pre><code>df.query(\"m2_living &gt; 2500 and price &lt; 300000\")\n</code></pre> <p>or by directly evaluating a mask:</p> <pre><code>df[(df[\"m2_living\"] &gt; 2500) &amp; (df[\"price\"] &lt; 300000)]\n</code></pre> <p>while in Polars you call the <code>filter</code> method:</p> <pre><code>df.filter(\n    (pl.col(\"m2_living\") &gt; 2500) &amp; (pl.col(\"price\") &lt; 300000)\n)\n</code></pre> <p>The query optimizer in Polars can also detect if you write multiple filters separately and combine them into a single filter in the optimized plan.</p>"},{"location":"user-guide/migration/pandas/#pandas-transform","title":"pandas transform","text":"<p>The pandas documentation demonstrates an operation on a group by called <code>transform</code>. In this case we have a dataframe <code>df</code> and we want a new column showing the number of rows in each group.</p> <p>In pandas we have:</p> <pre><code>df = pd.DataFrame({\n    \"c\": [1, 1, 1, 2, 2, 2, 2],\n    \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"],\n})\n\ndf[\"size\"] = df.groupby(\"c\")[\"type\"].transform(len)\n</code></pre> <p>Here pandas does a group by on <code>\"c\"</code>, takes column <code>\"type\"</code>, computes the group length and then joins the result back to the original <code>DataFrame</code> producing:</p> <pre><code>   c type size\n0  1    m    3\n1  1    n    3\n2  1    o    3\n3  2    m    4\n4  2    m    4\n5  2    n    4\n6  2    n    4\n</code></pre> <p>In Polars the same can be achieved with <code>window</code> functions:</p> <pre><code>df.with_columns(\n    pl.col(\"type\").count().over(\"c\").alias(\"size\")\n)\n</code></pre> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2502\n\u2502 1   \u2506 n    \u2506 3    \u2502\n\u2502 1   \u2506 o    \u2506 3    \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Because we can store the whole operation in a single expression, we can combine several <code>window</code> functions and even combine different groups!</p> <p>Polars will cache window expressions that are applied over the same group, so storing them in a single <code>with_columns</code> is both convenient and optimal. In the following example we look at a case where we are calculating group statistics over <code>\"c\"</code> twice:</p> <pre><code>df.with_columns(\n    pl.col(\"c\").count().over(\"c\").alias(\"size\"),\n    pl.col(\"c\").sum().over(\"type\").alias(\"sum\"),\n    pl.col(\"type\").reverse().over(\"c\").alias(\"reverse_type\")\n)\n</code></pre> <pre><code>shape: (7, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2506 sum \u2506 reverse_type \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 --- \u2506 ---          \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2506 i64 \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2506 5   \u2506 o            \u2502\n\u2502 1   \u2506 n    \u2506 3    \u2506 5   \u2506 n            \u2502\n\u2502 1   \u2506 o    \u2506 3    \u2506 1   \u2506 m            \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 n            \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 n            \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 m            \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 m            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/migration/pandas/#missing-data","title":"Missing data","text":"<p>pandas uses <code>NaN</code> and/or <code>None</code> values to indicate missing values depending on the dtype of the column. In addition the behaviour in pandas varies depending on whether the default dtypes or optional nullable arrays are used. In Polars missing data corresponds to a <code>null</code> value for all data types.</p> <p>For float columns Polars permits the use of <code>NaN</code> values. These <code>NaN</code> values are not considered to be missing data but instead a special floating point value.</p> <p>In pandas an integer column with missing values is cast to be a float column with <code>NaN</code> values for the missing values (unless using optional nullable integer dtypes). In Polars any missing values in an integer column are simply <code>null</code> values and the column remains an integer column.</p> <p>See the missing data section for more details.</p>"},{"location":"user-guide/migration/pandas/#pipe-littering","title":"Pipe littering","text":"<p>A common usage in pandas is utilizing <code>pipe</code> to apply some function to a <code>DataFrame</code>. Copying this coding style to Polars is unidiomatic and leads to suboptimal query plans.</p> <p>The snippet below shows a common pattern in pandas.</p> <pre><code>def add_foo(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"foo\"] = ...\n    return df\n\ndef add_bar(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"bar\"] = ...\n    return df\n\n\ndef add_ham(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"ham\"] = ...\n    return df\n\n(df\n .pipe(add_foo)\n .pipe(add_bar)\n .pipe(add_ham)\n)\n</code></pre> <p>If we do this in polars, we would create 3 <code>with_columns</code> contexts, that forces Polars to run the 3 pipes sequentially, utilizing zero parallelism.</p> <p>The way to get similar abstractions in polars is creating functions that create expressions. The snippet below creates 3 expressions that run on a single context and thus are allowed to run in parallel.</p> <pre><code>def get_foo(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"foo\")\n\ndef get_bar(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"bar\")\n\ndef get_ham(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"ham\")\n\n# This single context will run all 3 expressions in parallel\ndf.with_columns(\n    get_ham(\"col_a\"),\n    get_bar(\"col_b\"),\n    get_foo(\"col_c\"),\n)\n</code></pre> <p>If you need the schema in the functions that generate the expressions, you can utilize a single <code>pipe</code>:</p> <pre><code>from collections import OrderedDict\n\ndef get_foo(input_column: str, schema: OrderedDict) -&gt; pl.Expr:\n    if \"some_col\" in schema:\n        # branch_a\n        ...\n    else:\n        # branch b\n        ...\n\ndef get_bar(input_column: str, schema: OrderedDict) -&gt; pl.Expr:\n    if \"some_col\" in schema:\n        # branch_a\n        ...\n    else:\n        # branch b\n        ...\n\ndef get_ham(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"ham\")\n\n# Use pipe (just once) to get hold of the schema of the LazyFrame.\nlf.pipe(lambda lf: lf.with_columns(\n    get_ham(\"col_a\"),\n    get_bar(\"col_b\", lf.schema),\n    get_foo(\"col_c\", lf.schema),\n))\n</code></pre> <p>Another benefit of writing functions that return expressions is that these functions are composable, as expressions can be chained and partially applied, leading to much more flexibility in the design.</p>"},{"location":"user-guide/migration/spark/","title":"Coming from Apache Spark","text":""},{"location":"user-guide/migration/spark/#column-based-api-vs-row-based-api","title":"Column-based API vs. Row-based API","text":"<p>Whereas the <code>Spark</code> <code>DataFrame</code> is analogous to a collection of rows, a Polars <code>DataFrame</code> is closer to a collection of columns. This means that you can combine columns in Polars in ways that are not possible in <code>Spark</code>, because <code>Spark</code> preserves the relationship of the data in each row.</p> <p>Consider this sample dataset:</p> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\n    \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n    \"bar\": [1, 2, 3, 4, 5],\n})\n\ndfs = spark.createDataFrame(\n    [\n        (\"a\", 1),\n        (\"b\", 2),\n        (\"c\", 3),\n        (\"d\", 4),\n        (\"d\", 5),\n    ],\n    schema=[\"foo\", \"bar\"],\n)\n</code></pre>"},{"location":"user-guide/migration/spark/#example-1-combining-head-and-sum","title":"Example 1: Combining <code>head</code> and <code>sum</code>","text":"<p>In Polars you can write something like this:</p> <pre><code>df.select(\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").filter(pl.col(\"foo\") == \"d\").sum()\n)\n</code></pre> <p>Output:</p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 9   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 9   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The expressions on columns <code>foo</code> and <code>bar</code> are completely independent. Since the expression on <code>bar</code> returns a single value, that value is repeated for each value output by the expression on <code>foo</code>. But <code>a</code> and <code>b</code> have no relation to the data that produced the sum of <code>9</code>.</p> <p>To do something similar in <code>Spark</code>, you'd need to compute the sum separately and provide it as a literal:</p> <pre><code>from pyspark.sql.functions import col, sum, lit\n\nbar_sum = (\n    dfs\n    .where(col(\"foo\") == \"d\")\n    .groupBy()\n    .agg(sum(col(\"bar\")))\n    .take(1)[0][0]\n)\n\n(\n    dfs\n    .orderBy(\"foo\")\n    .limit(2)\n    .withColumn(\"bar\", lit(bar_sum))\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  9|\n|  b|  9|\n+---+---+\n</code></pre>"},{"location":"user-guide/migration/spark/#example-2-combining-two-heads","title":"Example 2: Combining Two <code>head</code>s","text":"<p>In Polars you can combine two different <code>head</code> expressions on the same DataFrame, provided that they return the same number of values.</p> <pre><code>df.select(\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").sort(descending=True).head(2),\n)\n</code></pre> <p>Output:</p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 5   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Again, the two <code>head</code> expressions here are completely independent, and the pairing of <code>a</code> to <code>5</code> and <code>b</code> to <code>4</code> results purely from the juxtaposition of the two columns output by the expressions.</p> <p>To accomplish something similar in <code>Spark</code>, you would need to generate an artificial key that enables you to join the values in this way.</p> <pre><code>from pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\nfoo_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(\"foo\"))\n    )\n)\n\nbar_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(col(\"bar\").desc()))\n    )\n)\n\n(\n    foo_dfs.alias(\"foo\")\n    .join(bar_dfs.alias(\"bar\"), on=\"rownum\")\n    .select(\"foo.foo\", \"bar.bar\")\n    .limit(2)\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  5|\n|  b|  4|\n+---+---+\n</code></pre>"},{"location":"user-guide/migration/spark/#example-3-composing-expressions","title":"Example 3: Composing expressions","text":"<p>Polars allows you compose expressions quite liberally. For example, if you want to find the rolling mean of a lagged variable, you can compose <code>shift</code> and <code>rolling_mean</code> and evaluate them in a single <code>over</code> expression:</p> <pre><code>df.with_columns(\n    feature=pl.col('price').shift(7).rolling_mean(7).over('store', order_by='date')\n)\n</code></pre> <p>In PySpark however this is not allowed. They allow composing expressions such as <code>F.mean(F.abs(\"price\")).over(window)</code> because <code>F.abs</code> is an elementwise function, but not <code>F.mean(F.lag(\"price\", 1)).over(window)</code> because <code>F.lag</code> is a window function. To produce the same result, both <code>F.lag</code> and <code>F.mean</code> need their own window.</p> <pre><code>from pyspark.sql import Window\nfrom pyspark.sql import functions as F\n\nwindow = Window().partitionBy(\"store\").orderBy(\"date\")\nrolling_window = window.rowsBetween(-6, 0)\n(\n    df.withColumn(\"lagged_price\", F.lag(\"price\", 7).over(window)).withColumn(\n        \"feature\",\n        F.when(\n            F.count(\"lagged_price\").over(rolling_window) &gt;= 7,\n            F.mean(\"lagged_price\").over(rolling_window),\n        ),\n    )\n)\n</code></pre>"},{"location":"user-guide/misc/arrow/","title":"Arrow producer/consumer","text":""},{"location":"user-guide/misc/arrow/#using-pyarrow","title":"Using pyarrow","text":"<p>Polars can move data in and out of arrow zero copy. This can be done either via pyarrow or natively. Let's first start by showing the pyarrow solution:</p>  Python <pre><code>import polars as pl\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [\"ham\", \"spam\", \"jam\"]})\n\narrow_table = df.to_arrow()\nprint(arrow_table)\n</code></pre> <pre><code>pyarrow.Table\nfoo: int64\nbar: large_string\n----\nfoo: [[1,2,3]]\nbar: [[\"ham\",\"spam\",\"jam\"]]\n</code></pre> <p>Or if you want to ensure the output is zero-copy:</p>  Python <pre><code>arrow_table_zero_copy = df.to_arrow(compat_level=pl.CompatLevel.newest())\nprint(arrow_table_zero_copy)\n</code></pre> <pre><code>pyarrow.Table\nfoo: int64\nbar: string_view\n----\nfoo: [[1,2,3]]\nbar: [[\"ham\",\"spam\",\"jam\"]]\n</code></pre> <p>Importing from pyarrow can be achieved with <code>pl.from_arrow</code>.</p>"},{"location":"user-guide/misc/arrow/#using-the-arrow-pycapsule-interface","title":"Using the Arrow PyCapsule Interface","text":"<p>As of Polars v1.3 and higher, Polars implements the Arrow PyCapsule Interface, a protocol for sharing Arrow data across Python libraries.</p>"},{"location":"user-guide/misc/arrow/#exporting-data-from-polars-to-pyarrow","title":"Exporting data from Polars to pyarrow","text":"<p>To convert a Polars <code>DataFrame</code> to a <code>pyarrow.Table</code>, use the <code>pyarrow.table</code> constructor:</p> <p>Note</p> <p>This requires pyarrow v15 or higher.</p>  Python <pre><code>import polars as pl\nimport pyarrow as pa\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [\"ham\", \"spam\", \"jam\"]})\narrow_table = pa.table(df)\nprint(arrow_table)\n</code></pre> <pre><code>pyarrow.Table\nfoo: int64\nbar: string_view\n----\nfoo: [[1,2,3]]\nbar: [[\"ham\",\"spam\",\"jam\"]]\n</code></pre> <p>To convert a Polars <code>Series</code> to a <code>pyarrow.ChunkedArray</code>, use the <code>pyarrow.chunked_array</code> constructor.</p>  Python <pre><code>arrow_chunked_array = pa.chunked_array(df[\"foo\"])\nprint(arrow_chunked_array)\n</code></pre> <pre><code>[\n  [\n    1,\n    2,\n    3\n  ]\n]\n</code></pre> <p>You can also pass a <code>Series</code> to the <code>pyarrow.array</code> constructor to create a contiguous array. Note that this will not be zero-copy if the underlying <code>Series</code> had multiple chunks.</p>  Python <pre><code>arrow_array = pa.array(df[\"foo\"])\nprint(arrow_array)\n</code></pre> <pre><code>[\n  1,\n  2,\n  3\n]\n</code></pre>"},{"location":"user-guide/misc/arrow/#importing-data-from-pyarrow-to-polars","title":"Importing data from pyarrow to Polars","text":"<p>We can pass the pyarrow <code>Table</code> back to Polars by using the <code>polars.DataFrame</code> constructor:</p>  Python <pre><code>polars_df = pl.DataFrame(arrow_table)\nprint(polars_df)\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar  \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 ham  \u2502\n\u2502 2   \u2506 spam \u2502\n\u2502 3   \u2506 jam  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Similarly, we can pass the pyarrow <code>ChunkedArray</code> or <code>Array</code> back to Polars by using the <code>polars.Series</code> constructor:</p>  Python <pre><code>polars_series = pl.Series(arrow_chunked_array)\nprint(polars_series)\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [i64]\n[\n    1\n    2\n    3\n]\n</code></pre>"},{"location":"user-guide/misc/arrow/#usage-with-other-arrow-libraries","title":"Usage with other arrow libraries","text":"<p>There's a growing list of libraries that support the PyCapsule Interface directly. Polars <code>Series</code> and <code>DataFrame</code> objects work automatically with every such library.</p>"},{"location":"user-guide/misc/arrow/#for-library-maintainers","title":"For library maintainers","text":"<p>If you're developing a library that you wish to integrate with Polars, it's suggested to implement the Arrow PyCapsule Interface yourself. This comes with a number of benefits:</p> <ul> <li>Zero-copy exchange for both Polars Series and DataFrame</li> <li>No required dependency on pyarrow.</li> <li>No direct dependency on Polars.</li> <li>Harder to cause memory leaks than handling pointers as raw integers.</li> <li>Automatic zero-copy integration other PyCapsule Interface-supported libraries.</li> </ul>"},{"location":"user-guide/misc/arrow/#using-polars-directly","title":"Using Polars directly","text":"<p>Polars can also consume and export to and import from the Arrow C Data Interface directly. This is recommended for libraries that don't support the Arrow PyCapsule Interface and want to interop with Polars without requiring a pyarrow installation.</p> <ul> <li>To export <code>ArrowArray</code> C structs, Polars exposes: <code>Series._export_arrow_to_c</code>.</li> <li>To import an <code>ArrowArray</code> C struct, Polars exposes <code>Series._import_arrow_from_c</code>.</li> </ul>"},{"location":"user-guide/misc/comparison/","title":"Comparison with other tools","text":"<p>These are several libraries and tools that share similar functionalities with Polars. This often leads to questions from data experts about what the differences are. Below is a short comparison between some of the more popular data processing tools and Polars, to help data experts make a deliberate decision on which tool to use.</p> <p>You can find performance benchmarks (h2oai benchmark) of these tools here: Polars blog post or a more recent benchmark done by DuckDB</p>"},{"location":"user-guide/misc/comparison/#pandas","title":"Pandas","text":"<p>Pandas stands as a widely-adopted and comprehensive tool in Python data analysis, renowned for its rich feature set and strong community support. However, due to its single threaded nature, it can struggle with performance and memory usage on medium and large datasets.</p> <p>In contrast, Polars is optimised for high-performance multithreaded computing on single nodes, providing significant improvements in speed and memory efficiency, particularly for medium to large data operations. Its more composable and stricter API results in greater expressiveness and fewer schema-related bugs.</p>"},{"location":"user-guide/misc/comparison/#dask","title":"Dask","text":"<p>Dask extends Pandas' capabilities to large, distributed datasets. Dask mimics Pandas' API, offering a familiar environment for Pandas users, but with the added benefit of parallel and distributed computing.</p> <p>While Dask excels at scaling Pandas workflows across clusters, it only supports a subset of the Pandas API and therefore cannot be used for all use cases. Polars offers a more versatile API that delivers strong performance within the constraints of a single node.</p> <p>The choice between Dask and Polars often comes down to familiarity with the Pandas API and the need for distributed processing for extremely large datasets versus the need for efficiency and speed in a vertically scaled environment for a wide range of use cases.</p>"},{"location":"user-guide/misc/comparison/#modin","title":"Modin","text":"<p>Similar to Dask. In 2023, Snowflake acquired Ponder, the organisation that maintains Modin.</p>"},{"location":"user-guide/misc/comparison/#spark","title":"Spark","text":"<p>Spark (specifically PySpark) represents a different approach to large-scale data processing. While Polars has an optimised performance for single-node environments, Spark is designed for distributed data processing across clusters, making it suitable for extremely large datasets.</p> <p>However, Spark's distributed nature can introduce complexity and overhead, especially for small datasets and tasks that can run on a single machine. Another consideration is collaboration between data scientists and engineers. As they typically work with different tools (Pandas and Pyspark), refactoring is often required by engineers to deploy data scientists' data processing pipelines. Polars offers a single syntax that, due to vertical scaling, works in local environments and on a single machine in the cloud.</p> <p>The choice between Polars and Spark often depends on the scale of data and the specific requirements of the processing task. If you need to process TBs of data, Spark is a better choice.</p>"},{"location":"user-guide/misc/comparison/#duckdb","title":"DuckDB","text":"<p>Polars and DuckDB have many similarities. However, DuckDB is focused on providing an in-process SQL OLAP database management system, while Polars is focused on providing a scalable <code>DataFrame</code> interface to many languages. The different front-ends lead to different optimisation strategies and different algorithm prioritisation. The interoperability between both is zero-copy. DuckDB offers a guide on how to integrate with Polars.</p>"},{"location":"user-guide/misc/multiprocessing/","title":"Multiprocessing","text":"<p>TLDR: if you find that using Python's built-in <code>multiprocessing</code> module together with Polars results in a Polars error about multiprocessing methods, you should make sure you are using <code>spawn</code>, not <code>fork</code>, as the starting method:</p>  Python <pre><code>from multiprocessing import get_context\n\n\ndef my_fun(s):\n    print(s)\n\n\nwith get_context(\"spawn\").Pool() as pool:\n    pool.map(my_fun, [\"input1\", \"input2\", ...])\n</code></pre>"},{"location":"user-guide/misc/multiprocessing/#when-not-to-use-multiprocessing","title":"When not to use multiprocessing","text":"<p>Before we dive into the details, it is important to emphasize that Polars has been built from the start to use all your CPU cores. It does this by executing computations which can be done in parallel in separate threads. For example, requesting two expressions in a <code>select</code> statement can be done in parallel, with the results only being combined at the end. Another example is aggregating a value within groups using <code>group_by().agg(&lt;expr&gt;)</code>, each group can be evaluated separately. It is very unlikely that the <code>multiprocessing</code> module can improve your code performance in these cases. If you're using the GPU Engine with Polars you should also avoid manual multiprocessing. When used simultaneously, they can compete for system memory and processing power, leading to reduced performance.</p> <p>See the optimizations section for more optimizations.</p>"},{"location":"user-guide/misc/multiprocessing/#when-to-use-multiprocessing","title":"When to use multiprocessing","text":"<p>Although Polars is multithreaded, other libraries may be single-threaded. When the other library is the bottleneck, and the problem at hand is parallelizable, it makes sense to use multiprocessing to gain a speed up.</p>"},{"location":"user-guide/misc/multiprocessing/#the-problem-with-the-default-multiprocessing-config","title":"The problem with the default multiprocessing config","text":""},{"location":"user-guide/misc/multiprocessing/#summary","title":"Summary","text":"<p>The Python multiprocessing documentation lists the three methods to create a process pool:</p> <ol> <li>spawn</li> <li>fork</li> <li>forkserver</li> </ol> <p>The description of fork is (as of 2022-10-15):</p> <p>The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.</p> <p>Available on Unix only. The default on Unix.</p> <p>The short summary is: Polars is multithreaded as to provide strong performance out-of-the-box. Thus, it cannot be combined with <code>fork</code>. If you are on Unix (Linux, BSD, etc), you are using <code>fork</code>, unless you explicitly override it.</p> <p>The reason you may not have encountered this before is that pure Python code, and most Python libraries, are (mostly) single threaded. Alternatively, you are on Windows or MacOS, on which <code>fork</code> is not even available as a method (for MacOS it was up to Python 3.7).</p> <p>Thus one should use <code>spawn</code>, or <code>forkserver</code>, instead. <code>spawn</code> is available on all platforms and the safest choice, and hence the recommended method.</p>"},{"location":"user-guide/misc/multiprocessing/#example","title":"Example","text":"<p>The problem with <code>fork</code> is in the copying of the parent's process. Consider the example below, which is a slightly modified example posted on the Polars issue tracker:</p>  Python <pre><code>import multiprocessing\nimport polars as pl\n\n\ndef test_sub_process(df: pl.DataFrame, job_id):\n    df_filtered = df.filter(pl.col(\"a\") &gt; 0)\n    print(f\"Filtered (job_id: {job_id})\", df_filtered, sep=\"\\n\")\n\n\ndef create_dataset():\n    return pl.DataFrame({\"a\": [0, 2, 3, 4, 5], \"b\": [0, 4, 5, 56, 4]})\n\n\ndef setup():\n    # some setup work\n    df = create_dataset()\n    df.write_parquet(\"/tmp/test.parquet\")\n\n\ndef main():\n    test_df = pl.read_parquet(\"/tmp/test.parquet\")\n\n    for i in range(0, 5):\n        proc = multiprocessing.get_context(\"spawn\").Process(\n            target=test_sub_process, args=(test_df, i)\n        )\n        proc.start()\n        proc.join()\n\n        print(f\"Executed sub process {i}\")\n\n\nif __name__ == \"__main__\":\n    setup()\n    main()\n</code></pre> <p>Using <code>fork</code> as the method, instead of <code>spawn</code>, will cause a dead lock.</p> <p>The fork method is equivalent to calling <code>os.fork()</code>, which is a system call as defined in the POSIX standard:</p> <p>A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called.</p> <p>In contrast, <code>spawn</code> will create a completely new fresh Python interpreter, and not inherit the state of mutexes.</p> <p>So what happens in the code example? For reading the file with <code>pl.read_parquet</code> the file has to be locked. Then <code>os.fork()</code> is called, copying the state of the parent process, including mutexes. Thus all child processes will copy the file lock in an acquired state, leaving them hanging indefinitely waiting for the file lock to be released, which never happens.</p> <p>What makes debugging these issues tricky is that <code>fork</code> can work. Change the example to not having the call to <code>pl.read_parquet</code>:</p>  Python <pre><code>import multiprocessing\nimport polars as pl\n\n\ndef test_sub_process(df: pl.DataFrame, job_id):\n    df_filtered = df.filter(pl.col(\"a\") &gt; 0)\n    print(f\"Filtered (job_id: {job_id})\", df_filtered, sep=\"\\n\")\n\n\ndef create_dataset():\n    return pl.DataFrame({\"a\": [0, 2, 3, 4, 5], \"b\": [0, 4, 5, 56, 4]})\n\n\ndef main():\n    test_df = create_dataset()\n\n    for i in range(0, 5):\n        proc = multiprocessing.get_context(\"fork\").Process(\n            target=test_sub_process, args=(test_df, i)\n        )\n        proc.start()\n        proc.join()\n\n        print(f\"Executed sub process {i}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This works fine. Therefore debugging these issues in larger code bases, i.e. not the small toy examples here, can be a real pain, as a seemingly unrelated change can break your multiprocessing code. In general, one should therefore never use the <code>fork</code> start method with multithreaded libraries unless there are very specific requirements that cannot be met otherwise.</p>"},{"location":"user-guide/misc/multiprocessing/#pros-and-cons-of-fork","title":"Pro's and cons of fork","text":"<p>Based on the example, you may think, why is <code>fork</code> available in Python to start with?</p> <p>First, probably because of historical reasons: <code>spawn</code> was added to Python in version 3.4, whilst <code>fork</code> has been part of Python from the 2.x series.</p> <p>Second, there are several limitations for <code>spawn</code> and <code>forkserver</code> that do not apply to <code>fork</code>, in particular all arguments should be pickleable. See the Python multiprocessing docs for more information.</p> <p>Third, because it is faster to create new processes compared to <code>spawn</code>, as <code>spawn</code> is effectively <code>fork</code> + creating a brand new Python process without the locks by calling execv. Hence the warning in the Python docs that it is slower: there is more overhead to <code>spawn</code>. However, in almost all cases, one would like to use multiple processes to speed up computations that take multiple minutes or even hours, meaning the overhead is negligible in the grand scheme of things. And more importantly, it actually works in combination with multithreaded libraries.</p> <p>Fourth, <code>spawn</code> starts a new process, and therefore it requires code to be importable, in contrast to <code>fork</code>. In particular, this means that when using <code>spawn</code> the relevant code should not be in the global scope, such as in Jupyter notebooks or in plain scripts. Hence in the examples above, we define functions where we spawn within, and run those functions from a <code>__main__</code> clause. This is not an issue for typical projects, but during quick experimentation in notebooks it could fail.</p>"},{"location":"user-guide/misc/multiprocessing/#references","title":"References","text":"<ol> <li> <p>https://docs.python.org/3/library/multiprocessing.html</p> </li> <li> <p>https://pythonspeed.com/articles/python-multiprocessing/</p> </li> <li> <p>https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html</p> </li> <li> <p>https://bnikolic.co.uk/blog/python/parallelism/2019/11/13/python-forkserver-preload.html</p> </li> </ol>"},{"location":"user-guide/misc/polars_llms/","title":"Generating Polars code with LLMs","text":"<p>Large Language Models (LLMs) can sometimes return pandas code or invalid Polars code in their output. This guide presents approaches that help LLMs generate valid Polars code more consistently.</p> <p>These approaches have been developed by the Polars community through testing model responses to various inputs. If you find additional effective approaches for generating Polars code from LLMs, please raise a pull request.</p>"},{"location":"user-guide/misc/polars_llms/#polars-mcp-server","title":"Polars MCP server","text":"<p>The new remote Model Context Protocol (MCP) server for Polars provides access to the official Polars and Polars Cloud documentation. The server enables LLMs to query the user guide and API references directly, making it easier to get more accurate answers about DataFrame operations, expressions, lazy evaluation, and cloud deployments.</p> <pre><code>{\n  \"mcpServers\": {\n    \"ask_polars\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.pola.rs/mcp\"]\n    }\n  }\n}\n</code></pre> <p>If you run into an issue or are missing a feature, please open an issue on the public issue tracker. We plan to expand the capabilities over time.</p> <p>MCP server installation</p> <p>Please refer to the documentation of your preferred client to connect to the MCP server.</p>"},{"location":"user-guide/misc/polars_llms/#system-prompt","title":"System prompt","text":"<p>Many LLMs allow you to provide a system prompt that is included with every individual prompt you send to the model. In the system prompt, you can specify your preferred defaults, such as \"Use Polars as the default dataframe library\". Including such a system prompt typically leads to models consistently generating Polars code rather than Pandas code.</p> <p>You can set this system prompt in the settings menu of both web-based LLMs like ChatGPT and IDE-based LLMs like Cursor. Refer to each application's documentation for specific instructions.</p>"},{"location":"user-guide/misc/polars_llms/#enable-web-search","title":"Enable web search","text":"<p>Some LLMs can search the web to access information beyond their pre-training data. Enabling web search allows an LLM to reference up-to-date Polars documentation for the current API.</p> <p>Some IDE-based LLMs can index the Polars API documentation and reference this when generating code. For example, in Cursor you can add Polars as a custom docs source and instruct the agent to reference the Polars documentation in a prompt.</p> <p>However, web search does not yet guarantee that valid code will be produced. If a model is confident in a result based on its pre-training data, it may not incorporate web search results in its output.</p> <p>The Polars API pages also have AI-enabled search to help you find the information you need more easily.</p>"},{"location":"user-guide/misc/polars_llms/#provide-examples","title":"Provide examples","text":"<p>You can guide LLMs to use correct syntax by including relevant examples in your prompt.</p> <p>For instance, this basic query:</p> <pre><code>df = pl.DataFrame({\n    \"id\": [\"a\", \"b\", \"a\", \"b\", \"c\"],\n    \"score\": [1, 2, 1, 3, 3],\n    \"year\": [2020, 2020, 2021, 2021, 2021],\n})\n# Compute average of score by id\n</code></pre> <p>Often results in outdated <code>groupby</code> syntax instead of the correct <code>group_by</code>.</p> <p>However, including a simple example from the Polars <code>group_by</code> documentation (preferably with web search enabled) like this:</p> <pre><code>df = pl.DataFrame({\n    \"id\": [\"a\", \"b\", \"a\", \"b\", \"c\"],\n    \"score\": [1, 2, 1, 3, 3],\n    \"year\": [2020, 2020, 2021, 2021, 2021],\n})\n# Compute average of score by id\n# Examples of Polars code:\n\n# df.group_by(\"a\").agg(pl.col(\"b\").mean())\n</code></pre> <p>Produces valid outputs more often. This approach has been validated across several leading models.</p> <p>The combination of web search and examples is more effective than either independently. Model outputs indicate that when an example contradicts the model's pre-trained expectations, it seems more likely to trigger a web search for verification.</p> <p>Additionally, explicit instructions like \"use <code>group_by</code> instead of <code>groupby</code>\" can be effective in guiding the model to use correct syntax.</p> <p>Common examples such as <code>df.group_by(\"a\").agg(pl.col(\"b\").mean())</code> can also be added the system prompt for more consistency.</p>"},{"location":"user-guide/misc/styling/","title":"Styling","text":"<p>Data in a Polars <code>DataFrame</code> can be styled for presentation use the <code>DataFrame.style</code> property. This returns a <code>GT</code> object from Great Tables, which enables structuring, formatting, and styling for table display.</p>  Python <pre><code>import polars as pl\nimport polars.selectors as cs\n\npath = \"docs/assets/data/iris.csv\"\n\ndf = (\n    pl.scan_csv(path)\n    .group_by(\"species\")\n    .agg(cs.starts_with(\"petal\").mean().round(3))\n    .collect()\n)\nprint(df)\n</code></pre> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species    \u2506 petal_length \u2506 petal_width \u2502\n\u2502 ---        \u2506 ---          \u2506 ---         \u2502\n\u2502 str        \u2506 f64          \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Versicolor \u2506 4.26         \u2506 1.326       \u2502\n\u2502 Setosa     \u2506 1.462        \u2506 0.246       \u2502\n\u2502 Virginica  \u2506 5.552        \u2506 2.026       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/misc/styling/#structure-add-header-title","title":"Structure: add header title","text":"Python <pre><code>df.style.tab_header(title=\"Iris Data\", subtitle=\"Mean measurement values per species\")\n</code></pre> Iris Data Mean measurement values per species species petal_length petal_width Versicolor 4.26 1.326 Setosa 1.462 0.246 Virginica 5.552 2.026"},{"location":"user-guide/misc/styling/#structure-add-row-stub","title":"Structure: add row stub","text":"Python <pre><code>df.style.tab_stub(rowname_col=\"species\")\n</code></pre> petal_length petal_width Versicolor 4.26 1.326 Setosa 1.462 0.246 Virginica 5.552 2.026"},{"location":"user-guide/misc/styling/#structure-add-column-spanner","title":"Structure: add column spanner","text":"Python <pre><code>(\n    df.style.tab_spanner(\"Petal\", cs.starts_with(\"petal\")).cols_label(\n        petal_length=\"Length\", petal_width=\"Width\"\n    )\n)\n</code></pre> species Petal Length Width Versicolor 4.26 1.326 Setosa 1.462 0.246 Virginica 5.552 2.026"},{"location":"user-guide/misc/styling/#format-limit-decimal-places","title":"Format: limit decimal places","text":"Python <pre><code>df.style.fmt_number(\"petal_width\", decimals=1)\n</code></pre> species petal_length petal_width Versicolor 4.26 1.3 Setosa 1.462 0.2 Virginica 5.552 2.0"},{"location":"user-guide/misc/styling/#style-highlight-max-row","title":"Style: highlight max row","text":"Python <pre><code>from great_tables import loc, style\n\ndf.style.tab_style(\n    style.fill(\"yellow\"),\n    loc.body(\n        rows=pl.col(\"petal_length\") == pl.col(\"petal_length\").max(),\n    ),\n)\n</code></pre> species petal_length petal_width Versicolor 4.26 1.326 Setosa 1.462 0.246 Virginica 5.552 2.026"},{"location":"user-guide/misc/styling/#style-bold-species-column","title":"Style: bold species column","text":"Python <pre><code>from great_tables import loc, style\n\ndf.style.tab_style(\n    style.text(weight=\"bold\"),\n    loc.body(columns=\"species\"),\n)\n</code></pre> species petal_length petal_width Versicolor 4.26 1.326 Setosa 1.462 0.246 Virginica 5.552 2.026"},{"location":"user-guide/misc/styling/#full-example","title":"Full example","text":"Python <pre><code>from great_tables import loc, style\n\n(\n    df.style.tab_header(\n        title=\"Iris Data\", subtitle=\"Mean measurement values per species\"\n    )\n    .tab_stub(rowname_col=\"species\")\n    .cols_label(petal_length=\"Length\", petal_width=\"Width\")\n    .tab_spanner(\"Petal\", cs.starts_with(\"petal\"))\n    .fmt_number(\"petal_width\", decimals=2)\n    .tab_style(\n        style.fill(\"yellow\"),\n        loc.body(\n            rows=pl.col(\"petal_length\") == pl.col(\"petal_length\").max(),\n        ),\n    )\n)\n</code></pre> Iris Data Mean measurement values per species Petal Length Width Versicolor 4.26 1.33 Setosa 1.462 0.25 Virginica 5.552 2.03"},{"location":"user-guide/misc/visualization/","title":"Visualization","text":"<p>Data in a Polars <code>DataFrame</code> can be visualized using common visualization libraries.</p> <p>We illustrate plotting capabilities using the Iris dataset. We read a CSV and then plot one column against another, colored by a yet another column.</p>  Python <pre><code>import polars as pl\n\npath = \"docs/assets/data/iris.csv\"\n\ndf = pl.read_csv(path)\nprint(df)\n</code></pre> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 5.0          \u2506 3.6         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.7          \u2506 3.0         \u2506 5.2          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 Virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 Virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 Virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/misc/visualization/#built-in-plotting-with-altair","title":"Built-in plotting with Altair","text":"<p>Polars has a <code>plot</code> method to create plots using Altair:</p>  Python <pre><code>chart =  (\n    df.plot.point(\n        x=\"sepal_width\",\n        y=\"sepal_length\",\n        color=\"species\",\n    )\n    .properties(width=500, title=\"Irises\")\n    .configure_scale(zero=False)\n    .configure_axisX(tickMinStep=1)\n)\nchart.encoding.x.title = \"Sepal Width\"\nchart.encoding.y.title = \"Sepal Length\"\nchart\n</code></pre> <p>This is shorthand for:</p> <pre><code>import altair as alt\n\n(\n    alt.Chart(df).mark_point(tooltip=True).encode(\n        x=\"sepal_length\",\n        y=\"sepal_width\",\n        color=\"species\",\n    )\n    .properties(width=500)\n    .configure_scale(zero=False)\n)\n</code></pre> <p>and is only provided for convenience, and to signal that Altair is known to work well with Polars.</p> <p>For configuration, we suggest reading Chart Configuration. For example, you can:</p> <ul> <li>Change the width/height/title with <code>.properties(width=500, height=350, title=\"My amazing plot\")</code>.</li> <li>Change the x-axis label rotation with <code>.configure_axisX(labelAngle=30)</code>.</li> <li>Change the opacity of the points in your scatter plot with <code>.configure_point(opacity=.5)</code>.</li> </ul>"},{"location":"user-guide/misc/visualization/#hvplot","title":"hvPlot","text":"<p>If you import <code>hvplot.polars</code>, then it registers a <code>hvplot</code> method which you can use to create interactive plots using hvPlot.</p>  Python <pre><code>import hvplot.polars\ndf.hvplot.scatter(\n    x=\"sepal_width\",\n    y=\"sepal_length\",\n    by=\"species\",\n    width=650,\n    title=\"Irises\",\n    xlabel='Sepal Width',\n    ylabel='Sepal Length',\n)\n</code></pre> hvplot_scatter"},{"location":"user-guide/misc/visualization/#matplotlib","title":"Matplotlib","text":"<p>To create a scatter plot we can pass columns of a <code>DataFrame</code> directly to Matplotlib as a <code>Series</code> for each column. Matplotlib does not have explicit support for Polars objects but can accept a Polars <code>Series</code> by converting it to a NumPy array (which is zero-copy for numeric data without null values).</p> <p>Note that because the column <code>'species'</code> isn't numeric, we need to first convert it to numeric values so that it can be passed as an argument to <code>c</code>.</p>  Python <pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter(\n    x=df[\"sepal_width\"],\n    y=df[\"sepal_length\"],\n    c=df[\"species\"].cast(pl.Categorical).to_physical(),\n)\nax.set_title('Irises')\nax.set_xlabel('Sepal Width')\nax.set_ylabel('Sepal Length')\n</code></pre> <p></p>"},{"location":"user-guide/misc/visualization/#plotnine","title":"Plotnine","text":"<p>Plotnine is a reimplementation of ggplot2 in Python, bringing the Grammar of Graphics to Python users with an interface similar to its R counterpart. It supports Polars <code>DataFrame</code> by internally converting it to a pandas <code>DataFrame</code>.</p>  Python <pre><code>from plotnine import ggplot, aes, geom_point, labs\n\n(\n    ggplot(df, mapping=aes(x=\"sepal_width\", y=\"sepal_length\", color=\"species\"))\n    + geom_point()\n    + labs(title=\"Irises\", x=\"Sepal Width\", y=\"Sepal Length\")\n)\n</code></pre> <p></p>"},{"location":"user-guide/misc/visualization/#seaborn","title":"Seaborn","text":"<p>Seaborn can accept a Polars <code>DataFrame</code> by leveraging the dataframe interchange protocol, which offers zero-copy conversion where possible. Note that the protocol does not support all Polars data types (e.g. <code>List</code>) so your mileage may vary here.</p>  Python <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nsns.scatterplot(\n    df,\n    x=\"sepal_width\",\n    y=\"sepal_length\",\n    hue=\"species\",\n    ax=ax,\n)\nax.set_title('Irises')\nax.set_xlabel('Sepal Width')\nax.set_ylabel('Sepal Length')\n</code></pre> <p></p>"},{"location":"user-guide/misc/visualization/#plotly","title":"Plotly","text":"<p>Plotly can accept a Polars <code>DataFrame</code> by leveraging:</p> <ul> <li>Narwhals, since plotly v6.0.0, and therefore running   execution natively without any conversion overhead.</li> <li>The dataframe interchange protocol, before plotly v6.0.0,   which offers zero-copy conversion where possible. Note that the protocol does not support all   Polars data types (e.g. <code>List</code>) so your mileage may vary here.</li> </ul>  Python <pre><code>import plotly.express as px\n\npx.scatter(\n    df,\n    x=\"sepal_width\",\n    y=\"sepal_length\",\n    color=\"species\",\n    width=650,\n    title=\"Irises\",\n    labels={'sepal_width': 'Sepal Width', 'sepal_length': 'Sepal Length'}\n)\n</code></pre>"},{"location":"user-guide/plugins/","title":"Plugins","text":"<p>Polars allows you to extend its functionality with either Expression plugins or IO plugins.</p> <ul> <li>Expression plugins</li> <li>IO plugins</li> </ul>"},{"location":"user-guide/plugins/#community-plugins","title":"Community plugins","text":"<p>Here is a curated (non-exhaustive) list of community-implemented plugins.</p>"},{"location":"user-guide/plugins/#various","title":"Various","text":"<ul> <li>polars-xdt Polars plugin with extra datetime-related   functionality which isn't quite in-scope for the main library</li> <li>polars-hash Stable non-cryptographic and   cryptographic hashing functions for Polars</li> </ul>"},{"location":"user-guide/plugins/#data-science","title":"Data science","text":"<ul> <li>polars-distance Polars plugin for pairwise   distance functions</li> <li>polars-ds Polars extension aiming to   simplify common numerical/string data analysis procedures</li> </ul>"},{"location":"user-guide/plugins/#geo","title":"Geo","text":"<ul> <li>polars-st Polars ST provides spatial operations on Polars   DataFrames, Series and Expressions. Just like Shapely and Geopandas.</li> <li>polars-reverse-geocode Offline reverse   geocoder for finding the closest city to a given (latitude, longitude) pair.</li> <li>polars-h3 This is a Polars extension that adds support for   the H3 discrete global grid system, so you can index points and geometries to hexagons directly in   Polars.</li> </ul>"},{"location":"user-guide/plugins/#other-material","title":"Other material","text":"<ul> <li>Ritchie Vink - Keynote on Polars Plugins</li> <li>Polars plugins tutorial Learn how to   write a plugin by going through some very simple and minimal examples</li> <li>cookiecutter-polars-plugin Project   template for Polars Plugins</li> </ul>"},{"location":"user-guide/plugins/expr_plugins/","title":"Expression Plugins","text":"<p>Expression plugins are the preferred way to create user defined functions. They allow you to compile a Rust function and register that as an expression into the Polars library. The Polars engine will dynamically link your function at runtime and your expression will run almost as fast as native expressions. Note that this works without any interference of Python and thus no GIL contention.</p> <p>They will benefit from the same benefits default expressions have:</p> <ul> <li>Optimization</li> <li>Parallelism</li> <li>Rust native performance</li> </ul> <p>To get started we will see what is needed to create a custom expression.</p>"},{"location":"user-guide/plugins/expr_plugins/#our-first-custom-expression-pig-latin","title":"Our first custom expression: Pig Latin","text":"<p>For our first expression we are going to create a pig latin converter. Pig latin is a silly language where in every word the first letter is removed, added to the back and finally \"ay\" is added. So the word \"pig\" would convert to \"igpay\".</p> <p>We could of course already do that with expressions, e.g. <code>col(\"name\").str.slice(1) + col(\"name\").str.slice(0, 1) + \"ay\"</code>, but a specialized function for this would perform better and allows us to learn about the plugins.</p>"},{"location":"user-guide/plugins/expr_plugins/#setting-up","title":"Setting up","text":"<p>We start with a new library as the following <code>Cargo.toml</code> file</p> <pre><code>[package]\nname = \"expression_lib\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[lib]\nname = \"expression_lib\"\ncrate-type = [\"cdylib\"]\n\n[dependencies]\npolars = { version = \"*\" }\npyo3 = { version = \"*\", features = [\"extension-module\", \"abi3-py310\"] }\npyo3-polars = { version = \"*\", features = [\"derive\"] }\nserde = { version = \"*\", features = [\"derive\"] }\n</code></pre>"},{"location":"user-guide/plugins/expr_plugins/#writing-the-expression","title":"Writing the expression","text":"<p>In this library we create a helper function that converts a <code>&amp;str</code> to pig-latin, and we create the function that we will expose as an expression. To expose a function we must add the <code>#[polars_expr(output_type=DataType)]</code> attribute and the function must always accept <code>inputs: &amp;[Series]</code> as its first argument.</p> <pre><code>// src/expressions.rs\nuse polars::prelude::*;\nuse pyo3_polars::derive::polars_expr;\nuse std::fmt::Write;\n\nfn pig_latin_str(value: &amp;str, output: &amp;mut String) {\n    if let Some(first_char) = value.chars().next() {\n        write!(output, \"{}{}ay\", &amp;value[1..], first_char).unwrap()\n    }\n}\n\n#[polars_expr(output_type=String)]\nfn pig_latinnify(inputs: &amp;[Series]) -&gt; PolarsResult&lt;Series&gt; {\n    let ca = inputs[0].str()?;\n    let out: StringChunked = ca.apply_into_string_amortized(pig_latin_str);\n    Ok(out.into_series())\n}\n</code></pre> <p>Note that we use <code>apply_into_string_amortized</code>, as opposed to <code>apply_values</code>, to avoid allocating a new string for each row. If your plugin takes in multiple inputs, operates elementwise, and produces a <code>String</code> output, then you may want to look at the <code>binary_elementwise_into_string_amortized</code> utility function in <code>polars::prelude::arity</code>.</p> <p>This is all that is needed on the Rust side. On the Python side we must setup a folder with the same name as defined in the <code>Cargo.toml</code>, in this case \"expression_lib\". We will create a folder in the same directory as our Rust <code>src</code> folder named <code>expression_lib</code> and we create an <code>expression_lib/__init__.py</code>. The resulting file structure should look something like this:</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 expression_lib/  # name must match \"lib.name\" in Cargo.toml\n|   \u2514\u2500\u2500 __init__.py\n|\n\u251c\u2500\u2500 \ud83d\udcc1src/\n|   \u251c\u2500\u2500 lib.rs\n|   \u2514\u2500\u2500 expressions.rs\n|\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>Then we create new expressions. The function name of our expression can be registered. Note that it is important that this name is correct, otherwise the main Polars package cannot resolve the function name. Furthermore we can set additional keyword arguments that explain to Polars how this expression behaves. In this case we tell Polars that this function is elementwise. This allows Polars to run this expression in batches. Whereas for other operations this would not be allowed, think for instance of a sort, or a slice.</p> <pre><code># expression_lib/__init__.py\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nimport polars as pl\nfrom polars.plugins import register_plugin_function\nfrom polars._typing import IntoExpr\n\nPLUGIN_PATH = Path(__file__).parent\n\ndef pig_latinnify(expr: IntoExpr) -&gt; pl.Expr:\n    \"\"\"Pig-latinnify expression.\"\"\"\n    return register_plugin_function(\n        plugin_path=PLUGIN_PATH,\n        function_name=\"pig_latinnify\",\n        args=expr,\n        is_elementwise=True,\n    )\n</code></pre> <p>We can then compile this library in our environment by installing <code>maturin</code> and running <code>maturin develop --release</code>.</p> <p>And that's it. Our expression is ready to use!</p> <pre><code>import polars as pl\nfrom expression_lib import pig_latinnify\n\ndf = pl.DataFrame(\n    {\n        \"convert\": [\"pig\", \"latin\", \"is\", \"silly\"],\n    }\n)\nout = df.with_columns(pig_latin=pig_latinnify(\"convert\"))\n</code></pre> <p>Alternatively, you can register a custom namespace, which enables you to create a <code>Expr.language</code> namespace, allowing users to write:</p> <pre><code>out = df.with_columns(\n    pig_latin=pl.col(\"convert\").language.pig_latinnify(),\n)\n</code></pre>"},{"location":"user-guide/plugins/expr_plugins/#accepting-kwargs","title":"Accepting kwargs","text":"<p>If you want to accept <code>kwargs</code> (keyword arguments) in a polars expression, all you have to do is define a Rust <code>struct</code> and make sure that it derives <code>serde::Deserialize</code>.</p> <pre><code>/// Provide your own kwargs struct with the proper schema and accept that type\n/// in your plugin expression.\n#[derive(Deserialize)]\npub struct MyKwargs {\n    float_arg: f64,\n    integer_arg: i64,\n    string_arg: String,\n    boolean_arg: bool,\n}\n\n/// If you want to accept `kwargs`. You define a `kwargs` argument\n/// on the second position in you plugin. You can provide any custom struct that is deserializable\n/// with the pickle protocol (on the Rust side).\n#[polars_expr(output_type=String)]\nfn append_kwargs(input: &amp;[Series], kwargs: MyKwargs) -&gt; PolarsResult&lt;Series&gt; {\n    let input = &amp;input[0];\n    let input = input.cast(&amp;DataType::String)?;\n    let ca = input.str().unwrap();\n\n    Ok(ca\n        .apply_into_string_amortized(|val, buf| {\n            write!(\n                buf,\n                \"{}-{}-{}-{}-{}\",\n                val, kwargs.float_arg, kwargs.integer_arg, kwargs.string_arg, kwargs.boolean_arg\n            )\n                .unwrap()\n        })\n        .into_series())\n}\n</code></pre> <p>On the Python side the kwargs can be passed when we register the plugin.</p> <pre><code>def append_args(\n    expr: IntoExpr,\n    float_arg: float,\n    integer_arg: int,\n    string_arg: str,\n    boolean_arg: bool,\n) -&gt; pl.Expr:\n    \"\"\"\n    This example shows how arguments other than `Series` can be used.\n    \"\"\"\n    return register_plugin_function(\n        plugin_path=PLUGIN_PATH,\n        function_name=\"append_kwargs\",\n        args=expr,\n        kwargs={\n            \"float_arg\": float_arg,\n            \"integer_arg\": integer_arg,\n            \"string_arg\": string_arg,\n            \"boolean_arg\": boolean_arg,\n        },\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"user-guide/plugins/expr_plugins/#output-data-types","title":"Output data types","text":"<p>Output data types of course don't have to be fixed. They often depend on the input types of an expression. To accommodate this you can provide the <code>#[polars_expr()]</code> macro with an <code>output_type_func</code> argument that points to a function. This function can map input fields <code>&amp;[Field]</code> to an output <code>Field</code> (name and data type).</p> <p>In the snippet below is an example where we use the utility <code>FieldsMapper</code> to help with this mapping.</p> <pre><code>use polars_plan::dsl::FieldsMapper;\n\nfn haversine_output(input_fields: &amp;[Field]) -&gt; PolarsResult&lt;Field&gt; {\n    FieldsMapper::new(input_fields).map_to_float_dtype()\n}\n\n#[polars_expr(output_type_func=haversine_output)]\nfn haversine(inputs: &amp;[Series]) -&gt; PolarsResult&lt;Series&gt; {\n    let out = match inputs[0].dtype() {\n        DataType::Float32 =&gt; {\n            let start_lat = inputs[0].f32().unwrap();\n            let start_long = inputs[1].f32().unwrap();\n            let end_lat = inputs[2].f32().unwrap();\n            let end_long = inputs[3].f32().unwrap();\n            crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?\n                .into_series()\n        }\n        DataType::Float64 =&gt; {\n            let start_lat = inputs[0].f64().unwrap();\n            let start_long = inputs[1].f64().unwrap();\n            let end_lat = inputs[2].f64().unwrap();\n            let end_long = inputs[3].f64().unwrap();\n            crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?\n                .into_series()\n        }\n        _ =&gt; polars_bail!(InvalidOperation: \"only supported for float types\"),\n    };\n    Ok(out)\n}\n</code></pre> <p>That's all you need to know to get started. Take a look at this repo to see how this all fits together, and at this tutorial to gain a more thorough understanding.</p>"},{"location":"user-guide/plugins/io_plugins/","title":"IO Plugins","text":"<p>Besides expression plugins, we also support IO plugins. These allow you to register different file formats as sources to the Polars engines. Because sources can move data zero copy via Arrow FFI and sources can produce large chunks of data before returning, we've decided to interface to IO plugins via Python for now, as we don't think the short time the GIL is needed should lead to any contention.</p> <p>E.g. an IO source can read their dataframe's in rust and only at the rendez-vous move the data zero-copy having only a short time the GIL is needed.</p>"},{"location":"user-guide/plugins/io_plugins/#use-case","title":"Use case","text":"<p>You want IO plugins if you have a source file not supported by Polars and you want to benefit from optimizations like projection pushdown, predicate pushdown, early stopping and support of our streaming engine.</p>"},{"location":"user-guide/plugins/io_plugins/#example","title":"Example","text":"<p>So let's write a simple, very bad, custom CSV source and register that as an IO plugin. I want to stress that this is a very bad example and is only given for learning purposes.</p> <p>First we define some imports we need:</p> <pre><code># Use python for csv parsing.\nimport csv\nimport polars as pl\n# Used to register a new generator on every instantiation.\nfrom polars.io.plugins import register_io_source\nfrom typing import Iterator\nimport io\n</code></pre>"},{"location":"user-guide/plugins/io_plugins/#parsing-the-schema","title":"Parsing the schema","text":"<p>Every <code>scan</code> function in Polars has to be able to provide the schema of the data it reads. For this simple csv parser we will always read the data as <code>pl.String</code>. The only thing that differs are the field names and the number of fields.</p> <pre><code>def parse_schema(csv_str: str) -&gt; pl.Schema:\n    first_line = csv_str.split(\"\\n\")[0]\n\n    return pl.Schema({k: pl.String for k in first_line.split(\",\")})\n</code></pre> <p>If we run this with small csv file <code>\"a,b,c\\n1,2,3\"</code> we get the schema: <code>Schema([('a', String), ('b', String), ('c', String)])</code>.</p> <pre><code>&gt;&gt;&gt; print(parse_schema(\"a,b,c\\n1,2,3\"))\nSchema([('a', String), ('b', String), ('c', String)])\n</code></pre>"},{"location":"user-guide/plugins/io_plugins/#writing-the-source","title":"Writing the source","text":"<p>Next up is the actual source. For this we create an outer and an inner function. The outer function <code>my_scan_csv</code> is the user facing function. This function will accept the file name and other potential arguments you would need for reading the source. For csv files, these arguments could be \"delimiter\", \"quote_char\" and such.</p> <p>This outer function calls <code>register_io_source</code> which accepts a <code>callable</code> and a <code>schema</code>. The schema is the Polars schema of the complete source file (independent of projection pushdown).</p> <p>The callable is a function that will return a generator that produces <code>pl.DataFrame</code> objects.</p> <p>The arguments of this function are predefined and this function must accept:</p> <ul> <li><code>with_columns</code></li> </ul> <p>Columns that are projected. The reader must project these columns if applied</p> <ul> <li><code>predicate</code></li> </ul> <p>Polars expression. The reader must filter their rows accordingly.</p> <ul> <li><code>n_rows</code></li> </ul> <p>Materialize only n rows from the source. The reader can stop when <code>n_rows</code> are read.</p> <ul> <li><code>batch_size</code></li> </ul> <p>A hint of the ideal batch size the reader's generator must produce.</p> <p>The inner function is the actual implementation of the IO source and can also call into Rust/C++ or wherever the IO plugin is written. If you want to see an IO source implemented in Rust, take a look at our plugins repository.</p> <pre><code>def my_scan_csv(csv_str: str) -&gt; pl.LazyFrame:\n    schema = parse_schema(csv_str)\n\n    def source_generator(\n        with_columns: list[str] | None,\n        predicate: pl.Expr | None,\n        n_rows: int | None,\n        batch_size: int | None,\n    ) -&gt; Iterator[pl.DataFrame]:\n        \"\"\"\n        Generator function that creates the source.\n        This function will be registered as IO source.\n        \"\"\"\n        if batch_size is None:\n            batch_size = 100\n\n        # Initialize the reader.\n        reader = csv.reader(io.StringIO(csv_str), delimiter=',')\n        # Skip the header.\n        _ = next(reader)\n\n        # Ensure we don't read more rows than requested from the engine\n        while n_rows is None or n_rows &gt; 0:\n            if n_rows is not None:\n                batch_size = min(batch_size, n_rows)\n\n            rows = []\n\n            for _ in range(batch_size):\n                try:\n                    row = next(reader)\n                except StopIteration:\n                    n_rows = 0\n                    break\n                rows.append(row)\n\n            df = pl.from_records(rows, schema=schema, orient=\"row\")\n            n_rows -= df.height\n\n            # If we would make a performant reader, we would not read these\n            # columns at all.\n            if with_columns is not None:\n                df = df.select(with_columns)\n\n            # If the source supports predicate pushdown, the expression can be parsed\n            # to skip rows/groups.\n            if predicate is not None:\n                df = df.filter(predicate)\n\n            yield df\n\n    return register_io_source(io_source=source_generator, schema=schema)\n</code></pre>"},{"location":"user-guide/plugins/io_plugins/#taking-it-for-a-very-slow-spin","title":"Taking it for a (very slow) spin","text":"<p>Finally we can test our source:</p> <pre><code>csv_str1 = \"\"\"a,b,c,d\n1,2,3,4\n9,10,11,2\n1,2,3,4\n1,122,3,4\"\"\"\n\nprint(my_scan_csv(csv_str1).collect())\n\n\ncsv_str2 = \"\"\"a,b\n1,2\n9,10\n1,2\n1,122\"\"\"\n\nprint(my_scan_csv(csv_str2).head(2).collect())\n</code></pre> <p>Running the script above would print the following output to the console:</p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2506 d   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 str \u2506 str \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2506 3   \u2506 4   \u2502\n\u2502 9   \u2506 10  \u2506 11  \u2506 2   \u2502\n\u2502 1   \u2506 2   \u2506 3   \u2506 4   \u2502\n\u2502 1   \u2506 122 \u2506 3   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2   \u2502\n\u2502 9   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/plugins/io_plugins/#further-reading","title":"Further reading","text":"<ul> <li>Rust example (distribution source)</li> </ul>"},{"location":"user-guide/sql/create/","title":"CREATE","text":"<p>In Polars, the <code>SQLContext</code> provides a way to execute SQL statements against <code>LazyFrames</code> and <code>DataFrames</code> using SQL syntax. One of the SQL statements that can be executed using <code>SQLContext</code> is the <code>CREATE TABLE</code> statement, which is used to create a new table.</p> <p>The syntax for the <code>CREATE TABLE</code> statement in Polars is as follows:</p> <pre><code>CREATE TABLE table_name\nAS\nSELECT ...\n</code></pre> <p>In this syntax, <code>table_name</code> is the name of the new table that will be created, and <code>SELECT ...</code> is a SELECT statement that defines the data that will be inserted into the table.</p> <p>Here's an example of how to use the <code>CREATE TABLE</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>data = {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\ndf = pl.LazyFrame(data)\n\nctx = pl.SQLContext(my_table=df, eager=True)\n\nresult = ctx.execute(\n    \"\"\"\n    CREATE TABLE older_people\n    AS\n    SELECT * FROM my_table WHERE age &gt; 30\n\"\"\"\n)\n\nprint(ctx.execute(\"SELECT * FROM older_people\"))\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2502 David   \u2506 40  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a <code>CREATE TABLE</code> statement that creates a new table called <code>older_people</code> based on a SELECT statement that selects all rows from the <code>my_table</code> DataFrame where the <code>age</code> column is greater than 30.</p> <p>Note</p> <p>Note that the result of a <code>CREATE TABLE</code> statement is not the table itself. The table is registered in the <code>SQLContext</code>. In case you want to turn the table back to a <code>DataFrame</code> you can use a <code>SELECT * FROM ...</code> statement</p>"},{"location":"user-guide/sql/cte/","title":"Common Table Expressions","text":"<p>Common Table Expressions (CTEs) are a feature of SQL that allow you to define a temporary named result set that can be referenced within a SQL statement. CTEs provide a way to break down complex SQL queries into smaller, more manageable pieces, making them easier to read, write, and maintain.</p> <p>A CTE is defined using the <code>WITH</code> keyword followed by a comma-separated list of subqueries, each of which defines a named result set that can be used in subsequent queries. The syntax for a CTE is as follows:</p> <pre><code>WITH cte_name AS (\n    subquery\n)\nSELECT ...\n</code></pre> <p>In this syntax, <code>cte_name</code> is the name of the CTE, and <code>subquery</code> is the subquery that defines the result set. The CTE can then be referenced in subsequent queries as if it were a table or view.</p> <p>CTEs are particularly useful when working with complex queries that involve multiple levels of subqueries, as they allow you to break down the query into smaller, more manageable pieces that are easier to understand and debug. Additionally, CTEs can help improve query performance by allowing the database to optimize and cache the results of subqueries, reducing the number of times they need to be executed.</p> <p>Polars supports Common Table Expressions (CTEs) using the WITH clause in SQL syntax. Below is an example</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>ctx = pl.SQLContext()\ndf = pl.LazyFrame(\n    {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\n)\nctx.register(\"my_table\", df)\n\nresult = ctx.execute(\n    \"\"\"\n    WITH older_people AS (\n        SELECT * FROM my_table WHERE age &gt; 30\n    )\n    SELECT * FROM older_people WHERE STARTS_WITH(name,'C')\n\"\"\",\n    eager=True,\n)\n\nprint(result)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a SQL query that includes a CTE. The CTE selects all rows from the <code>my_table</code> LazyFrame where the <code>age</code> column is greater than 30 and gives it the alias <code>older_people</code>. We then execute a second SQL query that selects all rows from the <code>older_people</code> CTE where the <code>name</code> column starts with the letter 'C'.</p>"},{"location":"user-guide/sql/intro/","title":"Introduction","text":"<p>While Polars supports interaction with SQL, it's recommended that users familiarize themselves with the expression syntax to produce more readable and expressive code. As the DataFrame interface is primary, new features are typically added to the expression API first. However, if you already have an existing SQL codebase or prefer the use of SQL, Polars does offers support for this.</p> <p>Note</p> <p>There is no separate SQL engine because Polars translates SQL queries into expressions, which are then executed using its own engine. This approach ensures that Polars maintains its performance and scalability advantages as a native DataFrame library, while still providing users with the ability to work with SQL.</p>"},{"location":"user-guide/sql/intro/#context","title":"Context","text":"<p>Polars uses the <code>SQLContext</code> object to manage SQL queries. The context contains a mapping of <code>DataFrame</code> and <code>LazyFrame</code> identifier names to their corresponding datasets<sup>1</sup>. The example below starts a <code>SQLContext</code>:</p>  Python <p> <code>SQLContext</code> <pre><code>ctx = pl.SQLContext()\n</code></pre></p> <p></p>"},{"location":"user-guide/sql/intro/#register-dataframes","title":"Register Dataframes","text":"<p>There are several ways to register DataFrames during <code>SQLContext</code> initialization.</p> <ul> <li>register all <code>LazyFrame</code> and <code>DataFrame</code> objects in the global namespace.</li> <li>register explicitly via a dictionary mapping, or kwargs.</li> </ul>  Python <p> <code>SQLContext</code> <pre><code>df = pl.DataFrame({\"a\": [1, 2, 3]})\nlf = pl.LazyFrame({\"b\": [4, 5, 6]})\n\n# Register all dataframes in the global namespace: registers both \"df\" and \"lf\"\nctx = pl.SQLContext(register_globals=True)\n\n# Register an explicit mapping of identifier name to frame\nctx = pl.SQLContext(frames={\"table_one\": df, \"table_two\": lf})\n\n# Register frames using kwargs; dataframe df as \"df\" and lazyframe lf as \"lf\"\nctx = pl.SQLContext(df=df, lf=lf)\n</code></pre></p> <p></p> <p>We can also register Pandas DataFrames by converting them to Polars first.</p>  Python <p> <code>SQLContext</code> <pre><code>import pandas as pd\n\ndf_pandas = pd.DataFrame({\"c\": [7, 8, 9]})\nctx = pl.SQLContext(df_pandas=pl.from_pandas(df_pandas))\n</code></pre></p> <p></p> <p>Note</p> <p>Converting a Pandas DataFrame backed by Numpy will trigger a potentially expensive conversion; however, if the Pandas DataFrame is already backed by Arrow then the conversion will be significantly cheaper (and in some cases close to free).</p> <p>Once the <code>SQLContext</code> is initialized, we can register additional Dataframes or unregister existing Dataframes with:</p> <ul> <li><code>register</code></li> <li><code>register_globals</code></li> <li><code>register_many</code></li> <li><code>unregister</code></li> </ul>"},{"location":"user-guide/sql/intro/#execute-queries-and-collect-results","title":"Execute queries and collect results","text":"<p>SQL queries are always executed in lazy mode to take advantage of the full set of query planning optimizations, so we have two options to collect the result:</p> <ul> <li>Set the parameter <code>eager_execution</code> to True in <code>SQLContext</code>; this ensures that Polars   automatically collects the LazyFrame results from <code>execute</code> calls.</li> <li>Set the parameter <code>eager</code> to True when executing a query with <code>execute</code>, or explicitly collect the   result using <code>collect</code>.</li> </ul> <p>We execute SQL queries by calling <code>execute</code> on a <code>SQLContext</code>.</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># For local files use scan_csv instead\npokemon = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\"\n)\nwith pl.SQLContext(register_globals=True, eager=True) as ctx:\n    df_small = ctx.execute(\"SELECT * from pokemon LIMIT 5\")\n    print(df_small)\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 str    \u2506 str    \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/intro/#execute-queries-from-multiple-sources","title":"Execute queries from multiple sources","text":"<p>SQL queries can be executed just as easily from multiple sources. In the example below, we register:</p> <ul> <li>a CSV file (loaded lazily)</li> <li>a NDJSON file (loaded lazily)</li> <li>a Pandas DataFrame</li> </ul> <p>And join them together using SQL. Lazy reading allows to only load the necessary rows and columns from the files.</p> <p>In the same way, it's possible to register cloud datalakes (S3, Azure Data Lake). A PyArrow dataset can point to the datalake, then Polars can read it with <code>scan_pyarrow_dataset</code>.</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># Input data:\n# products_masterdata.csv with schema {'product_id': Int64, 'product_name': String}\n# products_categories.json with schema {'product_id': Int64, 'category': String}\n# sales_data is a Pandas DataFrame with schema {'product_id': Int64, 'sales': Int64}\n\nwith pl.SQLContext(\n    products_masterdata=pl.scan_csv(\"docs/assets/data/products_masterdata.csv\"),\n    products_categories=pl.scan_ndjson(\"docs/assets/data/products_categories.json\"),\n    sales_data=pl.from_pandas(sales_data),\n    eager=True,\n) as ctx:\n    query = \"\"\"\n    SELECT\n        product_id,\n        product_name,\n        category,\n        sales\n    FROM\n        products_masterdata\n    LEFT JOIN products_categories USING (product_id)\n    LEFT JOIN sales_data USING (product_id)\n    \"\"\"\n    print(ctx.execute(query))\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 product_id \u2506 product_name \u2506 category   \u2506 sales \u2502\n\u2502 ---        \u2506 ---          \u2506 ---        \u2506 ---   \u2502\n\u2502 i64        \u2506 str          \u2506 str        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1          \u2506 Product A    \u2506 Category 1 \u2506 100   \u2502\n\u2502 2          \u2506 Product B    \u2506 Category 1 \u2506 200   \u2502\n\u2502 3          \u2506 Product C    \u2506 Category 2 \u2506 150   \u2502\n\u2502 4          \u2506 Product D    \u2506 Category 2 \u2506 250   \u2502\n\u2502 5          \u2506 Product E    \u2506 Category 3 \u2506 300   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/intro/#compatibility","title":"Compatibility","text":"<p>Polars does not support the complete SQL specification, but it does support a subset of the most common statement types.</p> <p>Note</p> <p>Where possible, Polars aims to follow PostgreSQL syntax definitions and function behaviour.</p> <p>For example, here is a non-exhaustive list of some of the supported functionality:</p> <ul> <li>Write a <code>CREATE</code> statements: <code>CREATE TABLE xxx AS ...</code></li> <li>Write a <code>SELECT</code> statements containing:<code>WHERE</code>,<code>ORDER</code>,<code>LIMIT</code>,<code>GROUP BY</code>,<code>UNION</code> and <code>JOIN</code>   clauses ...</li> <li>Write Common Table Expressions (CTE's) such as: <code>WITH tablename AS</code></li> <li>Explain a query: <code>EXPLAIN SELECT ...</code></li> <li>List registered tables: <code>SHOW TABLES</code></li> <li>Drop a table: <code>DROP TABLE tablename</code></li> <li>Truncate a table: <code>TRUNCATE TABLE tablename</code></li> </ul> <p>The following are some features that are not yet supported:</p> <ul> <li><code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code> statements</li> <li>Meta queries such as <code>ANALYZE</code></li> </ul> <p>In the upcoming sections we will cover each of the statements in more detail.</p> <ol> <li> <p>Additionally it also tracks the common table expressions as well.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/sql/select/","title":"SELECT","text":"<p>In Polars SQL, the <code>SELECT</code> statement is used to retrieve data from a table into a <code>DataFrame</code>. The basic syntax of a <code>SELECT</code> statement in Polars SQL is as follows:</p> <pre><code>SELECT column1, column2, ...\nFROM table_name;\n</code></pre> <p>Here, <code>column1</code>, <code>column2</code>, etc. are the columns that you want to select from the table. You can also use the wildcard <code>*</code> to select all columns. <code>table_name</code> is the name of the table or that you want to retrieve data from. In the sections below we will cover some of the more common SELECT variants</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>df = pl.DataFrame(\n    {\n        \"country\": [\"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"Netherlands\"],\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Phoenix\",\n            \"Amsterdam\",\n        ],\n        \"population\": [8399000, 3997000, 2705000, 2320000, 1680000, 900000],\n    }\n)\n\nctx = pl.SQLContext(population=df, eager=True)\n\nprint(ctx.execute(\"SELECT * FROM population\"))\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 USA         \u2506 New York    \u2506 8399000    \u2502\n\u2502 USA         \u2506 Los Angeles \u2506 3997000    \u2502\n\u2502 USA         \u2506 Chicago     \u2506 2705000    \u2502\n\u2502 USA         \u2506 Houston     \u2506 2320000    \u2502\n\u2502 USA         \u2506 Phoenix     \u2506 1680000    \u2502\n\u2502 Netherlands \u2506 Amsterdam   \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#group-by","title":"GROUP BY","text":"<p>The <code>GROUP BY</code> statement is used to group rows in a table by one or more columns and compute aggregate functions on each group.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT country, AVG(population) as avg_population\n        FROM population\n        GROUP BY country\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 avg_population \u2502\n\u2502 ---         \u2506 ---            \u2502\n\u2502 str         \u2506 f64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Netherlands \u2506 900000.0       \u2502\n\u2502 USA         \u2506 3.8202e6       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#order-by","title":"ORDER BY","text":"<p>The <code>ORDER BY</code> statement is used to sort the result set of a query by one or more columns in ascending or descending order.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT city, population\n        FROM population\n        ORDER BY population\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Amsterdam   \u2506 900000     \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 New York    \u2506 8399000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#join","title":"JOIN","text":"Python <p> <code>register_many</code> \u00b7 <code>execute</code> <pre><code>income = pl.DataFrame(\n    {\n        \"country\": [\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"Netherlands\",\n            \"Netherlands\",\n            \"Netherlands\",\n        ],\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Amsterdam\",\n            \"Rotterdam\",\n            \"Utrecht\",\n        ],\n        \"income\": [55000, 62000, 48000, 52000, 42000, 38000, 41000],\n    }\n)\nctx.register_many(income=income)\nresult = ctx.execute(\n    \"\"\"\n        SELECT income.*, population.population\n        FROM population\n        LEFT JOIN income ON population.city = income.city\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 city        \u2506 income \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---    \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64    \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 USA         \u2506 New York    \u2506 55000  \u2506 8399000    \u2502\n\u2502 USA         \u2506 Los Angeles \u2506 62000  \u2506 3997000    \u2502\n\u2502 USA         \u2506 Chicago     \u2506 48000  \u2506 2705000    \u2502\n\u2502 USA         \u2506 Houston     \u2506 52000  \u2506 2320000    \u2502\n\u2502 null        \u2506 null        \u2506 null   \u2506 1680000    \u2502\n\u2502 Netherlands \u2506 Amsterdam   \u2506 42000  \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#functions","title":"Functions","text":"<p>Polars provides a wide range of SQL functions, including:</p> <ul> <li>Mathematical functions: <code>ABS</code>, <code>EXP</code>, <code>LOG</code>, <code>ASIN</code>, <code>ACOS</code>, <code>ATAN</code>, etc.</li> <li>String functions: <code>LOWER</code>, <code>UPPER</code>, <code>LTRIM</code>, <code>RTRIM</code>, <code>STARTS_WITH</code>,<code>ENDS_WITH</code>.</li> <li>Aggregation functions: <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>STDDEV</code>, <code>FIRST</code> etc.</li> <li>Array functions: <code>EXPLODE</code>, <code>UNNEST</code>,<code>ARRAY_SUM</code>,<code>ARRAY_REVERSE</code>, etc.</li> </ul> <p>For a full list of supported functions go the API documentation. The example below demonstrates how to use a function in a query</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT city, population\n        FROM population\n        WHERE STARTS_WITH(country,'U')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 New York    \u2506 8399000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#table-functions","title":"Table Functions","text":"<p>In the examples earlier we first generated a DataFrame which we registered in the <code>SQLContext</code>. Polars also support directly reading from CSV, Parquet, JSON and IPC in your SQL query using table functions <code>read_xxx</code>.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT *\n        FROM read_csv('docs/assets/data/iris.csv')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 5.0          \u2506 3.6         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.7          \u2506 3.0         \u2506 5.2          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 Virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 Virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 Virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/show/","title":"SHOW TABLES","text":"<p>In Polars, the <code>SHOW TABLES</code> statement is used to list all the tables that have been registered in the current <code>SQLContext</code>. When you register a DataFrame with the <code>SQLContext</code>, you give it a name that can be used to refer to the DataFrame in subsequent SQL statements. The <code>SHOW TABLES</code> statement allows you to see a list of all the registered tables, along with their names.</p> <p>The syntax for the <code>SHOW TABLES</code> statement in Polars is as follows:</p> <pre><code>SHOW TABLES\n</code></pre> <p>Here's an example of how to use the <code>SHOW TABLES</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># Create some DataFrames and register them with the SQLContext\ndf1 = pl.LazyFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n        \"age\": [25, 30, 35, 40],\n    }\n)\ndf2 = pl.LazyFrame(\n    {\n        \"name\": [\"Ellen\", \"Frank\", \"Gina\", \"Henry\"],\n        \"age\": [45, 50, 55, 60],\n    }\n)\nctx = pl.SQLContext(mytable1=df1, mytable2=df2)\n\ntables = ctx.execute(\"SHOW TABLES\", eager=True)\n\nprint(tables)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name     \u2502\n\u2502 ---      \u2502\n\u2502 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 mytable1 \u2502\n\u2502 mytable2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we create two DataFrames and register them with the <code>SQLContext</code> using different names. We then execute a <code>SHOW TABLES</code> statement using the <code>execute()</code> method of the <code>SQLContext</code> object, which returns a DataFrame containing a list of all the registered tables and their names. The resulting DataFrame is then printed using the <code>print()</code> function.</p> <p>Note that the <code>SHOW TABLES</code> statement only lists tables that have been registered with the current <code>SQLContext</code>. If you register a DataFrame with a different <code>SQLContext</code> or in a different Python session, it will not appear in the list of tables returned by <code>SHOW TABLES</code>.</p>"},{"location":"user-guide/transformations/","title":"Transformations","text":"<p>The focus of this section is to describe different types of data transformations and provide some examples on how to use them.</p> <ul> <li>Joins</li> <li>Concatenation</li> <li>Pivot</li> <li>Unpivot</li> </ul>"},{"location":"user-guide/transformations/concatenation/","title":"Concatenation","text":"<p>There are a number of ways to concatenate data from separate DataFrames:</p> <ul> <li>two dataframes with the same columns can be vertically concatenated to make a longer   dataframe</li> <li>two dataframes with non-overlapping columns can be horizontally concatenated to make a   wider dataframe</li> <li>two dataframes with different numbers of rows and columns can be diagonally concatenated   to make a dataframe which might be longer and/ or wider. Where column names overlap values will be   vertically concatenated. Where column names do not overlap new rows and columns will be added.   Missing values will be set as <code>null</code></li> </ul>"},{"location":"user-guide/transformations/concatenation/#vertical-concatenation-getting-longer","title":"Vertical concatenation - getting longer","text":"<p>In a vertical concatenation you combine all of the rows from a list of <code>DataFrames</code> into a single longer <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_v1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_v2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"b\": [4],\n    }\n)\ndf_vertical_concat = pl.concat(\n    [\n        df_v1,\n        df_v2,\n    ],\n    how=\"vertical\",\n)\nprint(df_vertical_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_v1 = df!(\n        \"a\"=&gt; &amp;[1],\n        \"b\"=&gt; &amp;[3],\n)?;\nlet df_v2 = df!(\n        \"a\"=&gt; &amp;[2],\n        \"b\"=&gt; &amp;[4],\n)?;\nlet df_vertical_concat =\n    concat([df_v1.lazy(), df_v2.lazy()], UnionArgs::default())?.collect()?;\nprintln!(\"{}\", &amp;df_vertical_concat);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2502\n\u2502 2   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Vertical concatenation fails when the dataframes do not have the same column names.</p>"},{"location":"user-guide/transformations/concatenation/#horizontal-concatenation-getting-wider","title":"Horizontal concatenation - getting wider","text":"<p>In a horizontal concatenation you combine all of the columns from a list of <code>DataFrames</code> into a single wider <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_h1 = pl.DataFrame(\n    {\n        \"l1\": [1, 2],\n        \"l2\": [3, 4],\n    }\n)\ndf_h2 = pl.DataFrame(\n    {\n        \"r1\": [5, 6],\n        \"r2\": [7, 8],\n        \"r3\": [9, 10],\n    }\n)\ndf_horizontal_concat = pl.concat(\n    [\n        df_h1,\n        df_h2,\n    ],\n    how=\"horizontal\",\n)\nprint(df_horizontal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_h1 = df!(\n        \"l1\"=&gt; &amp;[1, 2],\n        \"l2\"=&gt; &amp;[3, 4],\n)?;\nlet df_h2 = df!(\n        \"r1\"=&gt; &amp;[5, 6],\n        \"r2\"=&gt; &amp;[7, 8],\n        \"r3\"=&gt; &amp;[9, 10],\n)?;\nlet df_horizontal_concat =\n    polars::functions::concat_df_horizontal(&amp;[df_h1, df_h2], true, false, false)?;\nprintln!(\"{}\", &amp;df_horizontal_concat);\n</code></pre></p> <pre><code>shape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l1  \u2506 l2  \u2506 r1  \u2506 r2  \u2506 r3  \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2506 5   \u2506 7   \u2506 9   \u2502\n\u2502 2   \u2506 4   \u2506 6   \u2506 8   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Horizontal concatenation fails when dataframes have overlapping columns.</p> <p>When dataframes have different numbers of rows, columns will be padded with <code>null</code> values at the end up to the maximum length.</p>  Python Rust <p> <code>concat</code> <pre><code>df_h1 = pl.DataFrame(\n    {\n        \"l1\": [1, 2],\n        \"l2\": [3, 4],\n    }\n)\ndf_h2 = pl.DataFrame(\n    {\n        \"r1\": [5, 6, 7],\n        \"r2\": [8, 9, 10],\n    }\n)\ndf_horizontal_concat = pl.concat(\n    [\n        df_h1,\n        df_h2,\n    ],\n    how=\"horizontal\",\n)\nprint(df_horizontal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_h1 = df!(\n        \"l1\"=&gt; &amp;[1, 2],\n        \"l2\"=&gt; &amp;[3, 4],\n)?;\nlet df_h2 = df!(\n        \"r1\"=&gt; &amp;[5, 6, 7],\n        \"r2\"=&gt; &amp;[8, 9, 10],\n)?;\nlet df_horizontal_concat =\n    polars::functions::concat_df_horizontal(&amp;[df_h1, df_h2], true, false, false)?;\nprintln!(\"{}\", &amp;df_horizontal_concat);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l1   \u2506 l2   \u2506 r1  \u2506 r2  \u2502\n\u2502 ---  \u2506 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 3    \u2506 5   \u2506 8   \u2502\n\u2502 2    \u2506 4    \u2506 6   \u2506 9   \u2502\n\u2502 null \u2506 null \u2506 7   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/concatenation/#diagonal-concatenation-getting-longer-wider-and-nullier","title":"Diagonal concatenation - getting longer, wider and <code>null</code>ier","text":"<p>In a diagonal concatenation you combine all of the row and columns from a list of <code>DataFrames</code> into a single longer and/or wider <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_d1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_d2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"d\": [4],\n    }\n)\n\ndf_diagonal_concat = pl.concat(\n    [\n        df_d1,\n        df_d2,\n    ],\n    how=\"diagonal\",\n)\nprint(df_diagonal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_d1 = df!(\n    \"a\"=&gt; &amp;[1],\n    \"b\"=&gt; &amp;[3],\n)?;\nlet df_d2 = df!(\n        \"a\"=&gt; &amp;[2],\n        \"d\"=&gt; &amp;[4],)?;\nlet df_diagonal_concat = polars::functions::concat_df_diagonal(&amp;[df_d1, df_d2])?;\nprintln!(\"{}\", &amp;df_diagonal_concat);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b    \u2506 d    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3    \u2506 null \u2502\n\u2502 2   \u2506 null \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Diagonal concatenation generates nulls when the column names do not overlap.</p> <p>When the dataframe shapes do not match and we have an overlapping semantic key then we can join the dataframes instead of concatenating them.</p>"},{"location":"user-guide/transformations/concatenation/#rechunking","title":"Rechunking","text":"<p>Before a concatenation we have two dataframes <code>df1</code> and <code>df2</code>. Each column in <code>df1</code> and <code>df2</code> is in one or more chunks in memory. By default, during concatenation the chunks in each column are not made contiguous. This makes the concat operation faster and consume less memory but it may slow down future operations that would benefit from having the data be in contiguous memory. The process of copying the fragmented chunks into a single new chunk is known as rechunking. Rechunking is an expensive operation. Prior to version 0.20.26, the default was to perform a rechunk but in new versions, the default is not to. If you do want Polars to rechunk the concatenated <code>DataFrame</code> you specify <code>rechunk = True</code> when doing the concatenation.</p>"},{"location":"user-guide/transformations/joins/","title":"Joins","text":"<p>A join operation combines columns from one or more dataframes into a new dataframe. The different \u201cjoining strategies\u201d and matching criteria used by the different types of joins influence how columns are combined and also what rows are included in the result of the join operation.</p> <p>The most common type of join is an \u201cequi join\u201d, in which rows are matched by a key expression. Polars supports several joining strategies for equi joins, which determine exactly how we handle the matching of rows. Polars also supports \u201cnon-equi joins\u201d, a type of join where the matching criterion is not an equality, and a type of join where rows are matched by key proximity, called \u201casof join\u201d.</p>"},{"location":"user-guide/transformations/joins/#quick-reference-table","title":"Quick reference table","text":"<p>The table below acts as a quick reference for people who know what they are looking for. If you want to learn about joins in general and how to work with them in Polars, feel free to skip the table and keep reading below.</p>  Python Rust <p> <code>join</code> <code>join_where</code> <code>join_asof</code></p> <p> <code>join</code> ( semi_anti_join needed for some options.)  <code>join_asof_by</code>  Available on feature asof_join <code>join_where</code>  Available on feature iejoin</p> Type Function Brief description Equi inner join <code>join(..., how=\"inner\")</code> Keeps rows that matched both on the left and right. Equi left outer join <code>join(..., how=\"left\")</code> Keeps all rows from the left plus matching rows from the right. Non-matching rows from the left have their right columns filled with <code>null</code>. Equi right outer join <code>join(..., how=\"right\")</code> Keeps all rows from the right plus matching rows from the left. Non-matching rows from the right have their left columns filled with <code>null</code>. Equi full join <code>join(..., how=\"full\")</code> Keeps all rows from either dataframe, regardless of whether they match or not. Non-matching rows from one side have the columns from the other side filled with <code>null</code>. Equi semi join <code>join(..., how=\"semi\")</code> Keeps rows from the left that have a match on the right. Equi anti join <code>join(..., how=\"anti\")</code> Keeps rows from the left that do not have a match on the right. Non-equi inner join <code>join_where</code> Finds all possible pairings of rows from the left and right that satisfy the given predicate(s). Asof join <code>join_asof</code>/<code>join_asof_by</code> Like a left outer join, but matches on the nearest key instead of on exact key matches. Cartesian product <code>join(..., how=\"cross\")</code> Computes the Cartesian product of the two dataframes."},{"location":"user-guide/transformations/joins/#equi-joins","title":"Equi joins","text":"<p>In an equi join, rows are matched by checking equality of a key expression. You can do an equi join with the function <code>join</code> by specifying the name of the column to be used as key. For the examples, we will be loading some (modified) Monopoly property data.</p> <p>First, we load a dataframe that contains property names and their colour group in the game:</p>  Python Rust <pre><code>import polars as pl\n\nprops_groups = pl.read_csv(\"docs/assets/data/monopoly_props_groups.csv\").head(5)\nprint(props_groups)\n</code></pre> <pre><code>let props_groups = CsvReadOptions::default()\n    .with_has_header(true)\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/monopoly_props_groups.csv\".into()))?\n    .finish()?\n    .head(Some(5));\nprintln!(\"{props_groups}\");\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2502\n\u2502 ---                  \u2506 ---        \u2502\n\u2502 str                  \u2506 str        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2502\n\u2502 The Shire            \u2506 fantasy    \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Next, we load a dataframe that contains property names and their price in the game:</p>  Python Rust <pre><code>props_prices = pl.read_csv(\"docs/assets/data/monopoly_props_prices.csv\").head(5)\nprint(props_prices)\n</code></pre> <pre><code>let props_prices = CsvReadOptions::default()\n    .with_has_header(true)\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/monopoly_props_prices.csv\".into()))?\n    .finish()?\n    .head(Some(5));\nprintln!(\"{props_prices}\");\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 cost \u2502\n\u2502 ---                  \u2506 ---  \u2502\n\u2502 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 60   \u2502\n\u2502 Sesame Street        \u2506 100  \u2502\n\u2502 Kings Cross Station  \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Now, we join both dataframes to create a dataframe that contains property names, colour groups, and prices:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\")\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>// In Rust, we cannot use the shorthand of specifying a common\n// column name just once.\nlet result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::default(),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2506 60   \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The result has four rows but both dataframes used in the operation had five rows. Polars uses a joining strategy to determine what happens with rows that have multiple matches or with rows that have no match at all. By default, Polars computes an \u201cinner join\u201d but there are other join strategies that we show next.</p> <p>In the example above, the two dataframes conveniently had the column we wish to use as key with the same name and with the values in the exact same format. Suppose, for the sake of argument, that one of the dataframes had a differently named column and the other had the property names in lower case:</p>  Python Rust <p> <code>str namespace</code> <pre><code>props_groups2 = props_groups.with_columns(\n    pl.col(\"property_name\").str.to_lowercase(),\n)\nprint(props_groups2)\n</code></pre></p> <p> <code>str namespace</code> \u00b7  Available on feature strings <pre><code>let props_groups2 = props_groups\n    .clone()\n    .lazy()\n    .with_column(col(\"property_name\").str().to_lowercase())\n    .collect()?;\nprintln!(\"{props_groups2}\");\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2502\n\u2502 ---                  \u2506 ---        \u2502\n\u2502 str                  \u2506 str        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 old ken road         \u2506 brown      \u2502\n\u2502 whitechapel road     \u2506 brown      \u2502\n\u2502 the shire            \u2506 fantasy    \u2502\n\u2502 kings cross station  \u2506 stations   \u2502\n\u2502 the angel, islington \u2506 light_blue \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <pre><code>props_prices2 = props_prices.select(\n    pl.col(\"property_name\").alias(\"name\"), pl.col(\"cost\")\n)\nprint(props_prices2)\n</code></pre> <pre><code>let props_prices2 = props_prices\n    .clone()\n    .lazy()\n    .select([col(\"property_name\").alias(\"name\"), col(\"cost\")])\n    .collect()?;\nprintln!(\"{props_prices2}\");\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name                 \u2506 cost \u2502\n\u2502 ---                  \u2506 ---  \u2502\n\u2502 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 60   \u2502\n\u2502 Sesame Street        \u2506 100  \u2502\n\u2502 Kings Cross Station  \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In a situation like this, where we may want to perform the same join as before, we can leverage <code>join</code>'s flexibility and specify arbitrary expressions to compute the joining key on the left and on the right, allowing one to compute row keys dynamically:</p>  Python Rust <p> <code>join</code> \u00b7 <code>str namespace</code> <pre><code>result = props_groups2.join(\n    props_prices2,\n    left_on=\"property_name\",\n    right_on=pl.col(\"name\").str.to_lowercase(),\n)\nprint(result)\n</code></pre></p> <p> <code>join</code> \u00b7 <code>str namespace</code> \u00b7  Available on feature strings <pre><code>let result = props_groups2\n    .lazy()\n    .join(\n        props_prices2.lazy(),\n        [col(\"property_name\")],\n        [col(\"name\").str().to_lowercase()],\n        JoinArgs::default(),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 name                 \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---                  \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 old ken road         \u2506 brown      \u2506 Old Ken Road         \u2506 60   \u2502\n\u2502 whitechapel road     \u2506 brown      \u2506 Whitechapel Road     \u2506 60   \u2502\n\u2502 kings cross station  \u2506 stations   \u2506 Kings Cross Station  \u2506 200  \u2502\n\u2502 the angel, islington \u2506 light_blue \u2506 The Angel, Islington \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Because we are joining on the right with an expression, Polars preserves the column \u201cproperty_name\u201d from the left and the column \u201cname\u201d from the right so we can have access to the original values that the key expressions were applied to.</p>"},{"location":"user-guide/transformations/joins/#join-strategies","title":"Join strategies","text":"<p>When computing a join with <code>df1.join(df2, ...)</code>, we can specify one of many different join strategies. A join strategy specifies what rows to keep from each dataframe based on whether they match rows from the other dataframe.</p>"},{"location":"user-guide/transformations/joins/#inner-join","title":"Inner join","text":"<p>In an inner join the resulting dataframe only contains the rows from the left and right dataframes that matched. That is the default strategy used by <code>join</code> and above we can see an example of that. We repeat the example here and explicitly specify the join strategy:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"inner\")\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Inner),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2506 60   \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The result does not include the row from <code>props_groups</code> that contains \u201cThe Shire\u201d and the result also does not include the row from <code>props_prices</code> that contains \u201cSesame Street\u201d.</p>"},{"location":"user-guide/transformations/joins/#left-join","title":"Left join","text":"<p>A left outer join is a join where the result contains all the rows from the left dataframe and the rows of the right dataframe that matched any rows from the left dataframe.</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"left\")\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Left),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2506 60   \u2502\n\u2502 The Shire            \u2506 fantasy    \u2506 null \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If there are any rows from the left dataframe that have no matching rows on the right dataframe, they get the value <code>null</code> on the new columns.</p>"},{"location":"user-guide/transformations/joins/#right-join","title":"Right join","text":"<p>Computationally speaking, a right outer join is exactly the same as a left outer join, but with the arguments swapped. Here is an example:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"right\")\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Right),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group      \u2506 property_name        \u2506 cost \u2502\n\u2502 ---        \u2506 ---                  \u2506 ---  \u2502\n\u2502 str        \u2506 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 brown      \u2506 Old Ken Road         \u2506 60   \u2502\n\u2502 brown      \u2506 Whitechapel Road     \u2506 60   \u2502\n\u2502 null       \u2506 Sesame Street        \u2506 100  \u2502\n\u2502 stations   \u2506 Kings Cross Station  \u2506 200  \u2502\n\u2502 light_blue \u2506 The Angel, Islington \u2506 100  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We show that <code>df1.join(df2, how=\"right\", ...)</code> is the same as <code>df2.join(df1, how=\"left\", ...)</code>, up to the order of the columns of the result, with the computation below:</p>  Python Rust <p> <code>join</code> <pre><code>print(\n    result.equals(\n        props_prices.join(\n            props_groups,\n            on=\"property_name\",\n            how=\"left\",\n            # Reorder the columns to match the order from above.\n        ).select(pl.col(\"group\"), pl.col(\"property_name\"), pl.col(\"cost\"))\n    )\n)\n</code></pre></p> <p> <code>join</code> <pre><code>// `equals_missing` is needed instead of `equals`\n// so that missing values compare as equal.\nlet dfs_match = result.equals_missing(\n    &amp;props_prices\n        .clone()\n        .lazy()\n        .join(\n            props_groups.clone().lazy(),\n            [col(\"property_name\")],\n            [col(\"property_name\")],\n            JoinArgs::new(JoinType::Left),\n        )\n        .select([\n            // Reorder the columns to match the order of `result`.\n            col(\"group\"),\n            col(\"property_name\"),\n            col(\"cost\"),\n        ])\n        .collect()?,\n);\nprintln!(\"{dfs_match}\");\n</code></pre></p> <pre><code>True\n</code></pre>"},{"location":"user-guide/transformations/joins/#full-join","title":"Full join","text":"<p>A full outer join will keep all of the rows from the left and right dataframes, even if they don't have matching rows in the other dataframe:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"full\")\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Full),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 property_name_right  \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---                  \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2506 Old Ken Road         \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2506 Whitechapel Road     \u2506 60   \u2502\n\u2502 null                 \u2506 null       \u2506 Sesame Street        \u2506 100  \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2506 Kings Cross Station  \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2506 The Angel, Islington \u2506 100  \u2502\n\u2502 The Shire            \u2506 fantasy    \u2506 null                 \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case, we see that we get two columns <code>property_name</code> and <code>property_name_right</code> to make up for the fact that we are matching on the column <code>property_name</code> of both dataframes and there are some names for which there are no matches. The two columns help differentiate the source of each row data. If we wanted to force <code>join</code> to coalesce the two columns <code>property_name</code> into a single column, we could set <code>coalesce=True</code> explicitly:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(\n    props_prices,\n    on=\"property_name\",\n    how=\"full\",\n    coalesce=True,\n)\nprint(result)\n</code></pre></p> <p> <code>join</code> <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Full).with_coalesce(JoinCoalesce::CoalesceColumns),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2506 cost \u2502\n\u2502 ---                  \u2506 ---        \u2506 ---  \u2502\n\u2502 str                  \u2506 str        \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2506 60   \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2506 60   \u2502\n\u2502 Sesame Street        \u2506 null       \u2506 100  \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2506 200  \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2506 100  \u2502\n\u2502 The Shire            \u2506 fantasy    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When not set, the parameter <code>coalesce</code> is determined automatically from the join strategy and the key(s) specified, which is why the inner, left, and right, joins acted as if <code>coalesce=True</code>, even though we didn't set it.</p>"},{"location":"user-guide/transformations/joins/#semi-join","title":"Semi join","text":"<p>A semi join will return the rows of the left dataframe that have a match in the right dataframe, but we do not actually join the matching rows:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"semi\")\nprint(result)\n</code></pre></p> <p> <code>join</code> \u00b7  Available on feature semi_anti_join <pre><code>let result = props_groups\n    .clone()\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Semi),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name        \u2506 group      \u2502\n\u2502 ---                  \u2506 ---        \u2502\n\u2502 str                  \u2506 str        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Old Ken Road         \u2506 brown      \u2502\n\u2502 Whitechapel Road     \u2506 brown      \u2502\n\u2502 Kings Cross Station  \u2506 stations   \u2502\n\u2502 The Angel, Islington \u2506 light_blue \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>A semi join acts as a sort of row filter based on a second dataframe.</p>"},{"location":"user-guide/transformations/joins/#anti-join","title":"Anti join","text":"<p>Conversely, an anti join will return the rows of the left dataframe that do not have a match in the right dataframe:</p>  Python Rust <p> <code>join</code> <pre><code>result = props_groups.join(props_prices, on=\"property_name\", how=\"anti\")\nprint(result)\n</code></pre></p> <p> <code>join</code> \u00b7  Available on feature semi_anti_join <pre><code>let result = props_groups\n    .lazy()\n    .join(\n        props_prices.clone().lazy(),\n        [col(\"property_name\")],\n        [col(\"property_name\")],\n        JoinArgs::new(JoinType::Anti),\n    )\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 property_name \u2506 group   \u2502\n\u2502 ---           \u2506 ---     \u2502\n\u2502 str           \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 The Shire     \u2506 fantasy \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#non-equi-joins","title":"Non-equi joins","text":"<p>In a non-equi join matches between the left and right dataframes are computed differently. Instead of looking for matches on key expressions, we provide a single predicate that determines what rows of the left dataframe can be paired up with what rows of the right dataframe.</p> <p>For example, consider the following Monopoly players and their current cash:</p>  Python Rust <pre><code>players = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\"],\n        \"cash\": [78, 135],\n    }\n)\nprint(players)\n</code></pre> <pre><code>let players = df!(\n    \"name\" =&gt; [\"Alice\", \"Bob\"],\n    \"cash\" =&gt; [78, 135],\n)?;\nprintln!(\"{players}\");\n</code></pre> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 cash \u2502\n\u2502 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice \u2506 78   \u2502\n\u2502 Bob   \u2506 135  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Using a non-equi join we can easily build a dataframe with all the possible properties that each player could be interested in buying. We use the function <code>join_where</code> to compute a non-equi join:</p>  Python Rust <p> <code>join_where</code> <pre><code>result = players.join_where(props_prices, pl.col(\"cash\") &gt; pl.col(\"cost\"))\nprint(result)\n</code></pre></p> <p> <code>join_where</code> \u00b7  Available on feature iejoin <pre><code>let result = players\n    .clone()\n    .lazy()\n    .join_builder()\n    .with(props_prices.lazy())\n    .join_where(vec![col(\"cash\").cast(DataType::Int64).gt(col(\"cost\"))])\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 cash \u2506 property_name        \u2506 cost \u2502\n\u2502 ---   \u2506 ---  \u2506 ---                  \u2506 ---  \u2502\n\u2502 str   \u2506 i64  \u2506 str                  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bob   \u2506 135  \u2506 Sesame Street        \u2506 100  \u2502\n\u2502 Bob   \u2506 135  \u2506 The Angel, Islington \u2506 100  \u2502\n\u2502 Bob   \u2506 135  \u2506 Old Ken Road         \u2506 60   \u2502\n\u2502 Bob   \u2506 135  \u2506 Whitechapel Road     \u2506 60   \u2502\n\u2502 Alice \u2506 78   \u2506 Old Ken Road         \u2506 60   \u2502\n\u2502 Alice \u2506 78   \u2506 Whitechapel Road     \u2506 60   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can provide multiple expressions as predicates, in that case they will be AND combined. You can also combine expressions in a single expression if you need other combinations like OR or XOR.</p>"},{"location":"user-guide/transformations/joins/#asof-join","title":"Asof join","text":"<p>An <code>asof</code> join is like a left join except that we match on nearest key rather than equal keys. In Polars we can do an asof join with the <code>join_asof</code> method.</p> <p>For the asof join we will consider a scenario inspired by the stock market. Suppose a stock market broker has a dataframe called <code>df_trades</code> showing transactions it has made for different stocks.</p>  Python Rust <pre><code>from datetime import datetime\n\ndf_trades = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 3, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"B\", \"C\"],\n        \"trade\": [101, 299, 301, 500],\n    }\n)\nprint(df_trades)\n</code></pre> <pre><code>use chrono::prelude::*;\n\nlet df_trades = df!(\n    \"time\" =&gt; [\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 1, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 1, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 3, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 6, 0).unwrap(),\n    ],\n    \"stock\" =&gt; [\"A\", \"B\", \"B\", \"C\"],\n    \"trade\" =&gt; [101, 299, 301, 500],\n)?;\nprintln!(\"{df_trades}\");\n</code></pre> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The broker has another dataframe called <code>df_quotes</code> showing prices it has quoted for these stocks:</p>  Python Rust <pre><code>df_quotes = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 0, 0),\n            datetime(2020, 1, 1, 9, 2, 0),\n            datetime(2020, 1, 1, 9, 4, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"C\", \"A\"],\n        \"quote\": [100, 300, 501, 102],\n    }\n)\n\nprint(df_quotes)\n</code></pre> <pre><code>let df_quotes = df!(\n    \"time\" =&gt; [\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 1, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 2, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 4, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 6, 0).unwrap(),\n    ],\n    \"stock\" =&gt; [\"A\", \"B\", \"C\", \"A\"],\n    \"quote\" =&gt; [100, 300, 501, 102],\n)?;\nprintln!(\"{df_quotes}\");\n</code></pre> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:00:00 \u2506 A     \u2506 100   \u2502\n\u2502 2020-01-01 09:02:00 \u2506 B     \u2506 300   \u2502\n\u2502 2020-01-01 09:04:00 \u2506 C     \u2506 501   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 A     \u2506 102   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You want to produce a dataframe showing for each trade the most recent quote provided on or before the time of the trade. You do this with <code>join_asof</code> (using the default <code>strategy = \"backward\"</code>). To avoid joining between trades on one stock with a quote on another you must specify an exact preliminary join on the stock column with <code>by=\"stock\"</code>.</p>  Python Rust <p> <code>join_asof</code> <pre><code>df_asof_join = df_trades.join_asof(df_quotes, on=\"time\", by=\"stock\")\nprint(df_asof_join)\n</code></pre></p> <p> <code>join_asof_by</code> \u00b7  Available on feature asof_join <pre><code>let result = df_trades.join_asof_by(\n    &amp;df_quotes,\n    \"time\",\n    \"time\",\n    [\"stock\"],\n    [\"stock\"],\n    AsofStrategy::Backward,\n    None,\n    true,\n    true,\n)?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 501   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you want to make sure that only quotes within a certain time range are joined to the trades you can specify the <code>tolerance</code> argument. In this case we want to make sure that the last preceding quote is within 1 minute of the trade so we set <code>tolerance = \"1m\"</code>.</p>  Python Rust <p> <code>join_asof</code> <pre><code>df_asof_tolerance_join = df_trades.join_asof(\n    df_quotes, on=\"time\", by=\"stock\", tolerance=\"1m\"\n)\nprint(df_asof_tolerance_join)\n</code></pre></p> <p> <code>join_asof_by</code> \u00b7  Available on feature asof_join <pre><code>let result = df_trades.join_asof_by(\n    &amp;df_quotes,\n    \"time\",\n    \"time\",\n    [\"stock\"],\n    [\"stock\"],\n    AsofStrategy::Backward,\n    Some(AnyValue::Duration(60000, TimeUnit::Milliseconds)),\n    true,\n    true,\n)?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#cartesian-product","title":"Cartesian product","text":"<p>Polars allows you to compute the Cartesian product of two dataframes, producing a dataframe where all rows of the left dataframe are paired up with all the rows of the right dataframe. To compute the Cartesian product of two dataframes, you can pass the strategy <code>how=\"cross\"</code> to the function <code>join</code> without specifying any of <code>on</code>, <code>left_on</code>, and <code>right_on</code>:</p>  Python Rust <p> <code>join</code> <pre><code>tokens = pl.DataFrame({\"monopoly_token\": [\"hat\", \"shoe\", \"boat\"]})\n\nresult = players.select(pl.col(\"name\")).join(tokens, how=\"cross\")\nprint(result)\n</code></pre></p> <p> <code>cross_join</code> \u00b7  Available on feature cross_join <pre><code>let tokens = df!(\n    \"monopoly_token\" =&gt; [\"hat\", \"shoe\", \"boat\"],\n)?;\n\nlet result = players\n    .lazy()\n    .select([col(\"name\")])\n    .cross_join(tokens.lazy(), None)\n    .collect()?;\nprintln!(\"{result}\");\n</code></pre></p> <pre><code>shape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2506 monopoly_token \u2502\n\u2502 ---   \u2506 ---            \u2502\n\u2502 str   \u2506 str            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice \u2506 hat            \u2502\n\u2502 Alice \u2506 shoe           \u2502\n\u2502 Alice \u2506 boat           \u2502\n\u2502 Bob   \u2506 hat            \u2502\n\u2502 Bob   \u2506 shoe           \u2502\n\u2502 Bob   \u2506 boat           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/","title":"Pivots","text":"<p>Pivot a column in a <code>DataFrame</code> and perform one of the following aggregations:</p> <ul> <li>first</li> <li>last</li> <li>sum</li> <li>min</li> <li>max</li> <li>mean</li> <li>median</li> <li>len</li> </ul> <p>The pivot operation consists of a group by one, or multiple columns (these will be the new y-axis), the column that will be pivoted (this will be the new x-axis) and an aggregation.</p>"},{"location":"user-guide/transformations/pivot/#dataset","title":"Dataset","text":"Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"foo\": [\"A\", \"A\", \"B\", \"B\", \"C\"],\n        \"N\": [1, 2, 2, 4, 2],\n        \"bar\": [\"k\", \"l\", \"m\", \"n\", \"o\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df!(\n        \"foo\"=&gt; [\"A\", \"A\", \"B\", \"B\", \"C\"],\n        \"bar\"=&gt; [\"k\", \"l\", \"m\", \"n\", \"o\"],\n        \"N\"=&gt; [1, 2, 2, 4, 2],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 N   \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1   \u2506 k   \u2502\n\u2502 A   \u2506 2   \u2506 l   \u2502\n\u2502 B   \u2506 2   \u2506 m   \u2502\n\u2502 B   \u2506 4   \u2506 n   \u2502\n\u2502 C   \u2506 2   \u2506 o   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#eager","title":"Eager","text":"Python Rust <p> <code>pivot</code> <pre><code>out = df.pivot(\"bar\", index=\"foo\", values=\"N\", aggregate_function=\"first\")\nprint(out)\n</code></pre></p> <p> <code>pivot</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .pivot(\n        Selector::ByName {\n            names: [PlSmallStr::from(\"foo\")].into(),\n            strict: true,\n        },\n        Arc::new(df!(\"\" =&gt; [\"A\", \"B\", \"C\"])?),\n        Selector::ByName {\n            names: [PlSmallStr::from(\"bar\")].into(),\n            strict: true,\n        },\n        Selector::ByName {\n            names: [PlSmallStr::from(\"N\")].into(),\n            strict: true,\n        },\n        Expr::Agg(AggExpr::Item {\n            input: Arc::new(Expr::Element),\n            allow_empty: true,\n        }),\n        false,\n        \"_\".into(),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#lazy","title":"Lazy","text":"<p>A Polars <code>LazyFrame</code> always need to know the schema of a computation statically (before collecting the query). As a pivot's output schema depends on the data, and it is therefore impossible to determine the schema without running the query.</p> <p>Polars could have abstracted this fact for you just like Spark does, but we don't want you to shoot yourself in the foot with a shotgun. The cost should be clear upfront.</p>  Python Rust <p> <code>pivot</code> <pre><code>q = (\n    df.lazy()\n    .collect()\n    .pivot(index=\"foo\", on=\"bar\", values=\"N\", aggregate_function=\"first\")\n    .lazy()\n)\nout = q.collect()\nprint(out)\n</code></pre></p> <p> <code>pivot</code> <pre><code>let q = df.clone().lazy();\nlet q2 = q.pivot(\n    Selector::ByName {\n        names: [PlSmallStr::from(\"foo\")].into(),\n        strict: true,\n    },\n    Arc::new(df!(\"\" =&gt; [\"A\", \"B\", \"C\"])?),\n    Selector::ByName {\n        names: [PlSmallStr::from(\"bar\")].into(),\n        strict: true,\n    },\n    Selector::ByName {\n        names: [PlSmallStr::from(\"N\")].into(),\n        strict: true,\n    },\n    Expr::Agg(AggExpr::Item {\n        input: Arc::new(Expr::Element),\n        allow_empty: true,\n    }),\n    false,\n    \"_\".into(),\n);\nlet out = q2.collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/unpivot/","title":"Unpivots","text":"<p>Unpivot unpivots a DataFrame from wide format to long format</p>"},{"location":"user-guide/transformations/unpivot/#dataset","title":"Dataset","text":"Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"A\": [\"a\", \"b\", \"a\"],\n        \"B\": [1, 3, 5],\n        \"C\": [10, 11, 12],\n        \"D\": [2, 4, 6],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df!(\n        \"A\"=&gt; &amp;[\"a\", \"b\", \"a\"],\n        \"B\"=&gt; &amp;[1, 3, 5],\n        \"C\"=&gt; &amp;[10, 11, 12],\n        \"D\"=&gt; &amp;[2, 4, 6],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 C   \u2506 D   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 10  \u2506 2   \u2502\n\u2502 b   \u2506 3   \u2506 11  \u2506 4   \u2502\n\u2502 a   \u2506 5   \u2506 12  \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/unpivot/#eager-lazy","title":"Eager + lazy","text":"<p><code>Eager</code> and <code>lazy</code> have the same API.</p>  Python Rust <p> <code>unpivot</code> <pre><code>out = df.unpivot([\"C\", \"D\"], index=[\"A\", \"B\"])\nprint(out)\n</code></pre></p> <p> <code>unpivot</code> <pre><code>let out = df.unpivot(Some([\"A\", \"B\"]), [\"C\", \"D\"])?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 variable \u2506 value \u2502\n\u2502 --- \u2506 --- \u2506 ---      \u2506 ---   \u2502\n\u2502 str \u2506 i64 \u2506 str      \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 C        \u2506 10    \u2502\n\u2502 b   \u2506 3   \u2506 C        \u2506 11    \u2502\n\u2502 a   \u2506 5   \u2506 C        \u2506 12    \u2502\n\u2502 a   \u2506 1   \u2506 D        \u2506 2     \u2502\n\u2502 b   \u2506 3   \u2506 D        \u2506 4     \u2502\n\u2502 a   \u2506 5   \u2506 D        \u2506 6     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/","title":"Filtering","text":"<p>Filtering date columns works in the same way as with other types of columns using the <code>.filter</code> method.</p> <p>Polars uses Python's native <code>datetime</code>, <code>date</code> and <code>timedelta</code> for equality comparisons between the datatypes <code>pl.Datetime</code>, <code>pl.Date</code> and <code>pl.Duration</code>.</p> <p>In the following example we use a time series of Apple stock prices.</p>  Python Rust <p> <code>read_csv</code> <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.read_csv(\"docs/assets/data/apple_stock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReadOptions::default()\n    .map_parse_options(|parse_options| parse_options.with_try_parse_dates(true))\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/apple_stock.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap();\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-single-dates","title":"Filtering by single dates","text":"<p>We can filter by a single date using an equality comparison in a filter expression:</p>  Python Rust <p> <code>filter</code> <pre><code>filtered_df = df.filter(\n    pl.col(\"Date\") == datetime(1995, 10, 16),\n)\nprint(filtered_df)\n</code></pre></p> <p> <code>filter</code> <pre><code>let filtered_df = df\n    .clone()\n    .lazy()\n    .filter(col(\"Date\").eq(lit(NaiveDate::from_ymd_opt(1995, 10, 16).unwrap())))\n    .collect()?;\nprintln!(\"{}\", &amp;filtered_df);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note we are using the lowercase <code>datetime</code> method rather than the uppercase <code>Datetime</code> data type.</p>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-a-date-range","title":"Filtering by a date range","text":"<p>We can filter by a range of dates using the <code>is_between</code> method in a filter expression with the start and end dates:</p>  Python Rust <p> <code>filter</code> \u00b7 <code>is_between</code> <pre><code>filtered_range_df = df.filter(\n    pl.col(\"Date\").is_between(datetime(1995, 7, 1), datetime(1995, 11, 1)),\n)\nprint(filtered_range_df)\n</code></pre></p> <p> <code>filter</code> \u00b7 <code>is_between</code> \u00b7  Available on feature is_between <pre><code>let filtered_range_df = df\n    .lazy()\n    .filter(\n        col(\"Date\")\n            .gt(lit(NaiveDate::from_ymd_opt(1995, 7, 1).unwrap()))\n            .and(col(\"Date\").lt(lit(NaiveDate::from_ymd_opt(1995, 11, 1).unwrap()))),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;filtered_range_df);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-07-06 \u2506 47.0  \u2502\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-with-negative-dates","title":"Filtering with negative dates","text":"<p>Say you are working with an archeologist and are dealing in negative dates. Polars can parse and store them just fine, but the Python <code>datetime</code> library does not. So for filtering, you should use attributes in the <code>.dt</code> namespace:</p>  Python Rust <p> <code>str.to_date</code> <pre><code>ts = pl.Series([\"-1300-05-23\", \"-1400-03-02\"]).str.to_date()\n\nnegative_dates_df = pl.DataFrame({\"ts\": ts, \"values\": [3, 4]})\n\nnegative_dates_filtered_df = negative_dates_df.filter(pl.col(\"ts\").dt.year() &lt; -1300)\nprint(negative_dates_filtered_df)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7  Available on feature dtype-date <pre><code>    let negative_dates_df = df!(\n    \"ts\"=&gt; &amp;[\"-1300-05-23\", \"-1400-03-02\"],\n    \"values\"=&gt; &amp;[3, 4])?\n    .lazy()\n    .with_column(col(\"ts\").str().to_date(StrptimeOptions::default()))\n    .collect()?;\n\n    let negative_dates_filtered_df = negative_dates_df\n        .lazy()\n        .filter(col(\"ts\").dt().year().lt(-1300))\n        .collect()?;\n    println!(\"{}\", &amp;negative_dates_filtered_df);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts          \u2506 values \u2502\n\u2502 ---         \u2506 ---    \u2502\n\u2502 date        \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -1400-03-02 \u2506 4      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/","title":"Parsing","text":"<p>Polars has native support for parsing time series data and doing more sophisticated operations such as temporal grouping and resampling.</p>"},{"location":"user-guide/transformations/time-series/parsing/#datatypes","title":"Datatypes","text":"<p>Polars has the following datetime datatypes:</p> <ul> <li><code>Date</code>: Date representation e.g. 2014-07-08. It is internally represented as days since UNIX epoch   encoded by a 32-bit signed integer.</li> <li><code>Datetime</code>: Datetime representation e.g. 2014-07-08 07:00:00. It is internally represented as a 64   bit integer since the Unix epoch and can have different units such as ns, us, ms.</li> <li><code>Duration</code>: A time delta type that is created when subtracting <code>Date/Datetime</code>. Similar to   <code>timedelta</code> in Python.</li> <li><code>Time</code>: Time representation, internally represented as nanoseconds since midnight.</li> </ul>"},{"location":"user-guide/transformations/time-series/parsing/#parsing-dates-from-a-file","title":"Parsing dates from a file","text":"<p>When loading from a CSV file Polars attempts to parse dates and times if the <code>try_parse_dates</code> flag is set to <code>True</code>:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/assets/data/apple_stock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReadOptions::default()\n    .map_parse_options(|parse_options| parse_options.with_try_parse_dates(true))\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/apple_stock.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap();\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This flag will trigger schema inference on a number of rows, as configured by the <code>infer_schema_length</code> setting (100 rows by default). Schema inference is computationally expensive and can slow down file loading if a high number of rows is used.</p> <p>On the other hand binary formats such as parquet have a schema that is respected by Polars.</p>"},{"location":"user-guide/transformations/time-series/parsing/#casting-strings-to-dates","title":"Casting strings to dates","text":"<p>You can also cast a column of datetimes encoded as strings to a datetime type. You do this by calling the string <code>str.to_date</code> method and passing the format of the date string:</p>  Python Rust <p> <code>read_csv</code> \u00b7 <code>str.to_date</code> <pre><code>df = pl.read_csv(\"docs/assets/data/apple_stock.csv\", try_parse_dates=False)\n\ndf = df.with_columns(pl.col(\"Date\").str.to_date(\"%Y-%m-%d\"))\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature dtype-date \u00b7  Available on feature csv <pre><code>let df = CsvReadOptions::default()\n    .map_parse_options(|parse_options| parse_options.with_try_parse_dates(false))\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/apple_stock.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap();\nlet df = df\n    .lazy()\n    .with_columns([col(\"Date\").str().to_date(StrptimeOptions::default())])\n    .collect()?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The format string specification can be found here..</p>"},{"location":"user-guide/transformations/time-series/parsing/#extracting-date-features-from-a-date-column","title":"Extracting date features from a date column","text":"<p>You can extract data features such as the year or day from a date column using the <code>.dt</code> namespace:</p>  Python Rust <p> <code>dt.year</code> <pre><code>df_with_year = df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <p> <code>dt.year</code> <pre><code>let df_with_year = df\n    .lazy()\n    .with_columns([col(\"Date\").dt().year().alias(\"year\")])\n    .collect()?;\nprintln!(\"{}\", &amp;df_with_year);\n</code></pre></p> <pre><code>shape: (100, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2506 year \u2502\n\u2502 ---        \u2506 ---    \u2506 ---  \u2502\n\u2502 date       \u2506 f64    \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2506 1981 \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2506 1981 \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2506 1981 \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2506 1981 \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2506 1982 \u2502\n\u2502 \u2026          \u2506 \u2026      \u2506 \u2026    \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2506 2012 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2506 2012 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2506 2013 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2506 2013 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/#mixed-offsets","title":"Mixed offsets","text":"<p>If your data contains datetimes with mixed UTC offsets (for example due to daylight-saving transitions), Polars parses them in UTC. You can either pass a target <code>time_zone</code> to <code>str.to_datetime</code>, or call <code>str.convert_time_zone</code> after parsing:</p>  Python Rust <p> <code>str.to_datetime</code> \u00b7 <code>dt.convert_time_zone</code> \u00b7  Available on feature timezone <pre><code>data = [\n    \"2021-03-27T00:00:00+0100\",\n    \"2021-03-28T00:00:00+0100\",\n    \"2021-03-29T00:00:00+0200\",\n    \"2021-03-30T00:00:00+0200\",\n]\nmixed_parsed = (\n    pl.Series(data)\n    .str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\")\n    .dt.convert_time_zone(\"Europe/Brussels\")\n)\nprint(mixed_parsed)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7 <code>dt.convert_time_zone</code> \u00b7  Available on feature dtype-datetime \u00b7  Available on feature timezones <pre><code>let data = [\n    \"2021-03-27T00:00:00+0100\",\n    \"2021-03-28T00:00:00+0100\",\n    \"2021-03-29T00:00:00+0200\",\n    \"2021-03-30T00:00:00+0200\",\n];\nlet q = col(\"date\")\n    .str()\n    .to_datetime(\n        Some(TimeUnit::Microseconds),\n        None,\n        StrptimeOptions {\n            format: Some(\"%Y-%m-%dT%H:%M:%S%z\".into()),\n            ..Default::default()\n        },\n        lit(\"raise\"),\n    )\n    .dt()\n    .convert_time_zone(\n        TimeZone::opt_try_new(Some(\"Europe/Brussels\"))\n            .unwrap()\n            .unwrap(),\n    );\nlet mixed_parsed = df!(\"date\" =&gt; &amp;data)?.lazy().select([q]).collect()?;\n\nprintln!(\"{}\", &amp;mixed_parsed);\n</code></pre></p> <pre><code>shape: (4,)\nSeries: '' [datetime[\u03bcs, Europe/Brussels]]\n[\n    2021-03-27 00:00:00 CET\n    2021-03-28 00:00:00 CET\n    2021-03-29 00:00:00 CEST\n    2021-03-30 00:00:00 CEST\n]\n</code></pre>"},{"location":"user-guide/transformations/time-series/resampling/","title":"Resampling","text":"<p>We can resample by either:</p> <ul> <li>upsampling (moving data to a higher frequency)</li> <li>downsampling (moving data to a lower frequency)</li> <li>combinations of these e.g. first upsample and then downsample</li> </ul>"},{"location":"user-guide/transformations/time-series/resampling/#downsampling-to-a-lower-frequency","title":"Downsampling to a lower frequency","text":"<p>Polars views downsampling as a special case of the group_by operation and you can do this with <code>group_by_dynamic</code> and <code>group_by_rolling</code> - see the temporal group by page for examples.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-to-a-higher-frequency","title":"Upsampling to a higher frequency","text":"<p>Let's go through an example where we generate data at 30 minute intervals:</p>  Python Rust <p> <code>DataFrame</code> \u00b7 <code>datetime_range</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.datetime_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n        \"values\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> \u00b7 <code>datetime_range</code> \u00b7  Available on feature lazy \u00b7  Available on feature dtype-datetime <pre><code>let time = polars::time::date_range(\n    \"time\".into(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(3, 0, 0)\n        .unwrap(),\n    Duration::parse(\"30m\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\nlet df = df!(\n    \"time\" =&gt; time,\n    \"groups\" =&gt; &amp;[\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n    \"values\" =&gt; &amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2506 4.0    \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Upsampling can be done by defining the new sampling interval. By upsampling we are adding in extra rows where we do not have data. As such upsampling by itself gives a DataFrame with nulls. These nulls can then be filled with a fill strategy or interpolation.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-strategies","title":"Upsampling strategies","text":"<p>In this example we upsample from the original 30 minutes to 15 minutes and then use a <code>forward</code> strategy to replace the nulls with the previous non-null value:</p>  Python Rust <p> <code>upsample</code> <pre><code>out1 = df.upsample(time_column=\"time\", every=\"15m\").fill_null(strategy=\"forward\")\nprint(out1)\n</code></pre></p> <p> <code>upsample</code> <pre><code>let out1 = df\n    .upsample::&lt;[String; 0]&gt;([], \"time\", Duration::parse(\"15m\"))?\n    .fill_null(FillNullStrategy::Forward(None))?;\nprintln!(\"{}\", &amp;out1);\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example we instead fill the nulls by linear interpolation:</p>  Python Rust <p> <code>upsample</code> \u00b7 <code>interpolate</code> \u00b7 <code>fill_null</code> <pre><code>out2 = (\n    df.upsample(time_column=\"time\", every=\"15m\")\n    .interpolate()\n    .fill_null(strategy=\"forward\")\n)\nprint(out2)\n</code></pre></p> <p> <code>upsample</code> \u00b7 <code>interpolate</code> \u00b7 <code>fill_null</code> <pre><code>let out2 = df\n    .upsample::&lt;[String; 0]&gt;([], \"time\", Duration::parse(\"15m\"))?\n    .lazy()\n    .with_columns([col(\"values\").interpolate(InterpolationMethod::Linear)])\n    .collect()?\n    .fill_null(FillNullStrategy::Forward(None))?;\nprintln!(\"{}\", &amp;out2);\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.5    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.5    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.5    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.5    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/","title":"Grouping","text":""},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-fixed-windows","title":"Grouping by fixed windows","text":"<p>We can calculate temporal statistics using <code>group_by_dynamic</code> to group rows into days/months/years etc.</p>"},{"location":"user-guide/transformations/time-series/rolling/#annual-average-example","title":"Annual average example","text":"<p>In following simple example we calculate the annual average closing price of Apple stock prices. We first load the data from CSV:</p>  Python Rust <p> <code>upsample</code> <pre><code>df = pl.read_csv(\"docs/assets/data/apple_stock.csv\", try_parse_dates=True)\ndf = df.sort(\"Date\")\nprint(df)\n</code></pre></p> <p> <code>upsample</code> <pre><code>let df = CsvReadOptions::default()\n    .map_parse_options(|parse_options| parse_options.with_try_parse_dates(true))\n    .try_into_reader_with_file_path(Some(\"docs/assets/data/apple_stock.csv\".into()))\n    .unwrap()\n    .finish()\n    .unwrap()\n    .sort(\n        [\"Date\"],\n        SortMultipleOptions::default().with_maintain_order(true),\n    )?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>The dates are sorted in ascending order - if they are not sorted in this way the <code>group_by_dynamic</code> output will not be correct!</p> <p>To get the annual average closing price we tell <code>group_by_dynamic</code> that we want to:</p> <ul> <li>group by the <code>Date</code> column on an annual (<code>1y</code>) basis</li> <li>take the mean values of the <code>Close</code> column for each year:</li> </ul>  Python Rust <p> <code>group_by_dynamic</code> <pre><code>annual_average_df = df.group_by_dynamic(\"Date\", every=\"1y\").agg(pl.col(\"Close\").mean())\n\ndf_with_year = annual_average_df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7  Available on feature dynamic_group_by <pre><code>let annual_average_df = df\n    .lazy()\n    .group_by_dynamic(\n        col(\"Date\"),\n        [],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1y\"),\n            period: Duration::parse(\"1y\"),\n            offset: Duration::parse(\"0\"),\n            ..Default::default()\n        },\n    )\n    .agg([col(\"Close\").mean()])\n    .collect()?;\n\nlet df_with_year = annual_average_df\n    .lazy()\n    .with_columns([col(\"Date\").dt().year().alias(\"year\")])\n    .collect()?;\nprintln!(\"{}\", &amp;df_with_year);\n</code></pre></p> <p>The annual average closing price is then:</p> <pre><code>shape: (34, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close     \u2506 year \u2502\n\u2502 ---        \u2506 ---       \u2506 ---  \u2502\n\u2502 date       \u2506 f64       \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-01-01 \u2506 23.5625   \u2506 1981 \u2502\n\u2502 1982-01-01 \u2506 11.0      \u2506 1982 \u2502\n\u2502 1983-01-01 \u2506 30.543333 \u2506 1983 \u2502\n\u2502 1984-01-01 \u2506 27.583333 \u2506 1984 \u2502\n\u2502 1985-01-01 \u2506 18.166667 \u2506 1985 \u2502\n\u2502 \u2026          \u2506 \u2026         \u2506 \u2026    \u2502\n\u2502 2010-01-01 \u2506 278.265   \u2506 2010 \u2502\n\u2502 2011-01-01 \u2506 368.225   \u2506 2011 \u2502\n\u2502 2012-01-01 \u2506 560.965   \u2506 2012 \u2502\n\u2502 2013-01-01 \u2506 464.955   \u2506 2013 \u2502\n\u2502 2014-01-01 \u2506 522.06    \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#parameters-for-group_by_dynamic","title":"Parameters for <code>group_by_dynamic</code>","text":"<p>A dynamic window is defined by a:</p> <ul> <li>every: indicates the interval of the window</li> <li>period: indicates the duration of the window</li> <li>offset: can be used to offset the start of the windows</li> </ul> <p>The value for <code>every</code> sets how often the groups start. The time period values are flexible - for example we could take:</p> <ul> <li>the average over 2 year intervals by replacing <code>1y</code> with <code>2y</code></li> <li>the average over 18 month periods by replacing <code>1y</code> with <code>1y6mo</code></li> </ul> <p>We can also use the <code>period</code> parameter to set how long the time period for each group is. For example, if we set the <code>every</code> parameter to be <code>1y</code> and the <code>period</code> parameter to be <code>2y</code> then we would get groups at one year intervals where each groups spanned two years.</p> <p>If the <code>period</code> parameter is not specified then it is set equal to the <code>every</code> parameter so that if the <code>every</code> parameter is set to be <code>1y</code> then each group spans <code>1y</code> as well.</p> <p>Because every does not have to be equal to period, we can create many groups in a very flexible way. They may overlap or leave boundaries between them.</p> <p>Let's see how the windows for some parameter combinations would look. Let's start out boring. \ud83e\udd71</p> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this creates adjacent windows of the same size\n|--|\n   |--|\n      |--|\n</code></pre> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 2 days -&gt; <code>\"2d\"</code></li> </ul> <pre><code>these windows have an overlap of 1 day\n|----|\n   |----|\n      |----|\n</code></pre> <ul> <li>every: 2 days -&gt; <code>\"2d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this would leave gaps between the windows\ndata points that in these gaps will not be a member of any group\n|--|\n       |--|\n              |--|\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#truncate","title":"<code>truncate</code>","text":"<p>The <code>truncate</code> parameter is a Boolean variable that determines what datetime value is associated with each group in the output. In the example above the first data point is on 23rd February 1981. If <code>truncate = True</code> (the default) then the date for the first year in the annual average is 1st January 1981. However, if <code>truncate = False</code> then the date for the first year in the annual average is the date of the first data point on 23rd February 1981. Note that <code>truncate</code> only affects what's shown in the <code>Date</code> column and does not affect the window boundaries.</p>"},{"location":"user-guide/transformations/time-series/rolling/#using-expressions-in-group_by_dynamic","title":"Using expressions in <code>group_by_dynamic</code>","text":"<p>We aren't restricted to using simple aggregations like <code>mean</code> in a group by operation - we can use the full range of expressions available in Polars.</p> <p>In the snippet below we create a <code>date range</code> with every day (<code>\"1d\"</code>) in 2021 and turn this into a <code>DataFrame</code>.</p> <p>Then in the <code>group_by_dynamic</code> we create dynamic windows that start every month (<code>\"1mo\"</code>) and have a window length of <code>1</code> month. The values that match these dynamic windows are then assigned to that group and can be aggregated with the powerful expression API.</p> <p>Below we show an example where we use group_by_dynamic to compute:</p> <ul> <li>the number of days until the end of the month</li> <li>the number of days in a month</li> </ul>  Python Rust <p> <code>group_by_dynamic</code> \u00b7 <code>DataFrame.explode</code> \u00b7 <code>date_range</code> <pre><code>df = (\n    pl.date_range(\n        start=date(2021, 1, 1),\n        end=date(2021, 12, 31),\n        interval=\"1d\",\n        eager=True,\n    )\n    .alias(\"time\")\n    .to_frame()\n)\n\nout = df.group_by_dynamic(\"time\", every=\"1mo\", period=\"1mo\", closed=\"left\").agg(\n    pl.col(\"time\").cum_count().reverse().head(3).alias(\"day/eom\"),\n    ((pl.col(\"time\") - pl.col(\"time\").first()).last().dt.total_days() + 1).alias(\n        \"days_in_month\"\n    ),\n)\nprint(out)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7 <code>DataFrame.explode</code> \u00b7 <code>date_range</code> \u00b7  Available on feature dtype-date \u00b7  Available on feature range \u00b7  Available on feature dynamic_group_by <pre><code>let time = polars::time::date_range(\n    \"time\".into(),\n    NaiveDate::from_ymd_opt(2021, 1, 1)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 31)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    Duration::parse(\"1d\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?\n.cast(&amp;DataType::Date)?;\n\nlet df = df!(\n    \"time\" =&gt; time,\n)?;\n\nlet out = df\n    .lazy()\n    .group_by_dynamic(\n        col(\"time\"),\n        [],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1mo\"),\n            period: Duration::parse(\"1mo\"),\n            offset: Duration::parse(\"0\"),\n            closed_window: ClosedWindow::Left,\n            ..Default::default()\n        },\n    )\n    .agg([\n        col(\"time\")\n            .cum_count(true) // python example has false\n            .reverse()\n            .head(Some(3))\n            .alias(\"day/eom\"),\n        ((col(\"time\").last() - col(\"time\").first()).map(\n            // had to use map as .duration().days() is not available\n            |s| {\n                Ok(s.duration()?\n                    .physical()\n                    .into_iter()\n                    .map(|d| d.map(|v| v / 1000 / 24 / 60 / 60))\n                    .collect::&lt;Int64Chunked&gt;()\n                    .into_column())\n            },\n            |_, f| Ok(Field::new(f.name().clone(), DataType::Int64)),\n        ) + lit(1))\n        .alias(\"days_in_month\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (12, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time       \u2506 day/eom      \u2506 days_in_month \u2502\n\u2502 ---        \u2506 ---          \u2506 ---           \u2502\n\u2502 date       \u2506 list[u32]    \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-02-01 \u2506 [28, 27, 26] \u2506 28            \u2502\n\u2502 2021-03-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-04-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-05-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 \u2026          \u2506 \u2026            \u2506 \u2026             \u2502\n\u2502 2021-08-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-09-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-10-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-11-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-12-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-rolling-windows","title":"Grouping by rolling windows","text":"<p>The rolling operation, <code>rolling</code>, is another entrance to the <code>group_by</code>/<code>agg</code> context. But different from the <code>group_by_dynamic</code> where the windows are fixed by a parameter <code>every</code> and <code>period</code>. In a <code>rolling</code>, the windows are not fixed at all! They are determined by the values in the <code>index_column</code>.</p> <p>So imagine having a time column with the values <code>{2021-01-06, 2021-01-10}</code> and a <code>period=\"5d\"</code> this would create the following windows:</p> <pre><code>2021-01-01   2021-01-06\n    |----------|\n\n       2021-01-05   2021-01-10\n             |----------|\n</code></pre> <p>Because the windows of a rolling group by are always determined by the values in the <code>DataFrame</code> column, the number of groups is always equal to the original <code>DataFrame</code>.</p>"},{"location":"user-guide/transformations/time-series/rolling/#combining-group-by-operations","title":"Combining group by operations","text":"<p>Rolling and dynamic group by operations can be combined with normal group by operations.</p> <p>Below is an example with a dynamic group by.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.datetime_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let time = polars::time::date_range(\n    \"time\".into(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(3, 0, 0)\n        .unwrap(),\n    Duration::parse(\"30m\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\nlet df = df!(\n    \"time\" =&gt; time,\n    \"groups\"=&gt; [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (7, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2502\n\u2502 ---                 \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>group_by_dynamic</code> <pre><code>out = df.group_by_dynamic(\n    \"time\",\n    every=\"1h\",\n    closed=\"both\",\n    group_by=\"groups\",\n    include_boundaries=True,\n).agg(pl.len())\nprint(out)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7  Available on feature dynamic_group_by <pre><code>let out = df\n    .lazy()\n    .group_by_dynamic(\n        col(\"time\"),\n        [col(\"groups\")],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1h\"),\n            period: Duration::parse(\"1h\"),\n            offset: Duration::parse(\"0\"),\n            include_boundaries: true,\n            closed_window: ClosedWindow::Both,\n            ..Default::default()\n        },\n    )\n    .agg([len()])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (6, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 groups \u2506 _lower_boundary     \u2506 _upper_boundary     \u2506 time                \u2506 len \u2502\n\u2502 ---    \u2506 ---                 \u2506 ---                 \u2506 ---                 \u2506 --- \u2502\n\u2502 str    \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a      \u2506 2021-12-16 00:00:00 \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 00:00:00 \u2506 3   \u2502\n\u2502 a      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 1   \u2502\n\u2502 a      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 2   \u2502\n\u2502 a      \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 04:00:00 \u2506 2021-12-16 03:00:00 \u2506 1   \u2502\n\u2502 b      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 2   \u2502\n\u2502 b      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/timezones/","title":"Time zones","text":"<p>Tom Scott</p> <p>You really should never, ever deal with time zones if you can help it.</p> <p>The <code>Datetime</code> datatype can have a time zone associated with it. Examples of valid time zones are:</p> <ul> <li><code>None</code>: no time zone, also known as \"time zone naive\".</li> <li><code>UTC</code>: Coordinated Universal Time.</li> <li><code>Asia/Kathmandu</code>: time zone in \"area/location\" format. See the   list of tz database time zones to   see what's available.</li> </ul> <p>Caution: Fixed offsets such as +02:00, should not be used for handling time zones. It's advised to use the \"Area/Location\" format mentioned above, as it can manage timezones more effectively.</p> <p>Note that, because a <code>Datetime</code> can only have a single time zone, it is impossible to have a column with multiple time zones. If you are parsing data with multiple offsets, you may want to pass <code>utc=True</code> to convert them all to a common time zone (<code>UTC</code>), see parsing dates and times.</p> <p>The main methods for setting and converting between time zones are:</p> <ul> <li><code>dt.convert_time_zone</code>: convert from one time zone to another.</li> <li><code>dt.replace_time_zone</code>: set/unset/change time zone.</li> </ul> <p>Let's look at some examples of common operations:</p>  Python Rust <p> <code>str.to_datetime</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>ts = [\"2021-03-27 03:00\", \"2021-03-28 03:00\"]\ntz_naive = pl.Series(\"tz_naive\", ts).str.to_datetime()\ntz_aware = tz_naive.dt.replace_time_zone(\"UTC\").rename(\"tz_aware\")\ntime_zones_df = pl.DataFrame([tz_naive, tz_aware])\nprint(time_zones_df)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature dtype-datetime \u00b7  Available on feature timezones <pre><code>let ts = [\"2021-03-27 03:00\", \"2021-03-28 03:00\"];\nlet tz_naive = Column::new(\"tz_naive\".into(), &amp;ts);\nlet time_zones_df = DataFrame::new_infer_height(vec![tz_naive])?\n    .lazy()\n    .select([col(\"tz_naive\").str().to_datetime(\n        Some(TimeUnit::Milliseconds),\n        None,\n        StrptimeOptions::default(),\n        lit(\"raise\"),\n    )])\n    .with_columns([col(\"tz_naive\")\n        .dt()\n        .replace_time_zone(Some(TimeZone::UTC), lit(\"raise\"), NonExistent::Raise)\n        .alias(\"tz_aware\")])\n    .collect()?;\n\nprintln!(\"{}\", &amp;time_zones_df);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tz_naive            \u2506 tz_aware                \u2502\n\u2502 ---                 \u2506 ---                     \u2502\n\u2502 datetime[\u03bcs]        \u2506 datetime[\u03bcs, UTC]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 \u2506 2021-03-27 03:00:00 UTC \u2502\n\u2502 2021-03-28 03:00:00 \u2506 2021-03-28 03:00:00 UTC \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>dt.convert_time_zone</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>time_zones_operations = time_zones_df.select(\n    [\n        pl.col(\"tz_aware\")\n        .dt.replace_time_zone(\"Europe/Brussels\")\n        .alias(\"replace time zone\"),\n        pl.col(\"tz_aware\")\n        .dt.convert_time_zone(\"Asia/Kathmandu\")\n        .alias(\"convert time zone\"),\n        pl.col(\"tz_aware\").dt.replace_time_zone(None).alias(\"unset time zone\"),\n    ]\n)\nprint(time_zones_operations)\n</code></pre></p> <p> <code>dt.convert_time_zone</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezones <pre><code>let time_zones_operations = time_zones_df\n    .lazy()\n    .select([\n        col(\"tz_aware\")\n            .dt()\n            .replace_time_zone(\n                TimeZone::opt_try_new(Some(\"Europe/Brussels\")).unwrap(),\n                lit(\"raise\"),\n                NonExistent::Raise,\n            )\n            .alias(\"replace time zone\"),\n        col(\"tz_aware\")\n            .dt()\n            .convert_time_zone(\n                TimeZone::opt_try_new(Some(\"Asia/Kathmandu\"))\n                    .unwrap()\n                    .unwrap(),\n            )\n            .alias(\"convert time zone\"),\n        col(\"tz_aware\")\n            .dt()\n            .replace_time_zone(None, lit(\"raise\"), NonExistent::Raise)\n            .alias(\"unset time zone\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;time_zones_operations);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 replace time zone             \u2506 convert time zone            \u2506 unset time zone     \u2502\n\u2502 ---                           \u2506 ---                          \u2506 ---                 \u2502\n\u2502 datetime[\u03bcs, Europe/Brussels] \u2506 datetime[\u03bcs, Asia/Kathmandu] \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 CET       \u2506 2021-03-27 08:45:00 +0545    \u2506 2021-03-27 03:00:00 \u2502\n\u2502 2021-03-28 03:00:00 CEST      \u2506 2021-03-28 08:45:00 +0545    \u2506 2021-03-28 03:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"}]}