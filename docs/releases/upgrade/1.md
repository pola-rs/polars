# Version 1

!!! warning "Work in progress"

    This upgrade guide is not yet complete. Check back when 1.0.0 is released for the full overview of breaking changes.

## Breaking changes

### Preserve nulls in `ewm_mean`, `ewm_std`, and `ewm_var`

Polars will no longer forward-fill null values in `ewm` methods.
The user can call `.forward_fill()` on the output to achieve the same result.

**Example**

Before:

```pycon
>>> s = pl.Series([1, 4, None, 3])
>>> s.ewm_mean(alpha=.9, ignore_nulls=False)
shape: (4,)
Series: '' [f64]
[
        1.0
        3.727273
        3.727273
        3.007913
]
```

After:

```pycon
>>> s.ewm_mean(alpha=.9, ignore_nulls=False)
shape: (4,)
Series: '' [f64]
[
        1.0
        3.727273
        null
        3.007913
]
```

### Update `clip` to no longer propagate nulls in the given bounds

Null values in the bounds no longer set the value to null - instead, the original value is retained.

**Before**

```pycon
>>> df = pl.DataFrame({"a": [0, 1, 2], "min": [1, None, 1]})
>>> df.select(pl.col("a").clip("min"))
shape: (3, 1)
┌──────┐
│ a    │
│ ---  │
│ i64  │
╞══════╡
│ 1    │
│ null │
│ 2    │
└──────┘
```

**After**

```pycon
>>> df.select(pl.col("a").clip("min"))
shape: (3, 1)
┌──────┐
│ a    │
│ ---  │
│ i64  │
╞══════╡
│ 1    │
│ 1    │
│ 2    │
└──────┘
```

### Read 2D NumPy arrays as `Array` type instead of `List`

**Example**

Before:

```pycon
>>> import numpy as np
>>> arr = np.array([[1, 2], [3, 4]])
>>> pl.Series(arr)
shape: (2,)
Series: '' [list[i64]]
[
        [1, 2]
        [3, 4]
]
```

After:

```pycon
>>> import numpy as np
>>> arr = np.array([[1, 2], [3, 4]])
>>> pl.Series(arr)
shape: (2,)
Series: '' [array[i64, 2]]
[
        [1, 2]
        [3, 4]
]
```

### Change `str.to_datetime` to default to microsecond precision for format specifiers `"%f"` and `"%.f"`

In `.str.to_datetime`, when specifying `%.f` as the format, the default was to set the resulting datatype to nanosecond precision. This has been changed to microsecond precision.

#### Example

**Before**

```pycon
>>> s = pl.Series(["2022-08-31 00:00:00.123456789"])
>>> s.str.to_datetime(format="%Y-%m-%d %H:%M:%S%.f")
shape: (1,)
Series: '' [datetime[ns]]
[
        2022-08-31 00:00:00.123456789
]
```

**After**

```pycon
>>> s.str.to_datetime(format="%Y-%m-%d %H:%M:%S%.f")
shape: (1,)
Series: '' [datetime[us]]
[
        2022-08-31 00:00:00.123456
]
```

### Update resulting column names in `pivot` when pivoting by multiple values

In `DataFrame.pivot`, when specifying multiple `values` columns, the result would redundantly include the `column` column in the column names.
This has been addressed.

**Example**

Before:

```python
>>> df = pl.DataFrame(
...     {
...         "name": ["Cady", "Cady", "Karen", "Karen"],
...         "subject": ["maths", "physics", "maths", "physics"],
...         "test_1": [98, 99, 61, 58],
...         "test_2": [100, 100, 60, 60],
...     }
... )
>>> df.pivot(index='name', columns='subject', values=['test_1', 'test_2'])
shape: (2, 5)
┌───────┬──────────────────────┬────────────────────────┬──────────────────────┬────────────────────────┐
│ name  ┆ test_1_subject_maths ┆ test_1_subject_physics ┆ test_2_subject_maths ┆ test_2_subject_physics │
│ ---   ┆ ---                  ┆ ---                    ┆ ---                  ┆ ---                    │
│ str   ┆ i64                  ┆ i64                    ┆ i64                  ┆ i64                    │
╞═══════╪══════════════════════╪════════════════════════╪══════════════════════╪════════════════════════╡
│ Cady  ┆ 98                   ┆ 99                     ┆ 100                  ┆ 100                    │
│ Karen ┆ 61                   ┆ 58                     ┆ 60                   ┆ 60                     │
└───────┴──────────────────────┴────────────────────────┴──────────────────────┴────────────────────────┘
```

After:

```python
>>> df = pl.DataFrame(
...     {
...         "name": ["Cady", "Cady", "Karen", "Karen"],
...         "subject": ["maths", "physics", "maths", "physics"],
...         "test_1": [98, 99, 61, 58],
...         "test_2": [100, 100, 60, 60],
...     }
... )
>>> df.pivot(index='name', columns='subject', values=['test_1', 'test_2'])
┌───────┬──────────────┬────────────────┬──────────────┬────────────────┐
│ name  ┆ test_1_maths ┆ test_1_physics ┆ test_2_maths ┆ test_2_physics │
│ ---   ┆ ---          ┆ ---            ┆ ---          ┆ ---            │
│ str   ┆ i64          ┆ i64            ┆ i64          ┆ i64            │
╞═══════╪══════════════╪════════════════╪══════════════╪════════════════╡
│ Cady  ┆ 98           ┆ 99             ┆ 100          ┆ 100            │
│ Karen ┆ 61           ┆ 58             ┆ 60           ┆ 60             │
└───────┴──────────────┴────────────────┴──────────────┴────────────────┘
```

### Support Decimal types by default when converting from Arrow

Update conversion from Arrow to always convert Decimals into Polars Decimals, rather than cast to Float64.
`Config.activate_decimals` has been removed.

**Example**

Before:

```pycon
>>> from decimal import Decimal as D
>>> import pyarrow as pa
>>> arr = pa.array([D("1.01"), D("2.25")])
>>> pl.from_arrow(arr)
shape: (2,)
Series: '' [f64]
[
        1.01
        2.25
]
```

After:

```pycon
>>> pl.from_arrow(arr)
shape: (2,)
Series: '' [decimal[3,2]]
[
        1.01
        2.25
]
```

### Remove serde functionality from `pl.read_json` and `DataFrame.write_json`

`pl.read_json` no longer supports reading JSON files produced by `DataFrame.serialize`.
Users should use `pl.DataFrame.deserialize` instead.

`DataFrame.write_json` now only writes row-oriented JSON. The parameters `row_oriented` and `pretty` have been removed.
Users should use `DataFrame.serialize` to serialize a DataFrame.

**Example - `write_json`**

Before:

```pycon
>>> df = pl.DataFrame({"a": [1, 2], "b": [3.0, 4.0]})
>>> df.write_json()
'{"columns":[{"name":"a","datatype":"Int64","bit_settings":"","values":[1,2]},{"name":"b","datatype":"Float64","bit_settings":"","values":[3.0,4.0]}]}'
```

After:

```pycon
>>> df.write_json()  # Same behavior as previously `df.write_json(row_oriented=True)`
'[{"a":1,"b":3.0},{"a":2,"b":4.0}]'
```

**Example - `read_json`**

Before:

```pycon
>>> import io
>>> df_ser = '{"columns":[{"name":"a","datatype":"Int64","bit_settings":"","values":[1,2]},{"name":"b","datatype":"Float64","bit_settings":"","values":[3.0,4.0]}]}'
>>> pl.read_json(io.StringIO(df_ser))
shape: (2, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ f64 │
╞═════╪═════╡
│ 1   ┆ 3.0 │
│ 2   ┆ 4.0 │
└─────┴─────┘
```

After:

```pycon
>>> pl.read_json(io.StringIO(df_ser))  # Format no longer supported: data is treated as a single row
shape: (1, 1)
┌─────────────────────────────────┐
│ columns                         │
│ ---                             │
│ list[struct[4]]                 │
╞═════════════════════════════════╡
│ [{"a","Int64","",[1.0, 2.0]}, … │
└─────────────────────────────────┘
```

Use instead:

```pycon
>>> pl.DataFrame.deserialize(io.StringIO(df_ser))
shape: (2, 2)
┌─────┬─────┐
│ a   ┆ b   │
│ --- ┆ --- │
│ i64 ┆ f64 │
╞═════╪═════╡
│ 1   ┆ 3.0 │
│ 2   ┆ 4.0 │
└─────┴─────┘
```

### `Series.equals` no longer checks names by default

Previously, `Series.equals` would return `False` if the Series names didn't match.
The method now no longer checks the names by default.
The previous behavior can be retained by setting `check_names=True`.

**Example**

Before:

```pycon
>>> s1 = pl.Series("foo", [1, 2, 3])
>>> s2 = pl.Series("bar", [1, 2, 3])
>>> s1.equals(s2)
False
```

After:

```pycon
>>> s1.equals(s2)
True
>>> s1.equals(s2, check_names=True)
False
```

### Remove `columns` parameter from `nth` expression function

The `columns` parameter was removed in favor of treating positional inputs as additional indices.
Use `Expr.get` instead to get the same functionality.

**Example**

Before:

```pycon
>>> df = pl.DataFrame({"a": [1, 2], "b": [3, 4], "c": [5, 6]})
>>> df.select(pl.nth(1, "a"))
shape: (1, 1)
┌─────┐
│ a   │
│ --- │
│ i64 │
╞═════╡
│ 2   │
└─────┘
```

After:

```pycon
>>> df.select(pl.nth(1, "a"))
...
TypeError: argument 'indices': 'str' object cannot be interpreted as an integer
```

Use instead:

```pycon
>>> df.select(pl.col("a").get(1))
shape: (1, 1)
┌─────┐
│ a   │
│ --- │
│ i64 │
╞═════╡
│ 2   │
└─────┘
```

### Rename struct fields of `rle` output

The struct fields of the `rle` method have been renamed from `lengths/values` to `len/value`.
The data type of the `len` field has also been updated to match the index type (was previously `Int32`, now `UInt32`).

**Before**

```pycon
>>> s = pl.Series(["a", "a", "b", "c", "c", "c"])
>>> s.rle().struct.unnest()
shape: (3, 2)
┌─────────┬────────┐
│ lengths ┆ values │
│ ---     ┆ ---    │
│ i32     ┆ str    │
╞═════════╪════════╡
│ 2       ┆ a      │
│ 1       ┆ b      │
│ 3       ┆ c      │
└─────────┴────────┘
```

**After**

```pycon
>>> s.rle().struct.unnest()
shape: (3, 2)
┌─────┬───────┐
│ len ┆ value │
│ --- ┆ ---   │
│ u32 ┆ str   │
╞═════╪═══════╡
│ 2   ┆ a     │
│ 1   ┆ b     │
│ 3   ┆ c     │
└─────┴───────┘
```

### Remove class variables from some DataTypes

Some DataType classes had class variables.
The `Datetime` class, for example, had `time_unit` and `time_zone` as class variables.
This was unintended: these should have been instance variables.
This has now been corrected.

**Example**

Before:

```pycon
>>> dtype = pl.Datetime
>>> dtype.time_unit is None
True
```

After:

```pycon
>>> dtype.time_unit is None
Traceback (most recent call last):
...
AttributeError: type object 'Datetime' has no attribute 'time_unit'
```

Use instead:

```pycon
>>> getattr(dtype, "time_unit", None) is None
True
```

### Change default `offset` in `group_by_dynamic` from 'negative `every`' to 'zero'

### Constrain access to globals from `DataFrame.sql` in favor of `pl.sql`

The `sql` methods on `DataFrame` and `LazyFrame` can no longer access global variables.
These methods should be used for operating on the frame itself.
For global access, there is now the top-level `sql` function.

**Example**

Before:

```pycon
>>> df1 = pl.DataFrame({"id1": [1, 2]})
>>> df2 = pl.DataFrame({"id2": [3, 4]})
>>> df1.sql("SELECT * FROM df1 CROSS JOIN df2")
shape: (4, 2)
┌─────┬─────┐
│ id1 ┆ id2 │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 1   ┆ 3   │
│ 1   ┆ 4   │
│ 2   ┆ 3   │
│ 2   ┆ 4   │
└─────┴─────┘
```

After:

```pycon
>>> df1.sql("SELECT * FROM df1 CROSS JOIN df2")
Traceback (most recent call last):
...
polars.exceptions.SQLInterfaceError: relation 'df1' was not found
```

Use instead:

```pycon
>>> pl.sql("SELECT * FROM df1 CROSS JOIN df2", eager=True)
shape: (4, 2)
┌─────┬─────┐
│ id1 ┆ id2 │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 1   ┆ 3   │
│ 1   ┆ 4   │
│ 2   ┆ 3   │
│ 2   ┆ 4   │
└─────┴─────┘
```
