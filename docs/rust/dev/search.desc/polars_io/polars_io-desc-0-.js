searchState.loadedDescShard("polars_io", 0, "Options for Hive partitioning.\nInterface with cloud storage through the object_store …\nFunctionality for reading and writing CSV files.\nTake the SerReader and return a parsed DataFrame.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\n(De)serialize JSON files.\nCreate a new instance of the <code>[SerReader]</code>\nFunctionality for reading and writing Apache Parquet files.\nFunctionality for writing a DataFrame partitioned into …\nValid compressions\nRead Apache Avro format into a <code>DataFrame</code>\nWrite a <code>DataFrame</code> to Apache Avro format\nValid compressions\nDeflate\nDeflate\nSnappy\nSnappy\nGet arrow schema of the avro File, this is faster than a …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet schema of the Avro File\nColumns to select/ project\nSet the compression used. Defaults to None.\nStop reading when <code>n</code> rows are read.\nSet the reader’s column projection. This counts from 0, …\nA location on cloud storage, may have wildcards.\nAdaptor which wraps the asynchronous interface of …\nPolars specific wrapper for <code>Arc&lt;dyn ObjectStore&gt;</code> that …\nThe bucket name.\nBuild an <code>ObjectStore</code> based on the URL and passed in url. …\nExecutes the given command directly.\nThe path components that need to be expanded.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nList files with a prefix derived from the pattern.\nFetch the metadata of the parquet file, do not memoize it.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstructs a new CloudWriter from a path and an optional …\nParse a CloudLocation from an url.\nConstruct a new CloudWriter, re-using the given …\nThe prefix inside the bucket, this will be the full key …\nQueues the given command for further execution.\nThe scheme (s3, …).\nPerforms a set of actions within a synchronous update.\nMaster key for accessing storage account\nAWS Access Key\nThe name of the azure storage account\nConfiguration keys for <code>AmazonS3Builder</code>\nApplication credentials path\nTenant id used in oauth flows\nConfiguration keys for <code>MicrosoftAzureBuilder</code>\nBucket name\nBucket name\nSet the checksum algorithm for this client\nClient options\nClient options\nClient options\nService principal client id for authorizing requests\nService principal client secret for authorizing requests\nOptions to connect to various cloud providers.\nConfigure how to provide conditional put operations\nSet the container credentials relative URI\nContainer name\nConfigure how to provide <code>copy_if_not_exists</code>\nDefault region\nDisable tagging objects\nDisables tagging objects\nEncryption options\nSets custom endpoint for communicating with AWS S3.\nOverride the endpoint used to communicate with blob storage\nFile containing token for Azure AD workload identity …\nConfiguration keys for <code>GoogleCloudStorageBuilder</code>\nFall back to ImdsV1\nSet the instance metadata endpoint\nEndpoint to request a imds managed identity token\nMsi resource id for use with managed identity …\nObject id for use with managed identity authentication\nRegion\nEnable Support for S3 Express One Zone\nShared access signature.\nSecret Access Key\nPath to the service account file\nThe serialized service account key.\nSkip signing request\nSkip signing requests\nToken to use for requests (passed to underlying provider)\nBearer token\nAvoid computing payload checksum when calculating …\nUse azure cli for acquiring access token\nUse object store with azurite storage emulator\nUse object store with url scheme …\nIf virtual hosted style request has to be used\nBuild the <code>object_store::ObjectStore</code> implementation for AWS.\nBuild the <code>object_store::ObjectStore</code> implementation for …\nBuild the <code>object_store::ObjectStore</code> implementation for GCP.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nParse a configuration from a Hashmap. This is the …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet the configuration for AWS connections. This is the …\nSet the configuration for Azure connections. This is the …\nSet the configuration for GCP connections. This is the …\nFunctionality for reading CSV files.\nFunctionality for writing CSV files.\nMultiple values that are used for all columns\nA single value that’s used for all columns\nCreate a new DataFrame by reading a csv file.\nUtf8 encoding and unknown bytes are replaced with �.\nA string that indicates the start of a comment line. This …\nTuples that map column names to null value of that column\nA single byte character that indicates the start of a …\nUtf8 encoding.\nRead the number of rows without parsing columns useful for …\nRead the file and create the DataFrame.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreates a CSV reader using a file handle.\ncheck if csv file is compressed\nApply a function to the parse options.\nCreate a new CsvReader from a file/stream using default …\nCreates a new <code>CommentPrefix</code> from a <code>&amp;str</code>.\nCreates a new <code>CommentPrefix</code> for the <code>Multi</code> variant.\nCreates a new <code>CommentPrefix</code> for the <code>Single</code> variant.\nCreates a CSV reader using a file path.\nNote: This does not update the schema from the inference …\nSets the chunk size used by the parser. This influences …\nWhich columns to select.\nSets the comment prefix for this instance. Lines starting …\nParse floats with a comma as decimal separator.\nOverwrite the dtypes in the schema in the order of the …\nSet the encoding used by the file.\nSet the character used to indicate an end-of-line (eol).\nSets whether the CSV file has a header row.\nContinue with next batch when a ParserError is encountered.\nNumber of rows to use for schema inference. Pass None to …\nReduce memory consumption at the expense of performance\nTreat missing fields as null.\nLimits the number of rows to read.\nNumber of threads to use for reading. Defaults to the size …\nSet values that will be interpreted as missing/null.\nSets the CSV parsing options. See map_parse_options for an …\nWhich columns to select denoted by their index. The index …\nSet the character used for field quoting. This is most …\nWhether to raise an error if the frame is empty. By …\nWhether to makes the columns contiguous in memory.\nAdds a row index column.\nSets the number of rows sampled from the file to determine …\nSet the schema to use for CSV file. The length of the …\nOverwrites the data types in the schema by column name.\nThe character used to separate fields in the CSV file. This\nNumber of rows to skip before the header row.\nNumber of rows to skip after the header row.\nTruncate lines that are longer than the schema.\nAutomatically try to parse dates/datetimes and time. If …\nReturns the argument unchanged.\nInfer the schema of a CSV file by reading through the …\nCalls <code>U::from(self)</code>.\nQuote every field. Always.\nWrite a DataFrame to csv.\nOptions for writing CSV files.\nQuote fields only when necessary.\nNever quote any fields, even if it would produce invalid …\nQuote non-numeric fields.\nQuote style indicating when to insert quotes around a …\nOptions to serialize logical types to CSV.\nUsed for <code>DataType::Date</code>.\nUsed for <code>DataType::Datetime</code>.\nWrites the header of the csv file if not done already. …\nUsed for <code>DataType::Float64</code> and <code>DataType::Float32</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSet whether to write UTF-8 BOM.\nSet whether to write headers.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nString appended after every row.\nNull value representation.\nQuoting character.\nWhen to insert quotes.\nUsed as separator.\nUsed for <code>DataType::Time</code>.\nSet the batch size to use while writing the CSV.\nSet the CSV file’s date format.\nSet the CSV file’s datetime format.\nSet the CSV file’s float precision.\nSet the CSV file’s line terminator.\nSet the CSV file’s null value representation.\nSet the single byte character used for quoting.\nSet the CSV file’s quoting behavior. See more on …\nSet the CSV file’s column separator as a byte character.\nSet the CSV file’s time format.\nWrite a batch to the csv writer.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nDirectly returns the cached file if it finds one without …\nReturns the cached file after ensuring it is up to date …\nCompression codec\nRead Arrows IPC format into a DataFrame\nAn Arrow IPC reader implemented on top of …\nRead Arrows Stream IPC format into a DataFrame\nWrite a DataFrame to Arrow’s Streaming IPC format\nWrite a DataFrame to Arrow’s IPC format\nLZ4 (framed)\nZSTD\nGet arrow schema of the Ipc Stream File, this is faster …\nData page compression\nWrites the footer of the IPC file.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nmaintain the order the data was processed\nSet if the file is to be memory_mapped. Only works with …\nGet arrow schema of the Ipc File.\nGet schema of the Ipc Stream File\nColumns to select/ project\nColumns to select/ project\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the compression used. Defaults to None.\nSet the extension. Defaults to “.ipc”.\nSet the extension. Defaults to “.ipc”.\nStop reading when <code>n</code> rows are read.\nStop reading when <code>n</code> rows are read.\nSet the reader’s column projection. This counts from 0, …\nSet the reader’s column projection. This counts from 0, …\nAdd a row index column.\nAdd a row index column.\nWrite a batch to the parquet writer.\nA single JSON array containing each DataFrame row as an …\nThe format to use to write the DataFrame to JSON: <code>Json</code> (a …\nEach DataFrame row is serialized as a JSON object on a …\nReads JSON in one of the formats in <code>JsonFormat</code> into a …\nWrites a DataFrame to JSON.\nTake the SerReader and return a parsed DataFrame.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSet the JSON reader to infer the schema of the file. …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nmaintain the order the data was processed\nCreate a new <code>JsonWriter</code> writing to <code>buffer</code> with format …\nSet the batch size (number of records to load at one time)\nReturn a <code>null</code> if an error occurs during parsing.\nSet the reader’s column projection: the names of the …\nSet the JSON file’s schema\nOverwrite parts of the inferred schema.\nWrite a batch to the json writer.\nTrait used to get a hold to file handler or to the …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nOpen a file to get write access. This will check if the …\nA <code>StructArray</code> is a nested <code>Array</code> with an optional validity …\nArcs this array into a <code>std::sync::Arc&lt;dyn Array&gt;</code>.\nBoxes this array into a <code>Box&lt;dyn Array&gt;</code>.\nReturns the fields of this <code>StructArray</code>.\nReturns the argument unchanged.\nReturns the argument unchanged.\nThis is the recommended way to create a json reader as …\nReturns the fields the <code>DataType::Struct</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nDeconstructs the <code>StructArray</code> into its individual …\nReturns an iterator of <code>Option&lt;Box&lt;dyn Array&gt;&gt;</code>\nReduce memory consumption at the expense of performance\nCreate a new JsonLineReader from a file/ stream\nReturns a new <code>StructArray</code>\nCreates an empty <code>StructArray</code>.\nCreates a null <code>StructArray</code> of length <code>length</code>.\nSets the validity of this array.\nSlices this <code>StructArray</code>.\nSlices this <code>StructArray</code>.\nReturns this array sliced.\nReturns this array sliced.\nTakes the validity of this array, leaving it without a …\nReturns a new <code>StructArray</code>.\nThe optional validity.\nReturns the values of this <code>StructArray</code>.\nReturns an iterator of <code>Box&lt;dyn Array&gt;</code>\nSets the chunk size used by the parser. This influences …\nSet values as <code>Null</code> if parsing fails because of schema …\nReturns this array with a new validity.\nApache Parquet file metadata.\nFunctionality for reading Apache Parquet files.\nFunctionality for reading and writing Apache Parquet files.\nMetadata for a Parquet file.\nArrow-deserialized parquet Statistics of a file\nReturns column order for <code>i</code>th column in this file. If …\nColumn (sort) order used for <code>min</code> and <code>max</code> values of each …\nString message for application that wrote this file.\nDeserializes the statistics in the column chunks from a …\nnumber of dictinct values. This is a <code>UInt64Array</code> for …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSerializes itself to thrift’s …\nreturns the metadata\nkey_value_metadata of this file.\nMaximum\nMinimum\nnumber of nulls. This is a <code>UInt64Array</code> for non-nested types\nnumber of rows in the file.\nThe row groups of this file\nReturns the <code>SchemaDescriptor</code> that describes schema of this …\nschema descriptor.\nDeserializes <code>crate::parquet::thrift_format::FileMetaData</code> …\nversion of this file.\nAutomatically determine over which unit to parallelize …\nParallelize over the columns\nDon’t parallelize\nA Parquet reader on top of the async object_store API. …\nRead Apache parquet format into a DataFrame.\nParallelize over the row groups\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nTurn the batched reader into an iterator.\nCreate a new <code>ParquetReader</code> from an existing <code>Reader</code>.\nNumber of rows in the parquet file.\nRead the parquet file in parallel (default). The single …\n<code>Schema</code> of the file.\nTry to reduce memory pressure at the expense of …\nUse statistics in the parquet to determine if pages can be …\nUse statistics in the parquet to determine if pages can be …\nColumns to select/ project\nStop reading at <code>num_rows</code> rows.\nSet the reader’s column projection. This counts from 0, …\nAdd a row index column.\nSet the <code>Schema</code> if already known. This must be exactly the …\nA valid Brotli compression level.\nA valid Gzip compression level.\nThe compression strategy to use for writing Parquet files.\nWrite a DataFrame to Parquet format.\nThe statistics to write\nA valid Zstandard compression level.\nData page compression\nif <code>None</code> will be 1024^2 bytes\nWrites the footer of the parquet file. Returns the total …\nWrite the given DataFrame in the writer <code>W</code>. Returns the …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nmaintain the order the data was processed\nCreate a new writer\nIf <code>None</code> will be all written to a single row group.\nSerialize columns in parallel\nCompute and write column statistics.\nSet the compression used. Defaults to <code>Zstd</code>.\nSets the maximum bytes size of a data page. If <code>None</code> will …\nSet the row group size (in number of rows) during writing. …\nCompute and write statistic\nWrite a batch to the parquet writer.\nWrite a DataFrame with disk partitioning\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nWrite the parquet file in parallel (default).\nKeep track of rayon threads that drive the runtime. Every …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nSpawns a future onto the Tokio runtime (see …\nA collection of column stats with a known schema.\nStatistics of the values in a column.\nCan take &amp;dyn Statistics and determine of a file should be …\nReturns the <code>ColumnStats</code> of all columns in the batch, if …\nReturns the <code>DataType</code> of the column.\nTake a <code>DataFrame</code> and produces a boolean <code>Series</code> that serves …\nReturns the argument unchanged.\nReturns the argument unchanged.\nConstructs a new <code>ColumnStats</code> from a single-value Series.\nConstructs a new <code>ColumnStats</code> with only the <code>Field</code> …\nReturns the maximum value of each row group of the column.\nReturns the minimum value of each row group of the column.\nReturns the null count of each row group of the column.\nReturns the <code>ColumnStats</code> of a single column in the batch.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConstructs a new <code>ColumnStats</code>.\nConstructs a new <code>BatchStats</code>.\nReturns the null count of the column.\nReturns the number of rows in the batch.\nReturns the <code>Schema</code> of the batch.\nReturns the maximum value of the column as a single-value …\nReturns the minimum value of the column as a single-value …\nReturns the minimum and maximum values of the column as a …\nTake the SerReader and return a parsed DataFrame.\nCreate a new instance of the <code>[SerReader]</code>\nMake sure that all columns are contiguous in memory by …\nMake sure that all columns are contiguous in memory by …\nMake sure that all columns are contiguous in memory by …\nChecks if the projected columns are equal\nChecks if the projected columns are equal\nCompute <code>remaining_rows_to_read</code> to be taken per file up …\nCheck if the path is a cloud url.")